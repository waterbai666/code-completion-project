project,filename,code
Albert,test_changes.py,"# coding=utf-8
import tensorflow as tf
from modeling import embedding_lookup_factorized,transformer_model
import os

""""""
测试albert主要的改进点：词嵌入的因式分解、层间参数共享、段落间连贯性
test main change of albert from bert
""""""
batch_size = 2048
sequence_length = 512
vocab_size = 30000
hidden_size = 1024
num_attention_heads = int(hidden_size / 64)

def get_total_parameters():
    """"""
    get total parameters of a graph
    :return:
    """"""
    total_parameters = 0
    for variable in tf.trainable_variables():
        # shape is an array of tf.Dimension
        shape = variable.get_shape()
        # print(shape)
        # print(len(shape))
        variable_parameters = 1
        for dim in shape:
            # print(dim)
            variable_parameters *= dim.value
        # print(variable_parameters)
        total_parameters += variable_parameters
    return total_parameters

def test_factorized_embedding():
    """"""
    test of Factorized embedding parameterization
    :return:
    """"""
    input_ids=tf.zeros((batch_size, sequence_length),dtype=tf.int32)
    output, embedding_table, embedding_table_2=embedding_lookup_factorized(input_ids,vocab_size,hidden_size)
    print(""output:"",output)

def test_share_parameters():
    """"""
    test of share parameters across all layers: how many parameter after share parameter across layers of transformer.
    :return:
    """"""
    def total_parameters_transformer(share_parameter_across_layers):
        input_tensor=tf.zeros((batch_size, sequence_length, hidden_size),dtype=tf.float32)
        print(""transformer_model. input:"",input_tensor)
        transformer_result=transformer_model(input_tensor,hidden_size=hidden_size,num_attention_heads=num_attention_heads,share_parameter_across_layers=share_parameter_across_layers)
        print(""transformer_result:"",transformer_result)
        total_parameters=get_total_parameters()
        print('total_parameters(not share):',total_parameters)

    share_parameter_across_layers=False
    total_parameters_transformer(share_parameter_across_layers) # total parameters, not share: 125,976,576 = 125 million

    tf.reset_default_graph() # Clears the default graph stack and resets the global default graph
    share_parameter_across_layers=True
    total_parameters_transformer(share_parameter_across_layers) #  total parameters,   share: 10,498,048 = 10.5 million

def test_sentence_order_prediction():
    """"""
    sentence order prediction.

    check method of create_instances_from_document_albert from create_pretrining_data.py

    :return:
    """"""
    # 添加运行权限
    os.system(""chmod +x create_pretrain_data.sh"")

    os.system(""./create_pretrain_data.sh"")


# 1.test of Factorized embedding parameterization
#test_factorized_embedding()

# 2. test of share parameters across all layers: how many parameter after share parameter across layers of transformer.
# before share parameter: 125,976,576; after share parameter:
#test_share_parameters()

# 3. test of sentence order prediction(SOP)
test_sentence_order_prediction()

"
Albert,run_classifier_clue.py,"# -*- coding: utf-8 -*-
# @Author: bo.shi
# @Date:   2019-11-04 09:56:36
# @Last Modified by:   bo.shi
# @Last Modified time: 2019-12-04 14:29:04
# coding=utf-8
# Copyright 2018 The Google AI Language Team Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""BERT finetuning runner.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import modeling
import optimization_finetuning as optimization
import tokenization
import tensorflow as tf
# from loss import bi_tempered_logistic_loss
import sys
sys.path.append('..')
from classifier_utils import *

flags = tf.flags

FLAGS = flags.FLAGS

# Required parameters
flags.DEFINE_string(
    ""data_dir"", None,
    ""The input data dir. Should contain the .tsv files (or other data files) ""
    ""for the task."")

flags.DEFINE_string(
    ""bert_config_file"", None,
    ""The config json file corresponding to the pre-trained BERT model. ""
    ""This specifies the model architecture."")

flags.DEFINE_string(""task_name"", None, ""The name of the task to train."")

flags.DEFINE_string(""vocab_file"", None,
                    ""The vocabulary file that the BERT model was trained on."")

flags.DEFINE_string(
    ""output_dir"", None,
    ""The output directory where the model checkpoints will be written."")

# Other parameters

flags.DEFINE_string(
    ""init_checkpoint"", None,
    ""Initial checkpoint (usually from a pre-trained BERT model)."")

flags.DEFINE_bool(
    ""do_lower_case"", True,
    ""Whether to lower case the input text. Should be True for uncased ""
    ""models and False for cased models."")

flags.DEFINE_integer(
    ""max_seq_length"", 128,
    ""The maximum total input sequence length after WordPiece tokenization. ""
    ""Sequences longer than this will be truncated, and sequences shorter ""
    ""than this will be padded."")

flags.DEFINE_bool(""do_train"", False, ""Whether to run training."")

flags.DEFINE_bool(""do_eval"", False, ""Whether to run eval on the dev set."")

flags.DEFINE_bool(
    ""do_predict"", False,
    ""Whether to run the model in inference mode on the test set."")

flags.DEFINE_integer(""train_batch_size"", 32, ""Total batch size for training."")

flags.DEFINE_integer(""eval_batch_size"", 8, ""Total batch size for eval."")

flags.DEFINE_integer(""predict_batch_size"", 8, ""Total batch size for predict."")

flags.DEFINE_float(""learning_rate"", 5e-5, ""The initial learning rate for Adam."")

flags.DEFINE_float(""num_train_epochs"", 3.0,
                   ""Total number of training epochs to perform."")

flags.DEFINE_float(
    ""warmup_proportion"", 0.1,
    ""Proportion of training to perform linear learning rate warmup for. ""
    ""E.g., 0.1 = 10% of training."")

flags.DEFINE_integer(""save_checkpoints_steps"", 1000,
                     ""How often to save the model checkpoint."")

flags.DEFINE_integer(""iterations_per_loop"", 1000,
                     ""How many steps to make in each estimator call."")

flags.DEFINE_bool(""use_tpu"", False, ""Whether to use TPU or GPU/CPU."")

tf.flags.DEFINE_string(
    ""tpu_name"", None,
    ""The Cloud TPU to use for training. This should be either the name ""
    ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 ""
    ""url."")

tf.flags.DEFINE_string(
    ""tpu_zone"", None,
    ""[Optional] GCE zone where the Cloud TPU is located in. If not ""
    ""specified, we will attempt to automatically detect the GCE project from ""
    ""metadata."")

tf.flags.DEFINE_string(
    ""gcp_project"", None,
    ""[Optional] Project name for the Cloud TPU-enabled project. If not ""
    ""specified, we will attempt to automatically detect the GCE project from ""
    ""metadata."")

tf.flags.DEFINE_string(""master"", None, ""[Optional] TensorFlow master URL."")

flags.DEFINE_integer(
    ""num_tpu_cores"", 8,
    ""Only used if `use_tpu` is True. Total number of TPU cores to use."")


class InputFeatures(object):
  """"""A single set of features of data.""""""

  def __init__(self,
               input_ids,
               input_mask,
               segment_ids,
               label_id,
               is_real_example=True):
    self.input_ids = input_ids
    self.input_mask = input_mask
    self.segment_ids = segment_ids
    self.label_id = label_id
    self.is_real_example = is_real_example


def convert_single_example_for_inews(ex_index, tokens_a, tokens_b, label_map, max_seq_length,
                                     tokenizer, example):
  if tokens_b:
    # Modifies `tokens_a` and `tokens_b` in place so that the total
    # length is less than the specified length.
    # Account for [CLS], [SEP], [SEP] with ""- 3""
    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)
  else:
    # Account for [CLS] and [SEP] with ""- 2""
    if len(tokens_a) > max_seq_length - 2:
      tokens_a = tokens_a[0:(max_seq_length - 2)]

  # The convention in BERT is:
  # (a) For sequence pairs:
  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]
  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
  # (b) For single sequences:
  #  tokens:   [CLS] the dog is hairy . [SEP]
  #  type_ids: 0     0   0   0  0     0 0
  #
  # Where ""type_ids"" are used to indicate whether this is the first
  # sequence or the second sequence. The embedding vectors for `type=0` and
  # `type=1` were learned during pre-training and are added to the wordpiece
  # embedding vector (and position vector). This is not *strictly* necessary
  # since the [SEP] token unambiguously separates the sequences, but it makes
  # it easier for the model to learn the concept of sequences.
  #
  # For classification tasks, the first vector (corresponding to [CLS]) is
  # used as the ""sentence vector"". Note that this only makes sense because
  # the entire model is fine-tuned.
  tokens = []
  segment_ids = []
  tokens.append(""[CLS]"")
  segment_ids.append(0)
  for token in tokens_a:
    tokens.append(token)
    segment_ids.append(0)
  tokens.append(""[SEP]"")
  segment_ids.append(0)

  if tokens_b:
    for token in tokens_b:
      tokens.append(token)
      segment_ids.append(1)
    tokens.append(""[SEP]"")
    segment_ids.append(1)

  input_ids = tokenizer.convert_tokens_to_ids(tokens)

  # The mask has 1 for real tokens and 0 for padding tokens. Only real
  # tokens are attended to.
  input_mask = [1] * len(input_ids)

  # Zero-pad up to the sequence length.
  while len(input_ids) < max_seq_length:
    input_ids.append(0)
    input_mask.append(0)
    segment_ids.append(0)

  assert len(input_ids) == max_seq_length
  assert len(input_mask) == max_seq_length
  assert len(segment_ids) == max_seq_length

  label_id = label_map[example.label]
  if ex_index < 5:
    tf.logging.info(""*** Example ***"")
    tf.logging.info(""guid: %s"" % (example.guid))
    tf.logging.info(""tokens: %s"" % "" "".join(
        [tokenization.printable_text(x) for x in tokens]))
    tf.logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))
    tf.logging.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))
    tf.logging.info(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))
    tf.logging.info(""label: %s (id = %d)"" % (example.label, label_id))

  feature = InputFeatures(
      input_ids=input_ids,
      input_mask=input_mask,
      segment_ids=segment_ids,
      label_id=label_id,
      is_real_example=True)

  return feature


def convert_example_list_for_inews(ex_index, example, label_list, max_seq_length,
                                   tokenizer):
  """"""Converts a single `InputExample` into a single `InputFeatures`.""""""

  if isinstance(example, PaddingInputExample):
    return [InputFeatures(
        input_ids=[0] * max_seq_length,
        input_mask=[0] * max_seq_length,
        segment_ids=[0] * max_seq_length,
        label_id=0,
        is_real_example=False)]

  label_map = {}
  for (i, label) in enumerate(label_list):
    label_map[label] = i

  tokens_a = tokenizer.tokenize(example.text_a)
  tokens_b = None
  if example.text_b:
    tokens_b = tokenizer.tokenize(example.text_b)
    must_len = len(tokens_a) + 3
    extra_len = max_seq_length - must_len
  feature_list = []
  if example.text_b and extra_len > 0:
    extra_num = int((len(tokens_b) - 1) / extra_len) + 1
    for num in range(extra_num):
      max_len = min((num + 1) * extra_len, len(tokens_b))
      tokens_b_sub = tokens_b[num * extra_len: max_len]
      feature = convert_single_example_for_inews(
          ex_index, tokens_a, tokens_b_sub, label_map, max_seq_length, tokenizer, example)
      feature_list.append(feature)
  else:
    feature = convert_single_example_for_inews(
        ex_index, tokens_a, tokens_b, label_map, max_seq_length, tokenizer, example)
    feature_list.append(feature)
  return feature_list


def file_based_convert_examples_to_features_for_inews(
        examples, label_list, max_seq_length, tokenizer, output_file):
  """"""Convert a set of `InputExample`s to a TFRecord file.""""""

  writer = tf.python_io.TFRecordWriter(output_file)
  num_example = 0
  for (ex_index, example) in enumerate(examples):
    if ex_index % 1000 == 0:
      tf.logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))

    feature_list = convert_example_list_for_inews(ex_index, example, label_list,
                                                  max_seq_length, tokenizer)
    num_example += len(feature_list)

    def create_int_feature(values):
      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))
      return f

    features = collections.OrderedDict()
    for feature in feature_list:
      features[""input_ids""] = create_int_feature(feature.input_ids)
      features[""input_mask""] = create_int_feature(feature.input_mask)
      features[""segment_ids""] = create_int_feature(feature.segment_ids)
      features[""label_ids""] = create_int_feature([feature.label_id])
      features[""is_real_example""] = create_int_feature(
          [int(feature.is_real_example)])

      tf_example = tf.train.Example(features=tf.train.Features(feature=features))
      writer.write(tf_example.SerializeToString())
  tf.logging.info(""feature num: %s"", num_example)
  writer.close()


def convert_single_example(ex_index, example, label_list, max_seq_length,
                           tokenizer):
  """"""Converts a single `InputExample` into a single `InputFeatures`.""""""

  if isinstance(example, PaddingInputExample):
    return InputFeatures(
        input_ids=[0] * max_seq_length,
        input_mask=[0] * max_seq_length,
        segment_ids=[0] * max_seq_length,
        label_id=0,
        is_real_example=False)

  label_map = {}
  for (i, label) in enumerate(label_list):
    label_map[label] = i

  tokens_a = tokenizer.tokenize(example.text_a)
  tokens_b = None
  if example.text_b:
    tokens_b = tokenizer.tokenize(example.text_b)

  if tokens_b:
    # Modifies `tokens_a` and `tokens_b` in place so that the total
    # length is less than the specified length.
    # Account for [CLS], [SEP], [SEP] with ""- 3""
    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)
  else:
    # Account for [CLS] and [SEP] with ""- 2""
    if len(tokens_a) > max_seq_length - 2:
      tokens_a = tokens_a[0:(max_seq_length - 2)]

  # The convention in BERT is:
  # (a) For sequence pairs:
  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]
  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
  # (b) For single sequences:
  #  tokens:   [CLS] the dog is hairy . [SEP]
  #  type_ids: 0     0   0   0  0     0 0
  #
  # Where ""type_ids"" are used to indicate whether this is the first
  # sequence or the second sequence. The embedding vectors for `type=0` and
  # `type=1` were learned during pre-training and are added to the wordpiece
  # embedding vector (and position vector). This is not *strictly* necessary
  # since the [SEP] token unambiguously separates the sequences, but it makes
  # it easier for the model to learn the concept of sequences.
  #
  # For classification tasks, the first vector (corresponding to [CLS]) is
  # used as the ""sentence vector"". Note that this only makes sense because
  # the entire model is fine-tuned.
  tokens = []
  segment_ids = []
  tokens.append(""[CLS]"")
  segment_ids.append(0)
  for token in tokens_a:
    tokens.append(token)
    segment_ids.append(0)
  tokens.append(""[SEP]"")
  segment_ids.append(0)

  if tokens_b:
    for token in tokens_b:
      tokens.append(token)
      segment_ids.append(1)
    tokens.append(""[SEP]"")
    segment_ids.append(1)

  input_ids = tokenizer.convert_tokens_to_ids(tokens)

  # The mask has 1 for real tokens and 0 for padding tokens. Only real
  # tokens are attended to.
  input_mask = [1] * len(input_ids)

  # Zero-pad up to the sequence length.
  while len(input_ids) < max_seq_length:
    input_ids.append(0)
    input_mask.append(0)
    segment_ids.append(0)

  assert len(input_ids) == max_seq_length
  assert len(input_mask) == max_seq_length
  assert len(segment_ids) == max_seq_length

  label_id = label_map[example.label]
  if ex_index < 5:
    tf.logging.info(""*** Example ***"")
    tf.logging.info(""guid: %s"" % (example.guid))
    tf.logging.info(""tokens: %s"" % "" "".join(
        [tokenization.printable_text(x) for x in tokens]))
    tf.logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))
    tf.logging.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))
    tf.logging.info(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))
    tf.logging.info(""label: %s (id = %d)"" % (example.label, label_id))

  feature = InputFeatures(
      input_ids=input_ids,
      input_mask=input_mask,
      segment_ids=segment_ids,
      label_id=label_id,
      is_real_example=True)
  return feature


def file_based_convert_examples_to_features(
        examples, label_list, max_seq_length, tokenizer, output_file):
  """"""Convert a set of `InputExample`s to a TFRecord file.""""""

  writer = tf.python_io.TFRecordWriter(output_file)

  for (ex_index, example) in enumerate(examples):
    if ex_index % 10000 == 0:
      tf.logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))

    feature = convert_single_example(ex_index, example, label_list,
                                     max_seq_length, tokenizer)

    def create_int_feature(values):
      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))
      return f

    features = collections.OrderedDict()
    features[""input_ids""] = create_int_feature(feature.input_ids)
    features[""input_mask""] = create_int_feature(feature.input_mask)
    features[""segment_ids""] = create_int_feature(feature.segment_ids)
    features[""label_ids""] = create_int_feature([feature.label_id])
    features[""is_real_example""] = create_int_feature(
        [int(feature.is_real_example)])

    tf_example = tf.train.Example(features=tf.train.Features(feature=features))
    writer.write(tf_example.SerializeToString())
  writer.close()


def file_based_input_fn_builder(input_file, seq_length, is_training,
                                drop_remainder):
  """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""

  name_to_features = {
      ""input_ids"": tf.FixedLenFeature([seq_length], tf.int64),
      ""input_mask"": tf.FixedLenFeature([seq_length], tf.int64),
      ""segment_ids"": tf.FixedLenFeature([seq_length], tf.int64),
      ""label_ids"": tf.FixedLenFeature([], tf.int64),
      ""is_real_example"": tf.FixedLenFeature([], tf.int64),
  }

  def _decode_record(record, name_to_features):
    """"""Decodes a record to a TensorFlow example.""""""
    example = tf.parse_single_example(record, name_to_features)

    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.
    # So cast all int64 to int32.
    for name in list(example.keys()):
      t = example[name]
      if t.dtype == tf.int64:
        t = tf.to_int32(t)
      example[name] = t

    return example

  def input_fn(params):
    """"""The actual input function.""""""
    batch_size = params[""batch_size""]

    # For training, we want a lot of parallel reading and shuffling.
    # For eval, we want no shuffling and parallel reading doesn't matter.
    d = tf.data.TFRecordDataset(input_file)
    if is_training:
      d = d.repeat()
      d = d.shuffle(buffer_size=100)

    d = d.apply(
        tf.contrib.data.map_and_batch(
            lambda record: _decode_record(record, name_to_features),
            batch_size=batch_size,
            drop_remainder=drop_remainder))

    return d

  return input_fn


def _truncate_seq_pair(tokens_a, tokens_b, max_length):
  """"""Truncates a sequence pair in place to the maximum length.""""""

  # This is a simple heuristic which will always truncate the longer sequence
  # one token at a time. This makes more sense than truncating an equal percent
  # of tokens from each, since if one sequence is very short then each token
  # that's truncated likely contains more information than a longer sequence.
  while True:
    total_length = len(tokens_a) + len(tokens_b)
    if total_length <= max_length:
      break
    if len(tokens_a) > len(tokens_b):
      tokens_a.pop()
    else:
      tokens_b.pop()


def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,
                 labels, num_labels, use_one_hot_embeddings):
  """"""Creates a classification model.""""""
  model = modeling.BertModel(
      config=bert_config,
      is_training=is_training,
      input_ids=input_ids,
      input_mask=input_mask,
      token_type_ids=segment_ids,
      use_one_hot_embeddings=use_one_hot_embeddings)

  # In the demo, we are doing a simple classification task on the entire
  # segment.
  #
  # If you want to use the token-level output, use model.get_sequence_output()
  # instead.
  output_layer = model.get_pooled_output()

  hidden_size = output_layer.shape[-1].value

  output_weights = tf.get_variable(
      ""output_weights"", [num_labels, hidden_size],
      initializer=tf.truncated_normal_initializer(stddev=0.02))

  output_bias = tf.get_variable(
      ""output_bias"", [num_labels], initializer=tf.zeros_initializer())

  with tf.variable_scope(""loss""):
    ln_type = bert_config.ln_type
    if ln_type == 'preln':  # add by brightmart, 10-06. if it is preln, we need to an additonal layer: layer normalization as suggested in paper ""ON LAYER NORMALIZATION IN THE TRANSFORMER ARCHITECTURE""
      print(""ln_type is preln. add LN layer."")
      output_layer = layer_norm(output_layer)
    else:
      print(""ln_type is postln or other,do nothing."")

    if is_training:
      # I.e., 0.1 dropout
      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)

    logits = tf.matmul(output_layer, output_weights, transpose_b=True)
    logits = tf.nn.bias_add(logits, output_bias)
    probabilities = tf.nn.softmax(logits, axis=-1)
    log_probs = tf.nn.log_softmax(logits, axis=-1)

    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)

    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs,
                                      axis=-1)  # todo 08-29 try temp-loss
    ###############bi_tempered_logistic_loss############################################################################
    # print(""##cross entropy loss is used....""); tf.logging.info(""##cross entropy loss is used...."")
    # t1=0.9 #t1=0.90
    # t2=1.05 #t2=1.05
    # per_example_loss=bi_tempered_logistic_loss(log_probs,one_hot_labels,t1,t2,label_smoothing=0.1,num_iters=5) # TODO label_smoothing=0.0
    # tf.logging.info(""per_example_loss:""+str(per_example_loss.shape))
    ##############bi_tempered_logistic_loss#############################################################################

    loss = tf.reduce_mean(per_example_loss)

    return (loss, per_example_loss, logits, probabilities)


def layer_norm(input_tensor, name=None):
  """"""Run layer normalization on the last dimension of the tensor.""""""
  return tf.contrib.layers.layer_norm(
      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)


def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,
                     num_train_steps, num_warmup_steps, use_tpu,
                     use_one_hot_embeddings):
  """"""Returns `model_fn` closure for TPUEstimator.""""""

  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
    """"""The `model_fn` for TPUEstimator.""""""

    tf.logging.info(""*** Features ***"")
    for name in sorted(features.keys()):
      tf.logging.info(""  name = %s, shape = %s"" % (name, features[name].shape))

    input_ids = features[""input_ids""]
    input_mask = features[""input_mask""]
    segment_ids = features[""segment_ids""]
    label_ids = features[""label_ids""]
    is_real_example = None
    if ""is_real_example"" in features:
      is_real_example = tf.cast(features[""is_real_example""], dtype=tf.float32)
    else:
      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)

    is_training = (mode == tf.estimator.ModeKeys.TRAIN)

    (total_loss, per_example_loss, logits, probabilities) = create_model(
        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,
        num_labels, use_one_hot_embeddings)

    tvars = tf.trainable_variables()
    initialized_variable_names = {}
    scaffold_fn = None
    if init_checkpoint:
      (assignment_map, initialized_variable_names
       ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)
      if use_tpu:

        def tpu_scaffold():
          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)
          return tf.train.Scaffold()

        scaffold_fn = tpu_scaffold
      else:
        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)

    tf.logging.info(""**** Trainable Variables ****"")
    for var in tvars:
      init_string = """"
      if var.name in initialized_variable_names:
        init_string = "", *INIT_FROM_CKPT*""
      tf.logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,
                      init_string)

    output_spec = None
    if mode == tf.estimator.ModeKeys.TRAIN:

      train_op = optimization.create_optimizer(
          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)

      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          loss=total_loss,
          train_op=train_op,
          scaffold_fn=scaffold_fn)
    elif mode == tf.estimator.ModeKeys.EVAL:

      def metric_fn(per_example_loss, label_ids, logits, is_real_example):
        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)
        accuracy = tf.metrics.accuracy(
            labels=label_ids, predictions=predictions, weights=is_real_example)
        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)
        return {
            ""eval_accuracy"": accuracy,
            ""eval_loss"": loss,
        }

      eval_metrics = (metric_fn,
                      [per_example_loss, label_ids, logits, is_real_example])
      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          loss=total_loss,
          eval_metrics=eval_metrics,
          scaffold_fn=scaffold_fn)
    else:
      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          predictions={""probabilities"": probabilities},
          scaffold_fn=scaffold_fn)
    return output_spec

  return model_fn


# This function is not used by this file but is still used by the Colab and
# people who depend on it.
def input_fn_builder(features, seq_length, is_training, drop_remainder):
  """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""

  all_input_ids = []
  all_input_mask = []
  all_segment_ids = []
  all_label_ids = []

  for feature in features:
    all_input_ids.append(feature.input_ids)
    all_input_mask.append(feature.input_mask)
    all_segment_ids.append(feature.segment_ids)
    all_label_ids.append(feature.label_id)

  def input_fn(params):
    """"""The actual input function.""""""
    batch_size = params[""batch_size""]

    num_examples = len(features)

    # This is for demo purposes and does NOT scale to large data sets. We do
    # not use Dataset.from_generator() because that uses tf.py_func which is
    # not TPU compatible. The right way to load data is with TFRecordReader.
    d = tf.data.Dataset.from_tensor_slices({
        ""input_ids"":
            tf.constant(
                all_input_ids, shape=[num_examples, seq_length],
                dtype=tf.int32),
        ""input_mask"":
            tf.constant(
                all_input_mask,
                shape=[num_examples, seq_length],
                dtype=tf.int32),
        ""segment_ids"":
            tf.constant(
                all_segment_ids,
                shape=[num_examples, seq_length],
                dtype=tf.int32),
        ""label_ids"":
            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),
    })

    if is_training:
      d = d.repeat()
      d = d.shuffle(buffer_size=100)

    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)
    return d

  return input_fn


# This function is not used by this file but is still used by the Colab and
# people who depend on it.
def convert_examples_to_features(examples, label_list, max_seq_length,
                                 tokenizer):
  """"""Convert a set of `InputExample`s to a list of `InputFeatures`.""""""

  features = []
  for (ex_index, example) in enumerate(examples):
    if ex_index % 10000 == 0:
      tf.logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))

    feature = convert_single_example(ex_index, example, label_list,
                                     max_seq_length, tokenizer)

    features.append(feature)
  return features


def main(_):
  tf.logging.set_verbosity(tf.logging.INFO)

  processors = {
      ""xnli"": XnliProcessor,
      ""tnews"": TnewsProcessor,
      ""afqmc"": AFQMCProcessor,
      ""iflytek"": iFLYTEKDataProcessor,
      ""copa"": COPAProcessor,
      ""cmnli"": CMNLIProcessor,
      ""wsc"": WSCProcessor,
      ""csl"": CslProcessor,
      ""copa"": COPAProcessor,
  }

  tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case,
                                                FLAGS.init_checkpoint)

  if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:
    raise ValueError(
        ""At least one of `do_train`, `do_eval` or `do_predict' must be True."")

  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)

  if FLAGS.max_seq_length > bert_config.max_position_embeddings:
    raise ValueError(
        ""Cannot use sequence length %d because the BERT model ""
        ""was only trained up to sequence length %d"" %
        (FLAGS.max_seq_length, bert_config.max_position_embeddings))

  tf.gfile.MakeDirs(FLAGS.output_dir)

  task_name = FLAGS.task_name.lower()

  if task_name not in processors:
    raise ValueError(""Task not found: %s"" % (task_name))

  processor = processors[task_name]()

  label_list = processor.get_labels()

  tokenizer = tokenization.FullTokenizer(
      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)

  tpu_cluster_resolver = None
  if FLAGS.use_tpu and FLAGS.tpu_name:
    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)

  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2
  # Cloud TPU: Invalid TPU configuration, ensure ClusterResolver is passed to tpu.
  print(""###tpu_cluster_resolver:"", tpu_cluster_resolver)
  run_config = tf.contrib.tpu.RunConfig(
      cluster=tpu_cluster_resolver,
      master=FLAGS.master,
      model_dir=FLAGS.output_dir,
      save_checkpoints_steps=FLAGS.save_checkpoints_steps,
      tpu_config=tf.contrib.tpu.TPUConfig(
          iterations_per_loop=FLAGS.iterations_per_loop,
          num_shards=FLAGS.num_tpu_cores,
          per_host_input_for_training=is_per_host))

  train_examples = None
  num_train_steps = None
  num_warmup_steps = None
  if FLAGS.do_train:
    train_examples = processor.get_train_examples(FLAGS.data_dir)  # TODO
    print(""###length of total train_examples:"", len(train_examples))
    num_train_steps = int(len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)
    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)

  model_fn = model_fn_builder(
      bert_config=bert_config,
      num_labels=len(label_list),
      init_checkpoint=FLAGS.init_checkpoint,
      learning_rate=FLAGS.learning_rate,
      num_train_steps=num_train_steps,
      num_warmup_steps=num_warmup_steps,
      use_tpu=FLAGS.use_tpu,
      use_one_hot_embeddings=FLAGS.use_tpu)

  # If TPU is not available, this will fall back to normal Estimator on CPU
  # or GPU.
  estimator = tf.contrib.tpu.TPUEstimator(
      use_tpu=FLAGS.use_tpu,
      model_fn=model_fn,
      config=run_config,
      train_batch_size=FLAGS.train_batch_size,
      eval_batch_size=FLAGS.eval_batch_size,
      predict_batch_size=FLAGS.predict_batch_size)

  if FLAGS.do_train:
    train_file = os.path.join(FLAGS.output_dir, ""train.tf_record"")
    train_file_exists = os.path.exists(train_file)
    print(""###train_file_exists:"", train_file_exists, "" ;train_file:"", train_file)
    if not train_file_exists:  # if tf_record file not exist, convert from raw text file. # TODO
      if task_name == ""inews"":
        file_based_convert_examples_to_features_for_inews(
            train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)
      else:
        file_based_convert_examples_to_features(
            train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)
    tf.logging.info(""***** Running training *****"")
    tf.logging.info(""  Num examples = %d"", len(train_examples))
    tf.logging.info(""  Batch size = %d"", FLAGS.train_batch_size)
    tf.logging.info(""  Num steps = %d"", num_train_steps)
    train_input_fn = file_based_input_fn_builder(
        input_file=train_file,
        seq_length=FLAGS.max_seq_length,
        is_training=True,
        drop_remainder=True)
    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)

  if FLAGS.do_eval:
    # dev dataset
    eval_examples = processor.get_dev_examples(FLAGS.data_dir)
    num_actual_eval_examples = len(eval_examples)
    if FLAGS.use_tpu:
      # TPU requires a fixed batch size for all batches, therefore the number
      # of examples must be a multiple of the batch size, or else examples
      # will get dropped. So we pad with fake examples which are ignored
      # later on. These do NOT count towards the metric (all tf.metrics
      # support a per-instance weight, and these get a weight of 0.0).
      while len(eval_examples) % FLAGS.eval_batch_size != 0:
        eval_examples.append(PaddingInputExample())

    eval_file = os.path.join(FLAGS.output_dir, ""dev.tf_record"")
    if task_name == ""inews"":
      file_based_convert_examples_to_features_for_inews(
          eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)
    else:
      file_based_convert_examples_to_features(
          eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)

    tf.logging.info(""***** Running evaluation *****"")
    tf.logging.info(""  Num examples = %d (%d actual, %d padding)"",
                    len(eval_examples), num_actual_eval_examples,
                    len(eval_examples) - num_actual_eval_examples)
    tf.logging.info(""  Batch size = %d"", FLAGS.eval_batch_size)

    # This tells the estimator to run through the entire set.
    eval_steps = None
    # However, if running eval on the TPU, you will need to specify the
    # number of steps.
    if FLAGS.use_tpu:
      assert len(eval_examples) % FLAGS.eval_batch_size == 0
      eval_steps = int(len(eval_examples) // FLAGS.eval_batch_size)

    eval_drop_remainder = True if FLAGS.use_tpu else False
    eval_input_fn = file_based_input_fn_builder(
        input_file=eval_file,
        seq_length=FLAGS.max_seq_length,
        is_training=False,
        drop_remainder=eval_drop_remainder)

    #######################################################################################################################
    # evaluate all checkpoints; you can use the checkpoint with the best dev accuarcy
    steps_and_files = []
    filenames = tf.gfile.ListDirectory(FLAGS.output_dir)
    for filename in filenames:
      if filename.endswith("".index""):
        ckpt_name = filename[:-6]
        cur_filename = os.path.join(FLAGS.output_dir, ckpt_name)
        global_step = int(cur_filename.split(""-"")[-1])
        tf.logging.info(""Add {} to eval list."".format(cur_filename))
        steps_and_files.append([global_step, cur_filename])
    steps_and_files = sorted(steps_and_files, key=lambda x: x[0])

    output_eval_file = os.path.join(FLAGS.data_dir, ""dev_results_albert_zh.txt"")
    print(""output_eval_file:"", output_eval_file)
    tf.logging.info(""output_eval_file:"" + output_eval_file)
    with tf.gfile.GFile(output_eval_file, ""w"") as writer:
      for global_step, filename in sorted(steps_and_files, key=lambda x: x[0]):
        result = estimator.evaluate(input_fn=eval_input_fn,
                                    steps=eval_steps, checkpoint_path=filename)

        tf.logging.info(""***** Eval results %s *****"" % (filename))
        writer.write(""***** Eval results %s *****\n"" % (filename))
        for key in sorted(result.keys()):
          tf.logging.info(""  %s = %s"", key, str(result[key]))
          writer.write(""%s = %s\n"" % (key, str(result[key])))
    #######################################################################################################################

    # result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)
    #
    # output_eval_file = os.path.join(FLAGS.output_dir, ""dev_results_albert_zh.txt"")
    # with tf.gfile.GFile(output_eval_file, ""w"") as writer:
    #  tf.logging.info(""***** Eval results *****"")
    #  for key in sorted(result.keys()):
    #    tf.logging.info(""  %s = %s"", key, str(result[key]))
    #    writer.write(""%s = %s\n"" % (key, str(result[key])))

  if FLAGS.do_predict:
    predict_examples = processor.get_test_examples(FLAGS.data_dir)
    num_actual_predict_examples = len(predict_examples)
    if FLAGS.use_tpu:
      # TPU requires a fixed batch size for all batches, therefore the number
      # of examples must be a multiple of the batch size, or else examples
      # will get dropped. So we pad with fake examples which are ignored
      # later on.
      while len(predict_examples) % FLAGS.predict_batch_size != 0:
        predict_examples.append(PaddingInputExample())

    predict_file = os.path.join(FLAGS.output_dir, ""predict.tf_record"")
    if task_name == ""inews"":
      file_based_convert_examples_to_features_for_inews(predict_examples, label_list,
                                                        FLAGS.max_seq_length, tokenizer,
                                                        predict_file)
    else:
      file_based_convert_examples_to_features(predict_examples, label_list,
                                              FLAGS.max_seq_length, tokenizer,
                                              predict_file)

    tf.logging.info(""***** Running prediction*****"")
    tf.logging.info(""  Num examples = %d (%d actual, %d padding)"",
                    len(predict_examples), num_actual_predict_examples,
                    len(predict_examples) - num_actual_predict_examples)
    tf.logging.info(""  Batch size = %d"", FLAGS.predict_batch_size)

    predict_drop_remainder = True if FLAGS.use_tpu else False
    predict_input_fn = file_based_input_fn_builder(
        input_file=predict_file,
        seq_length=FLAGS.max_seq_length,
        is_training=False,
        drop_remainder=predict_drop_remainder)

    result = estimator.predict(input_fn=predict_input_fn)
    index2label_map = {}
    for (i, label) in enumerate(label_list):
      index2label_map[i] = label
    output_predict_file_label_name = task_name + ""_predict.json""
    output_predict_file_label = os.path.join(FLAGS.output_dir, output_predict_file_label_name)
    output_predict_file = os.path.join(FLAGS.output_dir, ""test_results.tsv"")
    with tf.gfile.GFile(output_predict_file_label, ""w"") as writer_label:
      with tf.gfile.GFile(output_predict_file, ""w"") as writer:
        num_written_lines = 0
        tf.logging.info(""***** Predict results *****"")
        for (i, prediction) in enumerate(result):
          probabilities = prediction[""probabilities""]
          label_index = probabilities.argmax(0)
          if i >= num_actual_predict_examples:
            break
          output_line = ""\t"".join(
              str(class_probability)
              for class_probability in probabilities) + ""\n""
          test_label_dict = {}
          test_label_dict[""id""] = i
          test_label_dict[""label""] = str(index2label_map[label_index])
          if task_name == ""tnews"":
            test_label_dict[""label_desc""] = """"
          writer.write(output_line)
          json.dump(test_label_dict, writer_label)
          writer_label.write(""\n"")
          num_written_lines += 1
    assert num_written_lines == num_actual_predict_examples


if __name__ == ""__main__"":
  flags.mark_flag_as_required(""data_dir"")
  flags.mark_flag_as_required(""task_name"")
  flags.mark_flag_as_required(""vocab_file"")
  flags.mark_flag_as_required(""bert_config_file"")
  flags.mark_flag_as_required(""output_dir"")
  tf.app.run()"
Albert,similarity.py,"""""""
进行文本相似度预测的示例。可以直接运行进行预测。
参考了项目：https://github.com/chdd/bert-utils

""""""


import tensorflow as tf
import args
import tokenization
import modeling
from run_classifier import InputFeatures, InputExample, DataProcessor, create_model, convert_examples_to_features


# os.environ['CUDA_VISIBLE_DEVICES'] = '1'


class SimProcessor(DataProcessor):
    def get_sentence_examples(self, questions):
        examples = []
        for index, data in enumerate(questions):
            guid = 'test-%d' % index
            text_a = tokenization.convert_to_unicode(str(data[0]))
            text_b = tokenization.convert_to_unicode(str(data[1]))
            label = str(0)
            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
        return examples

    def get_labels(self):
        return ['0', '1']


""""""
模型类，负责载入checkpoint初始化模型
""""""
class BertSim:
    def __init__(self, batch_size=args.batch_size):
        self.mode = None
        self.max_seq_length = args.max_seq_len
        self.tokenizer = tokenization.FullTokenizer(vocab_file=args.vocab_file, do_lower_case=True)
        self.batch_size = batch_size
        self.estimator = None
        self.processor = SimProcessor()
        tf.logging.set_verbosity(tf.logging.INFO)



    #载入estimator,构造模型
    def start_model(self):
        self.estimator = self.get_estimator()


    def model_fn_builder(self, bert_config, num_labels, init_checkpoint, learning_rate,
                         num_train_steps, num_warmup_steps,
                         use_one_hot_embeddings):
        """"""Returns `model_fn` closurimport_tfe for TPUEstimator.""""""

        def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
            from tensorflow.python.estimator.model_fn import EstimatorSpec

            tf.logging.info(""*** Features ***"")
            for name in sorted(features.keys()):
                tf.logging.info(""  name = %s, shape = %s"" % (name, features[name].shape))

            input_ids = features[""input_ids""]
            input_mask = features[""input_mask""]
            segment_ids = features[""segment_ids""]
            label_ids = features[""label_ids""]

            is_training = (mode == tf.estimator.ModeKeys.TRAIN)

            (total_loss, per_example_loss, logits, probabilities) = create_model(
                bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,
                num_labels, use_one_hot_embeddings)

            tvars = tf.trainable_variables()
            initialized_variable_names = {}

            if init_checkpoint:
                (assignment_map, initialized_variable_names) \
                    = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)
                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)

            tf.logging.info(""**** Trainable Variables ****"")
            for var in tvars:
                init_string = """"
                if var.name in initialized_variable_names:
                    init_string = "", *INIT_FROM_CKPT*""
                tf.logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,
                                init_string)
            output_spec = EstimatorSpec(mode=mode, predictions=probabilities)

            return output_spec

        return model_fn

    def get_estimator(self):

        from tensorflow.python.estimator.estimator import Estimator
        from tensorflow.python.estimator.run_config import RunConfig

        bert_config = modeling.BertConfig.from_json_file(args.config_name)
        label_list = self.processor.get_labels()
        if self.mode == tf.estimator.ModeKeys.TRAIN:
            init_checkpoint = args.ckpt_name
        else:
            init_checkpoint = args.output_dir

        model_fn = self.model_fn_builder(
            bert_config=bert_config,
            num_labels=len(label_list),
            init_checkpoint=init_checkpoint,
            learning_rate=args.learning_rate,
            num_train_steps=None,
            num_warmup_steps=None,
            use_one_hot_embeddings=False)

        config = tf.ConfigProto()
        config.gpu_options.allow_growth = True
        config.gpu_options.per_process_gpu_memory_fraction = args.gpu_memory_fraction
        config.log_device_placement = False

        return Estimator(model_fn=model_fn, config=RunConfig(session_config=config), model_dir=args.output_dir,
                         params={'batch_size': self.batch_size})

    def predict_sentences(self,sentences):
        results= self.estimator.predict(input_fn=input_fn_builder(self,sentences), yield_single_examples=False)
        #打印预测结果
        for i in results:
            print(i)

    def _truncate_seq_pair(self, tokens_a, tokens_b, max_length):
        """"""Truncates a sequence pair in place to the maximum length.""""""

        # This is a simple heuristic which will always truncate the longer sequence
        # one token at a time. This makes more sense than truncating an equal percent
        # of tokens from each, since if one sequence is very short then each token
        # that's truncated likely contains more information than a longer sequence.
        while True:
            total_length = len(tokens_a) + len(tokens_b)
            if total_length <= max_length:
                break
            if len(tokens_a) > len(tokens_b):
                tokens_a.pop()
            else:
                tokens_b.pop()

    def convert_single_example(self, ex_index, example, label_list, max_seq_length, tokenizer):
        """"""Converts a single `InputExample` into a single `InputFeatures`.""""""
        label_map = {}
        for (i, label) in enumerate(label_list):
            label_map[label] = i

        tokens_a = tokenizer.tokenize(example.text_a)
        tokens_b = None
        if example.text_b:
            tokens_b = tokenizer.tokenize(example.text_b)

        if tokens_b:
            # Modifies `tokens_a` and `tokens_b` in place so that the total
            # length is less than the specified length.
            # Account for [CLS], [SEP], [SEP] with ""- 3""
            self._truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)
        else:
            # Account for [CLS] and [SEP] with ""- 2""
            if len(tokens_a) > max_seq_length - 2:
                tokens_a = tokens_a[0:(max_seq_length - 2)]

        # The convention in BERT is:
        # (a) For sequence pairs:
        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]
        #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
        # (b) For single sequences:
        #  tokens:   [CLS] the dog is hairy . [SEP]
        #  type_ids: 0     0   0   0  0     0 0
        #
        # Where ""type_ids"" are used to indicate whether this is the first
        # sequence or the second sequence. The embedding vectors for `type=0` and
        # `type=1` were learned during pre-training and are added to the wordpiece
        # embedding vector (and position vector). This is not *strictly* necessary
        # since the [SEP] token unambiguously separates the sequences, but it makes
        # it easier for the model to learn the concept of sequences.
        #
        # For classification tasks, the first vector (corresponding to [CLS]) is
        # used as as the ""sentence vector"". Note that this only makes sense because
        # the entire model is fine-tuned.
        tokens = []
        segment_ids = []
        tokens.append(""[CLS]"")
        segment_ids.append(0)
        for token in tokens_a:
            tokens.append(token)
            segment_ids.append(0)
        tokens.append(""[SEP]"")
        segment_ids.append(0)

        if tokens_b:
            for token in tokens_b:
                tokens.append(token)
                segment_ids.append(1)
            tokens.append(""[SEP]"")
            segment_ids.append(1)

        input_ids = tokenizer.convert_tokens_to_ids(tokens)

        # The mask has 1 for real tokens and 0 for padding tokens. Only real
        # tokens are attended to.
        input_mask = [1] * len(input_ids)

        # Zero-pad up to the sequence length.
        while len(input_ids) < max_seq_length:
            input_ids.append(0)
            input_mask.append(0)
            segment_ids.append(0)

        assert len(input_ids) == max_seq_length
        assert len(input_mask) == max_seq_length
        assert len(segment_ids) == max_seq_length

        label_id = label_map[example.label]
        if ex_index < 5:
            tf.logging.info(""*** Example ***"")
            tf.logging.info(""guid: %s"" % (example.guid))
            tf.logging.info(""tokens: %s"" % "" "".join(
                [tokenization.printable_text(x) for x in tokens]))
            tf.logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))
            tf.logging.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))
            tf.logging.info(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))
            tf.logging.info(""label: %s (id = %d)"" % (example.label, label_id))

        feature = InputFeatures(
            input_ids=input_ids,
            input_mask=input_mask,
            segment_ids=segment_ids,
            label_id=label_id)
        return feature




def input_fn_builder(bertSim,sentences):
    def predict_input_fn():
        return (tf.data.Dataset.from_generator(
            generate_from_input,
            output_types={
                'input_ids': tf.int32,
                'input_mask': tf.int32,
                'segment_ids': tf.int32,
                'label_ids': tf.int32},
            output_shapes={
                'input_ids': (None, bertSim.max_seq_length),
                'input_mask': (None, bertSim.max_seq_length),
                'segment_ids': (None, bertSim.max_seq_length),
                'label_ids': (1,)}).prefetch(10))

    def generate_from_input():
        processor = bertSim.processor
        predict_examples = processor.get_sentence_examples(sentences)
        features = convert_examples_to_features(predict_examples, processor.get_labels(), args.max_seq_len,
                                                bertSim.tokenizer)
        yield {
            'input_ids': [f.input_ids for f in features],
            'input_mask': [f.input_mask for f in features],
            'segment_ids': [f.segment_ids for f in features],
            'label_ids': [f.label_id for f in features]
        }

    return predict_input_fn


if __name__ == '__main__':
    sim = BertSim()
    sim.start_model()
    sim.predict_sentences([(""我喜欢妈妈做的汤"", ""妈妈做的汤我很喜欢喝"")])
"
Albert,run_pretraining_google.py,"# coding=utf-8
# Copyright 2019 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Lint as: python2, python3
""""""Run masked LM/next sentence masked_lm pre-training for ALBERT.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import time

from six.moves import range
import tensorflow as tf

import modeling_google as modeling
import optimization_google as optimization

flags = tf.flags

FLAGS = flags.FLAGS

## Required parameters
flags.DEFINE_string(
    ""albert_config_file"", None,
    ""The config json file corresponding to the pre-trained ALBERT model. ""
    ""This specifies the model architecture."")

flags.DEFINE_string(
    ""input_file"", None,
    ""Input TF example files (can be a glob or comma separated)."")

flags.DEFINE_string(
    ""output_dir"", None,
    ""The output directory where the model checkpoints will be written."")

flags.DEFINE_string(
    ""export_dir"", None,
    ""The output directory where the saved models will be written."")
## Other parameters
flags.DEFINE_string(
    ""init_checkpoint"", None,
    ""Initial checkpoint (usually from a pre-trained ALBERT model)."")

flags.DEFINE_integer(
    ""max_seq_length"", 512,
    ""The maximum total input sequence length after WordPiece tokenization. ""
    ""Sequences longer than this will be truncated, and sequences shorter ""
    ""than this will be padded. Must match data generation."")

flags.DEFINE_integer(
    ""max_predictions_per_seq"", 20,
    ""Maximum number of masked LM predictions per sequence. ""
    ""Must match data generation."")

flags.DEFINE_bool(""do_train"", True, ""Whether to run training."")

flags.DEFINE_bool(""do_eval"", False, ""Whether to run eval on the dev set."")

flags.DEFINE_integer(""train_batch_size"", 4096, ""Total batch size for training."")

flags.DEFINE_integer(""eval_batch_size"", 64, ""Total batch size for eval."")

flags.DEFINE_enum(""optimizer"", ""lamb"", [""adamw"", ""lamb""],
                  ""The optimizer for training."")

flags.DEFINE_float(""learning_rate"", 0.00176, ""The initial learning rate."")

flags.DEFINE_float(""poly_power"", 1.0, ""The power of poly decay."")

flags.DEFINE_integer(""num_train_steps"", 125000, ""Number of training steps."")

flags.DEFINE_integer(""num_warmup_steps"", 3125, ""Number of warmup steps."")

flags.DEFINE_integer(""start_warmup_step"", 0, ""The starting step of warmup."")

flags.DEFINE_integer(""save_checkpoints_steps"", 5000,
                     ""How often to save the model checkpoint."")

flags.DEFINE_integer(""iterations_per_loop"", 1000,
                     ""How many steps to make in each estimator call."")

flags.DEFINE_integer(""max_eval_steps"", 100, ""Maximum number of eval steps."")

flags.DEFINE_bool(""use_tpu"", False, ""Whether to use TPU or GPU/CPU."")

flags.DEFINE_bool(""init_from_group0"", False, ""Whether to initialize""
                  ""parameters of other groups from group 0"")

tf.flags.DEFINE_string(
    ""tpu_name"", None,
    ""The Cloud TPU to use for training. This should be either the name ""
    ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 ""
    ""url."")

tf.flags.DEFINE_string(
    ""tpu_zone"", None,
    ""[Optional] GCE zone where the Cloud TPU is located in. If not ""
    ""specified, we will attempt to automatically detect the GCE project from ""
    ""metadata."")

tf.flags.DEFINE_string(
    ""gcp_project"", None,
    ""[Optional] Project name for the Cloud TPU-enabled project. If not ""
    ""specified, we will attempt to automatically detect the GCE project from ""
    ""metadata."")

tf.flags.DEFINE_string(""master"", None, ""[Optional] TensorFlow master URL."")

flags.DEFINE_integer(
    ""num_tpu_cores"", 8,
    ""Only used if `use_tpu` is True. Total number of TPU cores to use."")

flags.DEFINE_float(
    ""masked_lm_budget"", 0,
    ""If >0, the ratio of masked ngrams to unmasked ngrams. Default 0,""
    ""for offline masking"")


def model_fn_builder(albert_config, init_checkpoint, learning_rate,
                     num_train_steps, num_warmup_steps, use_tpu,
                     use_one_hot_embeddings, optimizer, poly_power,
                     start_warmup_step):
  """"""Returns `model_fn` closure for TPUEstimator.""""""

  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
    """"""The `model_fn` for TPUEstimator.""""""

    tf.logging.info(""*** Features ***"")
    for name in sorted(features.keys()):
      tf.logging.info(""  name = %s, shape = %s"" % (name, features[name].shape))

    input_ids = features[""input_ids""]
    input_mask = features[""input_mask""]
    segment_ids = features[""segment_ids""]
    masked_lm_positions = features[""masked_lm_positions""]
    masked_lm_ids = features[""masked_lm_ids""]
    masked_lm_weights = features[""masked_lm_weights""]
    # Note: We keep this feature name `next_sentence_labels` to be compatible
    # with the original data created by lanzhzh@. However, in the ALBERT case
    # it does represent sentence_order_labels.
    sentence_order_labels = features[""next_sentence_labels""]

    is_training = (mode == tf.estimator.ModeKeys.TRAIN)

    model = modeling.AlbertModel(
        config=albert_config,
        is_training=is_training,
        input_ids=input_ids,
        input_mask=input_mask,
        token_type_ids=segment_ids,
        use_one_hot_embeddings=use_one_hot_embeddings)

    (masked_lm_loss, masked_lm_example_loss,
     masked_lm_log_probs) = get_masked_lm_output(albert_config,
                                                 model.get_sequence_output(),
                                                 model.get_embedding_table(),
                                                 masked_lm_positions,
                                                 masked_lm_ids,
                                                 masked_lm_weights)

    (sentence_order_loss, sentence_order_example_loss,
     sentence_order_log_probs) = get_sentence_order_output(
         albert_config, model.get_pooled_output(), sentence_order_labels)

    total_loss = masked_lm_loss + sentence_order_loss

    tvars = tf.trainable_variables()

    initialized_variable_names = {}
    scaffold_fn = None
    if init_checkpoint:
      tf.logging.info(""number of hidden group %d to initialize"",
                      albert_config.num_hidden_groups)
      num_of_initialize_group = 1
      if FLAGS.init_from_group0:
        num_of_initialize_group = albert_config.num_hidden_groups
        if albert_config.net_structure_type > 0:
          num_of_initialize_group = albert_config.num_hidden_layers
      (assignment_map, initialized_variable_names
      ) = modeling.get_assignment_map_from_checkpoint(
              tvars, init_checkpoint, num_of_initialize_group)
      if use_tpu:

        def tpu_scaffold():
          for gid in range(num_of_initialize_group):
            tf.logging.info(""initialize the %dth layer"", gid)
            tf.logging.info(assignment_map[gid])
            tf.train.init_from_checkpoint(init_checkpoint, assignment_map[gid])
          return tf.train.Scaffold()

        scaffold_fn = tpu_scaffold
      else:
        for gid in range(num_of_initialize_group):
          tf.logging.info(""initialize the %dth layer"", gid)
          tf.logging.info(assignment_map[gid])
          tf.train.init_from_checkpoint(init_checkpoint, assignment_map[gid])

    tf.logging.info(""**** Trainable Variables ****"")
    for var in tvars:
      init_string = """"
      if var.name in initialized_variable_names:
        init_string = "", *INIT_FROM_CKPT*""
      tf.logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,
                      init_string)

    output_spec = None
    if mode == tf.estimator.ModeKeys.TRAIN:
      train_op = optimization.create_optimizer(
          total_loss, learning_rate, num_train_steps, num_warmup_steps,
          use_tpu, optimizer, poly_power, start_warmup_step)

      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          loss=total_loss,
          train_op=train_op,
          scaffold_fn=scaffold_fn)
    elif mode == tf.estimator.ModeKeys.EVAL:

      def metric_fn(*args):
        """"""Computes the loss and accuracy of the model.""""""
        (masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,
         masked_lm_weights, sentence_order_example_loss,
         sentence_order_log_probs, sentence_order_labels) = args[:7]


        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,
                                         [-1, masked_lm_log_probs.shape[-1]])
        masked_lm_predictions = tf.argmax(
            masked_lm_log_probs, axis=-1, output_type=tf.int32)
        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])
        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])
        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])
        masked_lm_accuracy = tf.metrics.accuracy(
            labels=masked_lm_ids,
            predictions=masked_lm_predictions,
            weights=masked_lm_weights)
        masked_lm_mean_loss = tf.metrics.mean(
            values=masked_lm_example_loss, weights=masked_lm_weights)

        metrics = {
            ""masked_lm_accuracy"": masked_lm_accuracy,
            ""masked_lm_loss"": masked_lm_mean_loss,
        }

        sentence_order_log_probs = tf.reshape(
            sentence_order_log_probs, [-1, sentence_order_log_probs.shape[-1]])
        sentence_order_predictions = tf.argmax(
            sentence_order_log_probs, axis=-1, output_type=tf.int32)
        sentence_order_labels = tf.reshape(sentence_order_labels, [-1])
        sentence_order_accuracy = tf.metrics.accuracy(
            labels=sentence_order_labels,
            predictions=sentence_order_predictions)
        sentence_order_mean_loss = tf.metrics.mean(
            values=sentence_order_example_loss)
        metrics.update({
            ""sentence_order_accuracy"": sentence_order_accuracy,
            ""sentence_order_loss"": sentence_order_mean_loss
        })
        return metrics

      metric_values = [
          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,
          masked_lm_weights, sentence_order_example_loss,
          sentence_order_log_probs, sentence_order_labels
      ]

      eval_metrics = (metric_fn, metric_values)

      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          loss=total_loss,
          eval_metrics=eval_metrics,
          scaffold_fn=scaffold_fn)
    else:
      raise ValueError(""Only TRAIN and EVAL modes are supported: %s"" % (mode))

    return output_spec

  return model_fn


def get_masked_lm_output(albert_config, input_tensor, output_weights, positions,
                         label_ids, label_weights):
  """"""Get loss and log probs for the masked LM.""""""
  input_tensor = gather_indexes(input_tensor, positions)


  with tf.variable_scope(""cls/predictions""):
    # We apply one more non-linear transformation before the output layer.
    # This matrix is not used after pre-training.
    with tf.variable_scope(""transform""):
      input_tensor = tf.layers.dense(
          input_tensor,
          units=albert_config.embedding_size,
          activation=modeling.get_activation(albert_config.hidden_act),
          kernel_initializer=modeling.create_initializer(
              albert_config.initializer_range))
      input_tensor = modeling.layer_norm(input_tensor)

    # The output weights are the same as the input embeddings, but there is
    # an output-only bias for each token.
    output_bias = tf.get_variable(
        ""output_bias"",
        shape=[albert_config.vocab_size],
        initializer=tf.zeros_initializer())
    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)
    logits = tf.nn.bias_add(logits, output_bias)
    log_probs = tf.nn.log_softmax(logits, axis=-1)

    label_ids = tf.reshape(label_ids, [-1])
    label_weights = tf.reshape(label_weights, [-1])

    one_hot_labels = tf.one_hot(
        label_ids, depth=albert_config.vocab_size, dtype=tf.float32)

    # The `positions` tensor might be zero-padded (if the sequence is too
    # short to have the maximum number of predictions). The `label_weights`
    # tensor has a value of 1.0 for every real prediction and 0.0 for the
    # padding predictions.
    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])
    numerator = tf.reduce_sum(label_weights * per_example_loss)
    denominator = tf.reduce_sum(label_weights) + 1e-5
    loss = numerator / denominator

  return (loss, per_example_loss, log_probs)


def get_sentence_order_output(albert_config, input_tensor, labels):
  """"""Get loss and log probs for the next sentence prediction.""""""

  # Simple binary classification. Note that 0 is ""next sentence"" and 1 is
  # ""random sentence"". This weight matrix is not used after pre-training.
  with tf.variable_scope(""cls/seq_relationship""):
    output_weights = tf.get_variable(
        ""output_weights"",
        shape=[2, albert_config.hidden_size],
        initializer=modeling.create_initializer(
            albert_config.initializer_range))
    output_bias = tf.get_variable(
        ""output_bias"", shape=[2], initializer=tf.zeros_initializer())

    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)
    logits = tf.nn.bias_add(logits, output_bias)
    log_probs = tf.nn.log_softmax(logits, axis=-1)
    labels = tf.reshape(labels, [-1])
    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)
    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)
    loss = tf.reduce_mean(per_example_loss)
    return (loss, per_example_loss, log_probs)


def gather_indexes(sequence_tensor, positions):
  """"""Gathers the vectors at the specific positions over a minibatch.""""""
  sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)
  batch_size = sequence_shape[0]
  seq_length = sequence_shape[1]
  width = sequence_shape[2]

  flat_offsets = tf.reshape(
      tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])
  flat_positions = tf.reshape(positions + flat_offsets, [-1])
  flat_sequence_tensor = tf.reshape(sequence_tensor,
                                    [batch_size * seq_length, width])
  output_tensor = tf.gather(flat_sequence_tensor, flat_positions)
  return output_tensor


def input_fn_builder(input_files,
                     max_seq_length,
                     max_predictions_per_seq,
                     is_training,
                     num_cpu_threads=4):
  """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""

  def input_fn(params):
    """"""The actual input function.""""""
    batch_size = params[""batch_size""]

    name_to_features = {
        ""input_ids"": tf.FixedLenFeature([max_seq_length], tf.int64),
        ""input_mask"": tf.FixedLenFeature([max_seq_length], tf.int64),
        ""segment_ids"": tf.FixedLenFeature([max_seq_length], tf.int64),
        # Note: We keep this feature name `next_sentence_labels` to be
        # compatible with the original data created by lanzhzh@. However, in
        # the ALBERT case it does represent sentence_order_labels.
        ""next_sentence_labels"": tf.FixedLenFeature([1], tf.int64),
    }

    if FLAGS.masked_lm_budget:
      name_to_features.update({
          ""token_boundary"":
              tf.FixedLenFeature([max_seq_length], tf.int64)})
    else:
      name_to_features.update({
          ""masked_lm_positions"":
              tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
          ""masked_lm_ids"":
              tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
          ""masked_lm_weights"":
              tf.FixedLenFeature([max_predictions_per_seq], tf.float32)})

    # For training, we want a lot of parallel reading and shuffling.
    # For eval, we want no shuffling and parallel reading doesn't matter.
    if is_training:
      d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))
      d = d.repeat()
      d = d.shuffle(buffer_size=len(input_files))

      # `cycle_length` is the number of parallel files that get read.
      cycle_length = min(num_cpu_threads, len(input_files))

      # `sloppy` mode means that the interleaving is not exact. This adds
      # even more randomness to the training pipeline.
      d = d.apply(
          tf.contrib.data.parallel_interleave(
              tf.data.TFRecordDataset,
              sloppy=is_training,
              cycle_length=cycle_length))
      d = d.shuffle(buffer_size=100)
    else:
      d = tf.data.TFRecordDataset(input_files)
      # Since we evaluate for a fixed number of steps we don't want to encounter
      # out-of-range exceptions.
      d = d.repeat()

    # We must `drop_remainder` on training because the TPU requires fixed
    # size dimensions. For eval, we assume we are evaluating on the CPU or GPU
    # and we *don't* want to drop the remainder, otherwise we wont cover
    # every sample.
    d = d.apply(
        tf.data.experimental.map_and_batch_with_legacy_function(
            lambda record: _decode_record(record, name_to_features),
            batch_size=batch_size,
            num_parallel_batches=num_cpu_threads,
            drop_remainder=True))
    tf.logging.info(d)
    return d

  return input_fn


def _decode_record(record, name_to_features):
  """"""Decodes a record to a TensorFlow example.""""""
  example = tf.parse_single_example(record, name_to_features)

  # tf.Example only supports tf.int64, but the TPU only supports tf.int32.
  # So cast all int64 to int32.
  for name in list(example.keys()):
    t = example[name]
    if t.dtype == tf.int64:
      t = tf.to_int32(t)
    example[name] = t

  return example


def main(_):
  tf.logging.set_verbosity(tf.logging.INFO)

  if not FLAGS.do_train and not FLAGS.do_eval:
    raise ValueError(""At least one of `do_train` or `do_eval` must be True."")

  albert_config = modeling.AlbertConfig.from_json_file(FLAGS.albert_config_file)

  tf.gfile.MakeDirs(FLAGS.output_dir)

  input_files = []
  for input_pattern in FLAGS.input_file.split("",""):
    input_files.extend(tf.gfile.Glob(input_pattern))

  tf.logging.info(""*** Input Files ***"")
  for input_file in input_files:
    tf.logging.info(""  %s"" % input_file)

  tpu_cluster_resolver = None
  if FLAGS.use_tpu and FLAGS.tpu_name:
    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)

  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2
  run_config = tf.contrib.tpu.RunConfig(
      cluster=tpu_cluster_resolver,
      master=FLAGS.master,
      model_dir=FLAGS.output_dir,
      save_checkpoints_steps=FLAGS.save_checkpoints_steps,
      tpu_config=tf.contrib.tpu.TPUConfig(
          iterations_per_loop=FLAGS.iterations_per_loop,
          num_shards=FLAGS.num_tpu_cores,
          per_host_input_for_training=is_per_host))

  model_fn = model_fn_builder(
      albert_config=albert_config,
      init_checkpoint=FLAGS.init_checkpoint,
      learning_rate=FLAGS.learning_rate,
      num_train_steps=FLAGS.num_train_steps,
      num_warmup_steps=FLAGS.num_warmup_steps,
      use_tpu=FLAGS.use_tpu,
      use_one_hot_embeddings=FLAGS.use_tpu,
      optimizer=FLAGS.optimizer,
      poly_power=FLAGS.poly_power,
      start_warmup_step=FLAGS.start_warmup_step)

  # If TPU is not available, this will fall back to normal Estimator on CPU
  # or GPU.
  estimator = tf.contrib.tpu.TPUEstimator(
      use_tpu=FLAGS.use_tpu,
      model_fn=model_fn,
      config=run_config,
      train_batch_size=FLAGS.train_batch_size,
      eval_batch_size=FLAGS.eval_batch_size)

  if FLAGS.do_train:
    tf.logging.info(""***** Running training *****"")
    tf.logging.info(""  Batch size = %d"", FLAGS.train_batch_size)
    train_input_fn = input_fn_builder(
        input_files=input_files,
        max_seq_length=FLAGS.max_seq_length,
        max_predictions_per_seq=FLAGS.max_predictions_per_seq,
        is_training=True)
    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)

  if FLAGS.do_eval:
    tf.logging.info(""***** Running evaluation *****"")
    tf.logging.info(""  Batch size = %d"", FLAGS.eval_batch_size)
    global_step = -1
    output_eval_file = os.path.join(FLAGS.output_dir, ""eval_results.txt"")
    writer = tf.gfile.GFile(output_eval_file, ""w"")
    tf.gfile.MakeDirs(FLAGS.export_dir)
    eval_input_fn = input_fn_builder(
        input_files=input_files,
        max_seq_length=FLAGS.max_seq_length,
        max_predictions_per_seq=FLAGS.max_predictions_per_seq,
        is_training=False)
    while global_step < FLAGS.num_train_steps:
      if estimator.latest_checkpoint() is None:
        tf.logging.info(""No checkpoint found yet. Sleeping."")
        time.sleep(1)
      else:
        result = estimator.evaluate(
            input_fn=eval_input_fn, steps=FLAGS.max_eval_steps)
        global_step = result[""global_step""]
        tf.logging.info(""***** Eval results *****"")
        for key in sorted(result.keys()):
          tf.logging.info(""  %s = %s"", key, str(result[key]))
          writer.write(""%s = %s\n"" % (key, str(result[key])))

if __name__ == ""__main__"":
  flags.mark_flag_as_required(""input_file"")
  flags.mark_flag_as_required(""albert_config_file"")
  flags.mark_flag_as_required(""output_dir"")
  tf.app.run()"
Albert,bert_utils.py,"from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import copy
import json
import math
import re
import six
import tensorflow as tf

def get_shape_list(tensor, expected_rank=None, name=None):
	""""""Returns a list of the shape of tensor, preferring static dimensions.

	Args:
		tensor: A tf.Tensor object to find the shape of.
		expected_rank: (optional) int. The expected rank of `tensor`. If this is
			specified and the `tensor` has a different rank, and exception will be
			thrown.
		name: Optional name of the tensor for the error message.

	Returns:
		A list of dimensions of the shape of tensor. All static dimensions will
		be returned as python integers, and dynamic dimensions will be returned
		as tf.Tensor scalars.
	""""""
	if name is None:
		name = tensor.name

	if expected_rank is not None:
		assert_rank(tensor, expected_rank, name)

	shape = tensor.shape.as_list()

	non_static_indexes = []
	for (index, dim) in enumerate(shape):
		if dim is None:
			non_static_indexes.append(index)

	if not non_static_indexes:
		return shape

	dyn_shape = tf.shape(tensor)
	for index in non_static_indexes:
		shape[index] = dyn_shape[index]
	return shape

def reshape_to_matrix(input_tensor):
	""""""Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).""""""
	ndims = input_tensor.shape.ndims
	if ndims < 2:
		raise ValueError(""Input tensor must have at least rank 2. Shape = %s"" %
										 (input_tensor.shape))
	if ndims == 2:
		return input_tensor

	width = input_tensor.shape[-1]
	output_tensor = tf.reshape(input_tensor, [-1, width])
	return output_tensor

def reshape_from_matrix(output_tensor, orig_shape_list):
	""""""Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""""""
	if len(orig_shape_list) == 2:
		return output_tensor

	output_shape = get_shape_list(output_tensor)

	orig_dims = orig_shape_list[0:-1]
	width = output_shape[-1]

	return tf.reshape(output_tensor, orig_dims + [width])

def assert_rank(tensor, expected_rank, name=None):
	""""""Raises an exception if the tensor rank is not of the expected rank.

	Args:
		tensor: A tf.Tensor to check the rank of.
		expected_rank: Python integer or list of integers, expected rank.
		name: Optional name of the tensor for the error message.

	Raises:
		ValueError: If the expected shape doesn't match the actual shape.
	""""""
	if name is None:
		name = tensor.name

	expected_rank_dict = {}
	if isinstance(expected_rank, six.integer_types):
		expected_rank_dict[expected_rank] = True
	else:
		for x in expected_rank:
			expected_rank_dict[x] = True

	actual_rank = tensor.shape.ndims
	if actual_rank not in expected_rank_dict:
		scope_name = tf.get_variable_scope().name
		raise ValueError(
				""For the tensor `%s` in scope `%s`, the actual rank ""
				""`%d` (shape = %s) is not equal to the expected rank `%s`"" %
				(name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))

def gather_indexes(sequence_tensor, positions):
	""""""Gathers the vectors at the specific positions over a minibatch.""""""
	sequence_shape = get_shape_list(sequence_tensor, expected_rank=3)
	batch_size = sequence_shape[0]
	seq_length = sequence_shape[1]
	width = sequence_shape[2]

	flat_offsets = tf.reshape(
			tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])
	flat_positions = tf.reshape(positions + flat_offsets, [-1])
	flat_sequence_tensor = tf.reshape(sequence_tensor,
																		[batch_size * seq_length, width])
	output_tensor = tf.gather(flat_sequence_tensor, flat_positions)
	return output_tensor

# add sequence mask for:
# 1. random shuffle lm modeling---xlnet with random shuffled input
# 2. left2right and right2left language modeling
# 3. conditional generation
def generate_seq2seq_mask(attention_mask, mask_sequence, seq_type, **kargs):
	if seq_type == 'seq2seq':
		if mask_sequence is not None:
			seq_shape = get_shape_list(mask_sequence, expected_rank=2)
			seq_len = seq_shape[1]
			ones = tf.ones((1, seq_len, seq_len))
			a_mask = tf.matrix_band_part(ones, -1, 0)
			s_ex12 = tf.expand_dims(tf.expand_dims(mask_sequence, 1), 2)
			s_ex13 = tf.expand_dims(tf.expand_dims(mask_sequence, 1), 3)
			a_mask = (1 - s_ex13) * (1 - s_ex12) + s_ex13 * a_mask
			# generate mask of batch x seq_len x seq_len
			a_mask = tf.reshape(a_mask, (-1, seq_len, seq_len))
			out_mask = attention_mask * a_mask
		else:
			ones = tf.ones_like(attention_mask[:1])
			mask = (tf.matrix_band_part(ones, -1, 0))
			out_mask = attention_mask * mask
	else:
		out_mask = attention_mask

	return out_mask

"
Albert,modeling_google.py,"# coding=utf-8
# Copyright 2019 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Lint as: python2, python3
""""""The main ALBERT model and related functions.
For a description of the algorithm, see https://arxiv.org/abs/1909.11942.
""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import copy
import json
import math
import re
import numpy as np
import six
from six.moves import range
import tensorflow as tf


class AlbertConfig(object):
  """"""Configuration for `AlbertModel`.
  The default settings match the configuration of model `albert_xxlarge`.
  """"""

  def __init__(self,
               vocab_size,
               embedding_size=128,
               hidden_size=4096,
               num_hidden_layers=12,
               num_hidden_groups=1,
               num_attention_heads=64,
               intermediate_size=16384,
               inner_group_num=1,
               down_scale_factor=1,
               hidden_act=""gelu"",
               hidden_dropout_prob=0,
               attention_probs_dropout_prob=0,
               max_position_embeddings=512,
               type_vocab_size=2,
               initializer_range=0.02):
    """"""Constructs AlbertConfig.
    Args:
      vocab_size: Vocabulary size of `inputs_ids` in `AlbertModel`.
      embedding_size: size of voc embeddings.
      hidden_size: Size of the encoder layers and the pooler layer.
      num_hidden_layers: Number of hidden layers in the Transformer encoder.
      num_hidden_groups: Number of group for the hidden layers, parameters in
        the same group are shared.
      num_attention_heads: Number of attention heads for each attention layer in
        the Transformer encoder.
      intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)
        layer in the Transformer encoder.
      inner_group_num: int, number of inner repetition of attention and ffn.
      down_scale_factor: float, the scale to apply
      hidden_act: The non-linear activation function (function or string) in the
        encoder and pooler.
      hidden_dropout_prob: The dropout probability for all fully connected
        layers in the embeddings, encoder, and pooler.
      attention_probs_dropout_prob: The dropout ratio for the attention
        probabilities.
      max_position_embeddings: The maximum sequence length that this model might
        ever be used with. Typically set this to something large just in case
        (e.g., 512 or 1024 or 2048).
      type_vocab_size: The vocabulary size of the `token_type_ids` passed into
        `AlbertModel`.
      initializer_range: The stdev of the truncated_normal_initializer for
        initializing all weight matrices.
    """"""
    self.vocab_size = vocab_size
    self.embedding_size = embedding_size
    self.hidden_size = hidden_size
    self.num_hidden_layers = num_hidden_layers
    self.num_hidden_groups = num_hidden_groups
    self.num_attention_heads = num_attention_heads
    self.inner_group_num = inner_group_num
    self.down_scale_factor = down_scale_factor
    self.hidden_act = hidden_act
    self.intermediate_size = intermediate_size
    self.hidden_dropout_prob = hidden_dropout_prob
    self.attention_probs_dropout_prob = attention_probs_dropout_prob
    self.max_position_embeddings = max_position_embeddings
    self.type_vocab_size = type_vocab_size
    self.initializer_range = initializer_range

  @classmethod
  def from_dict(cls, json_object):
    """"""Constructs a `AlbertConfig` from a Python dictionary of parameters.""""""
    config = AlbertConfig(vocab_size=None)
    for (key, value) in six.iteritems(json_object):
      config.__dict__[key] = value
    return config

  @classmethod
  def from_json_file(cls, json_file):
    """"""Constructs a `AlbertConfig` from a json file of parameters.""""""
    with tf.gfile.GFile(json_file, ""r"") as reader:
      text = reader.read()
    return cls.from_dict(json.loads(text))

  def to_dict(self):
    """"""Serializes this instance to a Python dictionary.""""""
    output = copy.deepcopy(self.__dict__)
    return output

  def to_json_string(self):
    """"""Serializes this instance to a JSON string.""""""
    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\n""


class AlbertModel(object):
  """"""BERT model (""Bidirectional Encoder Representations from Transformers"").
  Example usage:
  ```python
  # Already been converted from strings into ids
  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])
  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])
  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])
  config = modeling.AlbertConfig(vocab_size=32000, hidden_size=512,
    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)
  model = modeling.AlbertModel(config=config, is_training=True,
    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)
  label_embeddings = tf.get_variable(...)
  pooled_output = model.get_pooled_output()
  logits = tf.matmul(pooled_output, label_embeddings)
  ...
  ```
  """"""

  def __init__(self,
               config,
               is_training,
               input_ids,
               input_mask=None,
               token_type_ids=None,
               use_one_hot_embeddings=False,
               scope=None):
    """"""Constructor for AlbertModel.
    Args:
      config: `AlbertConfig` instance.
      is_training: bool. true for training model, false for eval model. Controls
        whether dropout will be applied.
      input_ids: int32 Tensor of shape [batch_size, seq_length].
      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].
      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].
      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word
        embeddings or tf.embedding_lookup() for the word embeddings.
      scope: (optional) variable scope. Defaults to ""bert"".
    Raises:
      ValueError: The config is invalid or one of the input tensor shapes
        is invalid.
    """"""
    config = copy.deepcopy(config)
    if not is_training:
      config.hidden_dropout_prob = 0.0
      config.attention_probs_dropout_prob = 0.0

    input_shape = get_shape_list(input_ids, expected_rank=2)
    batch_size = input_shape[0]
    seq_length = input_shape[1]

    if input_mask is None:
      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)

    if token_type_ids is None:
      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)

    with tf.variable_scope(scope, default_name=""bert""):
      with tf.variable_scope(""embeddings""):
        # Perform embedding lookup on the word ids.
        (self.word_embedding_output,
         self.output_embedding_table) = embedding_lookup(
            input_ids=input_ids,
            vocab_size=config.vocab_size,
            embedding_size=config.embedding_size,
            initializer_range=config.initializer_range,
            word_embedding_name=""word_embeddings"",
            use_one_hot_embeddings=use_one_hot_embeddings)

        # Add positional embeddings and token type embeddings, then layer
        # normalize and perform dropout.
        self.embedding_output = embedding_postprocessor(
            input_tensor=self.word_embedding_output,
            use_token_type=True,
            token_type_ids=token_type_ids,
            token_type_vocab_size=config.type_vocab_size,
            token_type_embedding_name=""token_type_embeddings"",
            use_position_embeddings=True,
            position_embedding_name=""position_embeddings"",
            initializer_range=config.initializer_range,
            max_position_embeddings=config.max_position_embeddings,
            dropout_prob=config.hidden_dropout_prob)

      with tf.variable_scope(""encoder""):

        # Run the stacked transformer.
        # `sequence_output` shape = [batch_size, seq_length, hidden_size].
        self.all_encoder_layers = transformer_model(
            input_tensor=self.embedding_output,
            attention_mask=input_mask,
            hidden_size=config.hidden_size,
            num_hidden_layers=config.num_hidden_layers,
            num_hidden_groups=config.num_hidden_groups,
            num_attention_heads=config.num_attention_heads,
            intermediate_size=config.intermediate_size,
            inner_group_num=config.inner_group_num,
            intermediate_act_fn=get_activation(config.hidden_act),
            hidden_dropout_prob=config.hidden_dropout_prob,
            attention_probs_dropout_prob=config.attention_probs_dropout_prob,
            initializer_range=config.initializer_range,
            do_return_all_layers=True)

      self.sequence_output = self.all_encoder_layers[-1]
      # The ""pooler"" converts the encoded sequence tensor of shape
      # [batch_size, seq_length, hidden_size] to a tensor of shape
      # [batch_size, hidden_size]. This is necessary for segment-level
      # (or segment-pair-level) classification tasks where we need a fixed
      # dimensional representation of the segment.
      with tf.variable_scope(""pooler""):
        # We ""pool"" the model by simply taking the hidden state corresponding
        # to the first token. We assume that this has been pre-trained
        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)
        self.pooled_output = tf.layers.dense(
            first_token_tensor,
            config.hidden_size,
            activation=tf.tanh,
            kernel_initializer=create_initializer(config.initializer_range))

  def get_pooled_output(self):
    return self.pooled_output

  def get_sequence_output(self):
    """"""Gets final hidden layer of encoder.
    Returns:
      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
      to the final hidden of the transformer encoder.
    """"""
    return self.sequence_output

  def get_all_encoder_layers(self):
    return self.all_encoder_layers

  def get_word_embedding_output(self):
    """"""Get output of the word(piece) embedding lookup.
    This is BEFORE positional embeddings and token type embeddings have been
    added.
    Returns:
      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
      to the output of the word(piece) embedding layer.
    """"""
    return self.word_embedding_output

  def get_embedding_output(self):
    """"""Gets output of the embedding lookup (i.e., input to the transformer).
    Returns:
      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
      to the output of the embedding layer, after summing the word
      embeddings with the positional embeddings and the token type embeddings,
      then performing layer normalization. This is the input to the transformer.
    """"""
    return self.embedding_output

  def get_embedding_table(self):
    return self.output_embedding_table


def gelu(x):
  """"""Gaussian Error Linear Unit.
  This is a smoother version of the RELU.
  Original paper: https://arxiv.org/abs/1606.08415
  Args:
    x: float Tensor to perform activation.
  Returns:
    `x` with the GELU activation applied.
  """"""
  cdf = 0.5 * (1.0 + tf.tanh(
      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))
  return x * cdf


def get_activation(activation_string):
  """"""Maps a string to a Python function, e.g., ""relu"" => `tf.nn.relu`.
  Args:
    activation_string: String name of the activation function.
  Returns:
    A Python function corresponding to the activation function. If
    `activation_string` is None, empty, or ""linear"", this will return None.
    If `activation_string` is not a string, it will return `activation_string`.
  Raises:
    ValueError: The `activation_string` does not correspond to a known
      activation.
  """"""

  # We assume that anything that""s not a string is already an activation
  # function, so we just return it.
  if not isinstance(activation_string, six.string_types):
    return activation_string

  if not activation_string:
    return None

  act = activation_string.lower()
  if act == ""linear"":
    return None
  elif act == ""relu"":
    return tf.nn.relu
  elif act == ""gelu"":
    return gelu
  elif act == ""tanh"":
    return tf.tanh
  else:
    raise ValueError(""Unsupported activation: %s"" % act)


def get_assignment_map_from_checkpoint(tvars, init_checkpoint, num_of_group=0):
  """"""Compute the union of the current variables and checkpoint variables.""""""
  assignment_map = {}
  initialized_variable_names = {}

  name_to_variable = collections.OrderedDict()
  for var in tvars:
    name = var.name
    m = re.match(""^(.*):\\d+$"", name)
    if m is not None:
      name = m.group(1)
    name_to_variable[name] = var
  init_vars = tf.train.list_variables(init_checkpoint)
  init_vars_name = [name for (name, _) in init_vars]

  if num_of_group > 0:
    assignment_map = []
    for gid in range(num_of_group):
      assignment_map.append(collections.OrderedDict())
  else:
    assignment_map = collections.OrderedDict()

  for name in name_to_variable:
    if name in init_vars_name:
      tvar_name = name
    elif (re.sub(r""/group_\d+/"", ""/group_0/"",
                 six.ensure_str(name)) in init_vars_name and
          num_of_group > 1):
      tvar_name = re.sub(r""/group_\d+/"", ""/group_0/"", six.ensure_str(name))
    elif (re.sub(r""/ffn_\d+/"", ""/ffn_1/"", six.ensure_str(name))
          in init_vars_name and num_of_group > 1):
      tvar_name = re.sub(r""/ffn_\d+/"", ""/ffn_1/"", six.ensure_str(name))
    elif (re.sub(r""/attention_\d+/"", ""/attention_1/"", six.ensure_str(name))
          in init_vars_name and num_of_group > 1):
      tvar_name = re.sub(r""/attention_\d+/"", ""/attention_1/"",
                         six.ensure_str(name))
    else:
      tf.logging.info(""name %s does not get matched"", name)
      continue
    tf.logging.info(""name %s match to %s"", name, tvar_name)
    if num_of_group > 0:
      group_matched = False
      for gid in range(1, num_of_group):
        if ((""/group_"" + str(gid) + ""/"" in name) or
            (""/ffn_"" + str(gid) + ""/"" in name) or
            (""/attention_"" + str(gid) + ""/"" in name)):
          group_matched = True
          tf.logging.info(""%s belongs to %dth"", name, gid)
          assignment_map[gid][tvar_name] = name
      if not group_matched:
        assignment_map[0][tvar_name] = name
    else:
      assignment_map[tvar_name] = name
    initialized_variable_names[name] = 1
    initialized_variable_names[six.ensure_str(name) + "":0""] = 1

  return (assignment_map, initialized_variable_names)


def dropout(input_tensor, dropout_prob):
  """"""Perform dropout.
  Args:
    input_tensor: float Tensor.
    dropout_prob: Python float. The probability of dropping out a value (NOT of
      *keeping* a dimension as in `tf.nn.dropout`).
  Returns:
    A version of `input_tensor` with dropout applied.
  """"""
  if dropout_prob is None or dropout_prob == 0.0:
    return input_tensor

  output = tf.nn.dropout(input_tensor, rate=dropout_prob)
  return output


def layer_norm(input_tensor, name=None):
  """"""Run layer normalization on the last dimension of the tensor.""""""
  return tf.contrib.layers.layer_norm(
      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)


def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):
  """"""Runs layer normalization followed by dropout.""""""
  output_tensor = layer_norm(input_tensor, name)
  output_tensor = dropout(output_tensor, dropout_prob)
  return output_tensor


def create_initializer(initializer_range=0.02):
  """"""Creates a `truncated_normal_initializer` with the given range.""""""
  return tf.truncated_normal_initializer(stddev=initializer_range)


def get_timing_signal_1d_given_position(channels,
                                        position,
                                        min_timescale=1.0,
                                        max_timescale=1.0e4):
  """"""Get sinusoids of diff frequencies, with timing position given.
  Adapted from add_timing_signal_1d_given_position in
  //third_party/py/tensor2tensor/layers/common_attention.py
  Args:
    channels: scalar, size of timing embeddings to create. The number of
        different timescales is equal to channels / 2.
    position: a Tensor with shape [batch, seq_len]
    min_timescale: a float
    max_timescale: a float
  Returns:
    a Tensor of timing signals [batch, seq_len, channels]
  """"""
  num_timescales = channels // 2
  log_timescale_increment = (
      math.log(float(max_timescale) / float(min_timescale)) /
      (tf.to_float(num_timescales) - 1))
  inv_timescales = min_timescale * tf.exp(
      tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)
  scaled_time = (
      tf.expand_dims(tf.to_float(position), 2) * tf.expand_dims(
          tf.expand_dims(inv_timescales, 0), 0))
  signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=2)
  signal = tf.pad(signal, [[0, 0], [0, 0], [0, tf.mod(channels, 2)]])
  return signal


def embedding_lookup(input_ids,
                     vocab_size,
                     embedding_size=128,
                     initializer_range=0.02,
                     word_embedding_name=""word_embeddings"",
                     use_one_hot_embeddings=False):
  """"""Looks up words embeddings for id tensor.
  Args:
    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word
      ids.
    vocab_size: int. Size of the embedding vocabulary.
    embedding_size: int. Width of the word embeddings.
    initializer_range: float. Embedding initialization range.
    word_embedding_name: string. Name of the embedding table.
    use_one_hot_embeddings: bool. If True, use one-hot method for word
      embeddings. If False, use `tf.nn.embedding_lookup()`.
  Returns:
    float Tensor of shape [batch_size, seq_length, embedding_size].
  """"""
  # This function assumes that the input is of shape [batch_size, seq_length,
  # num_inputs].
  #
  # If the input is a 2D tensor of shape [batch_size, seq_length], we
  # reshape to [batch_size, seq_length, 1].
  if input_ids.shape.ndims == 2:
    input_ids = tf.expand_dims(input_ids, axis=[-1])

  embedding_table = tf.get_variable(
      name=word_embedding_name,
      shape=[vocab_size, embedding_size],
      initializer=create_initializer(initializer_range))

  if use_one_hot_embeddings:
    flat_input_ids = tf.reshape(input_ids, [-1])
    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)
    output = tf.matmul(one_hot_input_ids, embedding_table)
  else:
    output = tf.nn.embedding_lookup(embedding_table, input_ids)

  input_shape = get_shape_list(input_ids)

  output = tf.reshape(output,
                      input_shape[0:-1] + [input_shape[-1] * embedding_size])
  return (output, embedding_table)


def embedding_postprocessor(input_tensor,
                            use_token_type=False,
                            token_type_ids=None,
                            token_type_vocab_size=16,
                            token_type_embedding_name=""token_type_embeddings"",
                            use_position_embeddings=True,
                            position_embedding_name=""position_embeddings"",
                            initializer_range=0.02,
                            max_position_embeddings=512,
                            dropout_prob=0.1):
  """"""Performs various post-processing on a word embedding tensor.
  Args:
    input_tensor: float Tensor of shape [batch_size, seq_length,
      embedding_size].
    use_token_type: bool. Whether to add embeddings for `token_type_ids`.
    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].
      Must be specified if `use_token_type` is True.
    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.
    token_type_embedding_name: string. The name of the embedding table variable
      for token type ids.
    use_position_embeddings: bool. Whether to add position embeddings for the
      position of each token in the sequence.
    position_embedding_name: string. The name of the embedding table variable
      for positional embeddings.
    initializer_range: float. Range of the weight initialization.
    max_position_embeddings: int. Maximum sequence length that might ever be
      used with this model. This can be longer than the sequence length of
      input_tensor, but cannot be shorter.
    dropout_prob: float. Dropout probability applied to the final output tensor.
  Returns:
    float tensor with same shape as `input_tensor`.
  Raises:
    ValueError: One of the tensor shapes or input values is invalid.
  """"""
  input_shape = get_shape_list(input_tensor, expected_rank=3)
  batch_size = input_shape[0]
  seq_length = input_shape[1]
  width = input_shape[2]

  output = input_tensor

  if use_token_type:
    if token_type_ids is None:
      raise ValueError(""`token_type_ids` must be specified if""
                       ""`use_token_type` is True."")
    token_type_table = tf.get_variable(
        name=token_type_embedding_name,
        shape=[token_type_vocab_size, width],
        initializer=create_initializer(initializer_range))
    # This vocab will be small so we always do one-hot here, since it is always
    # faster for a small vocabulary.
    flat_token_type_ids = tf.reshape(token_type_ids, [-1])
    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)
    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)
    token_type_embeddings = tf.reshape(token_type_embeddings,
                                       [batch_size, seq_length, width])
    output += token_type_embeddings

  if use_position_embeddings:
    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)
    with tf.control_dependencies([assert_op]):
      full_position_embeddings = tf.get_variable(
          name=position_embedding_name,
          shape=[max_position_embeddings, width],
          initializer=create_initializer(initializer_range))
      # Since the position embedding table is a learned variable, we create it
      # using a (long) sequence length `max_position_embeddings`. The actual
      # sequence length might be shorter than this, for faster training of
      # tasks that do not have long sequences.
      #
      # So `full_position_embeddings` is effectively an embedding table
      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current
      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just
      # perform a slice.
      position_embeddings = tf.slice(full_position_embeddings, [0, 0],
                                     [seq_length, -1])
      num_dims = len(output.shape.as_list())

      # Only the last two dimensions are relevant (`seq_length` and `width`), so
      # we broadcast among the first dimensions, which is typically just
      # the batch size.
      position_broadcast_shape = []
      for _ in range(num_dims - 2):
        position_broadcast_shape.append(1)
      position_broadcast_shape.extend([seq_length, width])
      position_embeddings = tf.reshape(position_embeddings,
                                       position_broadcast_shape)
      output += position_embeddings

  output = layer_norm_and_dropout(output, dropout_prob)
  return output


def dense_layer_3d(input_tensor,
                   num_attention_heads,
                   head_size,
                   initializer,
                   activation,
                   name=None):
  """"""A dense layer with 3D kernel.
  Args:
    input_tensor: float Tensor of shape [batch, seq_length, hidden_size].
    num_attention_heads: Number of attention heads.
    head_size: The size per attention head.
    initializer: Kernel initializer.
    activation: Actication function.
    name: The name scope of this layer.
  Returns:
    float logits Tensor.
  """"""

  input_shape = get_shape_list(input_tensor)
  hidden_size = input_shape[2]

  with tf.variable_scope(name):
    w = tf.get_variable(
        name=""kernel"",
        shape=[hidden_size, num_attention_heads * head_size],
        initializer=initializer)
    w = tf.reshape(w, [hidden_size, num_attention_heads, head_size])
    b = tf.get_variable(
        name=""bias"",
        shape=[num_attention_heads * head_size],
        initializer=tf.zeros_initializer)
    b = tf.reshape(b, [num_attention_heads, head_size])
    ret = tf.einsum(""BFH,HND->BFND"", input_tensor, w)
    ret += b
  if activation is not None:
    return activation(ret)
  else:
    return ret


def dense_layer_3d_proj(input_tensor,
                        hidden_size,
                        head_size,
                        initializer,
                        activation,
                        name=None):
  """"""A dense layer with 3D kernel for projection.
  Args:
    input_tensor: float Tensor of shape [batch,from_seq_length,
      num_attention_heads, size_per_head].
    hidden_size: The size of hidden layer.
    num_attention_heads: The size of output dimension.
    head_size: The size of head.
    initializer: Kernel initializer.
    activation: Actication function.
    name: The name scope of this layer.
  Returns:
    float logits Tensor.
  """"""
  input_shape = get_shape_list(input_tensor)
  num_attention_heads= input_shape[2]
  with tf.variable_scope(name):
    w = tf.get_variable(
        name=""kernel"",
        shape=[num_attention_heads * head_size, hidden_size],
        initializer=initializer)
    w = tf.reshape(w, [num_attention_heads, head_size, hidden_size])
    b = tf.get_variable(
        name=""bias"", shape=[hidden_size], initializer=tf.zeros_initializer)
    ret = tf.einsum(""BFND,NDH->BFH"", input_tensor, w)
    ret += b
  if activation is not None:
    return activation(ret)
  else:
    return ret


def dense_layer_2d(input_tensor,
                   output_size,
                   initializer,
                   activation,
                   num_attention_heads=1,
                   name=None):
  """"""A dense layer with 2D kernel.
  Args:
    input_tensor: Float tensor with rank 3.
    output_size: The size of output dimension.
    initializer: Kernel initializer.
    activation: Activation function.
    num_attention_heads: number of attention head in attention layer.
    name: The name scope of this layer.
  Returns:
    float logits Tensor.
  """"""
  del num_attention_heads  # unused
  input_shape = get_shape_list(input_tensor)
  hidden_size = input_shape[2]
  with tf.variable_scope(name):
    w = tf.get_variable(
        name=""kernel"",
        shape=[hidden_size, output_size],
        initializer=initializer)
    b = tf.get_variable(
        name=""bias"", shape=[output_size], initializer=tf.zeros_initializer)
    ret = tf.einsum(""BFH,HO->BFO"", input_tensor, w)
    ret += b
  if activation is not None:
    return activation(ret)
  else:
    return ret


def dot_product_attention(q, k, v, bias, dropout_rate=0.0):
  """"""Dot-product attention.
  Args:
    q: Tensor with shape [..., length_q, depth_k].
    k: Tensor with shape [..., length_kv, depth_k]. Leading dimensions must
      match with q.
    v: Tensor with shape [..., length_kv, depth_v] Leading dimensions must
      match with q.
    bias: bias Tensor (see attention_bias())
    dropout_rate: a float.
  Returns:
    Tensor with shape [..., length_q, depth_v].
  """"""
  logits = tf.matmul(q, k, transpose_b=True)  # [..., length_q, length_kv]
  logits = tf.multiply(logits, 1.0 / math.sqrt(float(get_shape_list(q)[-1])))
  if bias is not None:
    # `attention_mask` = [B, T]
    from_shape = get_shape_list(q)
    if len(from_shape) == 4:
      broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], 1], tf.float32)
    elif len(from_shape) == 5:
      # from_shape = [B, N, Block_num, block_size, depth]#
      broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], from_shape[3],
                                1], tf.float32)

    bias = tf.matmul(broadcast_ones,
                     tf.cast(bias, tf.float32), transpose_b=True)

    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
    # masked positions, this operation will create a tensor which is 0.0 for
    # positions we want to attend and -10000.0 for masked positions.
    adder = (1.0 - bias) * -10000.0

    # Since we are adding it to the raw scores before the softmax, this is
    # effectively the same as removing these entirely.
    logits += adder
  else:
    adder = 0.0

  attention_probs = tf.nn.softmax(logits, name=""attention_probs"")
  attention_probs = dropout(attention_probs, dropout_rate)
  return tf.matmul(attention_probs, v)


def attention_layer(from_tensor,
                    to_tensor,
                    attention_mask=None,
                    num_attention_heads=1,
                    query_act=None,
                    key_act=None,
                    value_act=None,
                    attention_probs_dropout_prob=0.0,
                    initializer_range=0.02,
                    batch_size=None,
                    from_seq_length=None,
                    to_seq_length=None):
  """"""Performs multi-headed attention from `from_tensor` to `to_tensor`.
  Args:
    from_tensor: float Tensor of shape [batch_size, from_seq_length,
      from_width].
    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].
    attention_mask: (optional) int32 Tensor of shape [batch_size,
      from_seq_length, to_seq_length]. The values should be 1 or 0. The
      attention scores will effectively be set to -infinity for any positions in
      the mask that are 0, and will be unchanged for positions that are 1.
    num_attention_heads: int. Number of attention heads.
    query_act: (optional) Activation function for the query transform.
    key_act: (optional) Activation function for the key transform.
    value_act: (optional) Activation function for the value transform.
    attention_probs_dropout_prob: (optional) float. Dropout probability of the
      attention probabilities.
    initializer_range: float. Range of the weight initializer.
    batch_size: (Optional) int. If the input is 2D, this might be the batch size
      of the 3D version of the `from_tensor` and `to_tensor`.
    from_seq_length: (Optional) If the input is 2D, this might be the seq length
      of the 3D version of the `from_tensor`.
    to_seq_length: (Optional) If the input is 2D, this might be the seq length
      of the 3D version of the `to_tensor`.
  Returns:
    float Tensor of shape [batch_size, from_seq_length, num_attention_heads,
      size_per_head].
  Raises:
    ValueError: Any of the arguments or tensor shapes are invalid.
  """"""
  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])
  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])
  size_per_head = int(from_shape[2]/num_attention_heads)

  if len(from_shape) != len(to_shape):
    raise ValueError(
        ""The rank of `from_tensor` must match the rank of `to_tensor`."")

  if len(from_shape) == 3:
    batch_size = from_shape[0]
    from_seq_length = from_shape[1]
    to_seq_length = to_shape[1]
  elif len(from_shape) == 2:
    if (batch_size is None or from_seq_length is None or to_seq_length is None):
      raise ValueError(
          ""When passing in rank 2 tensors to attention_layer, the values ""
          ""for `batch_size`, `from_seq_length`, and `to_seq_length` ""
          ""must all be specified."")

  # Scalar dimensions referenced here:
  #   B = batch size (number of sequences)
  #   F = `from_tensor` sequence length
  #   T = `to_tensor` sequence length
  #   N = `num_attention_heads`
  #   H = `size_per_head`

  # `query_layer` = [B, F, N, H]
  q = dense_layer_3d(from_tensor, num_attention_heads, size_per_head,
                     create_initializer(initializer_range), query_act, ""query"")

  # `key_layer` = [B, T, N, H]
  k = dense_layer_3d(to_tensor, num_attention_heads, size_per_head,
                     create_initializer(initializer_range), key_act, ""key"")
  # `value_layer` = [B, T, N, H]
  v = dense_layer_3d(to_tensor, num_attention_heads, size_per_head,
                     create_initializer(initializer_range), value_act, ""value"")
  q = tf.transpose(q, [0, 2, 1, 3])
  k = tf.transpose(k, [0, 2, 1, 3])
  v = tf.transpose(v, [0, 2, 1, 3])
  if attention_mask is not None:
    attention_mask = tf.reshape(
        attention_mask, [batch_size, 1, to_seq_length, 1])
    # 'new_embeddings = [B, N, F, H]'
  new_embeddings = dot_product_attention(q, k, v, attention_mask,
                                         attention_probs_dropout_prob)

  return tf.transpose(new_embeddings, [0, 2, 1, 3])


def attention_ffn_block(layer_input,
                        hidden_size=768,
                        attention_mask=None,
                        num_attention_heads=1,
                        attention_head_size=64,
                        attention_probs_dropout_prob=0.0,
                        intermediate_size=3072,
                        intermediate_act_fn=None,
                        initializer_range=0.02,
                        hidden_dropout_prob=0.0):
  """"""A network with attention-ffn as sub-block.
  Args:
    layer_input: float Tensor of shape [batch_size, from_seq_length,
      from_width].
    hidden_size: (optional) int, size of hidden layer.
    attention_mask: (optional) int32 Tensor of shape [batch_size,
      from_seq_length, to_seq_length]. The values should be 1 or 0. The
      attention scores will effectively be set to -infinity for any positions in
      the mask that are 0, and will be unchanged for positions that are 1.
    num_attention_heads: int. Number of attention heads.
    attention_head_size: int. Size of attention head.
    attention_probs_dropout_prob: float. dropout probability for attention_layer
    intermediate_size: int. Size of intermediate hidden layer.
    intermediate_act_fn: (optional) Activation function for the intermediate
      layer.
    initializer_range: float. Range of the weight initializer.
    hidden_dropout_prob: (optional) float. Dropout probability of the hidden
      layer.
  Returns:
    layer output
  """"""

  with tf.variable_scope(""attention_1""):
    with tf.variable_scope(""self""):
      attention_output = attention_layer(
          from_tensor=layer_input,
          to_tensor=layer_input,
          attention_mask=attention_mask,
          num_attention_heads=num_attention_heads,
          attention_probs_dropout_prob=attention_probs_dropout_prob,
          initializer_range=initializer_range)

    # Run a linear projection of `hidden_size` then add a residual
    # with `layer_input`.
    with tf.variable_scope(""output""):
      attention_output = dense_layer_3d_proj(
          attention_output,
          hidden_size,
          attention_head_size,
          create_initializer(initializer_range),
          None,
          name=""dense"")
      attention_output = dropout(attention_output, hidden_dropout_prob)
  attention_output = layer_norm(attention_output + layer_input)
  with tf.variable_scope(""ffn_1""):
    with tf.variable_scope(""intermediate""):
      intermediate_output = dense_layer_2d(
          attention_output,
          intermediate_size,
          create_initializer(initializer_range),
          intermediate_act_fn,
          num_attention_heads=num_attention_heads,
          name=""dense"")
      with tf.variable_scope(""output""):
        ffn_output = dense_layer_2d(
            intermediate_output,
            hidden_size,
            create_initializer(initializer_range),
            None,
            num_attention_heads=num_attention_heads,
            name=""dense"")
      ffn_output = dropout(ffn_output, hidden_dropout_prob)
  ffn_output = layer_norm(ffn_output + attention_output)
  return ffn_output


def transformer_model(input_tensor,
                      attention_mask=None,
                      hidden_size=768,
                      num_hidden_layers=12,
                      num_hidden_groups=12,
                      num_attention_heads=12,
                      intermediate_size=3072,
                      inner_group_num=1,
                      intermediate_act_fn=""gelu"",
                      hidden_dropout_prob=0.1,
                      attention_probs_dropout_prob=0.1,
                      initializer_range=0.02,
                      do_return_all_layers=False):
  """"""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".
  This is almost an exact implementation of the original Transformer encoder.
  See the original paper:
  https://arxiv.org/abs/1706.03762
  Also see:
  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py
  Args:
    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].
    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,
      seq_length], with 1 for positions that can be attended to and 0 in
      positions that should not be.
    hidden_size: int. Hidden size of the Transformer.
    num_hidden_layers: int. Number of layers (blocks) in the Transformer.
    num_hidden_groups: int. Number of group for the hidden layers, parameters
      in the same group are shared.
    num_attention_heads: int. Number of attention heads in the Transformer.
    intermediate_size: int. The size of the ""intermediate"" (a.k.a., feed
      forward) layer.
    inner_group_num: int, number of inner repetition of attention and ffn.
    intermediate_act_fn: function. The non-linear activation function to apply
      to the output of the intermediate/feed-forward layer.
    hidden_dropout_prob: float. Dropout probability for the hidden layers.
    attention_probs_dropout_prob: float. Dropout probability of the attention
      probabilities.
    initializer_range: float. Range of the initializer (stddev of truncated
      normal).
    do_return_all_layers: Whether to also return all layers or just the final
      layer.
  Returns:
    float Tensor of shape [batch_size, seq_length, hidden_size], the final
    hidden layer of the Transformer.
  Raises:
    ValueError: A Tensor shape or parameter is invalid.
  """"""
  if hidden_size % num_attention_heads != 0:
    raise ValueError(
        ""The hidden size (%d) is not a multiple of the number of attention ""
        ""heads (%d)"" % (hidden_size, num_attention_heads))

  attention_head_size = hidden_size // num_attention_heads
  input_shape = get_shape_list(input_tensor, expected_rank=3)
  input_width = input_shape[2]

  all_layer_outputs = []
  if input_width != hidden_size:
    prev_output = dense_layer_2d(
        input_tensor, hidden_size, create_initializer(initializer_range),
        None, name=""embedding_hidden_mapping_in"")
  else:
    prev_output = input_tensor
  with tf.variable_scope(""transformer"", reuse=tf.AUTO_REUSE):
    for layer_idx in range(num_hidden_layers):
      group_idx = int(layer_idx / num_hidden_layers * num_hidden_groups)
      with tf.variable_scope(""group_%d"" % group_idx):
        with tf.name_scope(""layer_%d"" % layer_idx):
          layer_output = prev_output
          for inner_group_idx in range(inner_group_num):
            with tf.variable_scope(""inner_group_%d"" % inner_group_idx):
              layer_output = attention_ffn_block(
                  layer_output, hidden_size, attention_mask,
                  num_attention_heads, attention_head_size,
                  attention_probs_dropout_prob, intermediate_size,
                  intermediate_act_fn, initializer_range, hidden_dropout_prob)
              prev_output = layer_output
              all_layer_outputs.append(layer_output)
  if do_return_all_layers:
    return all_layer_outputs
  else:
    return all_layer_outputs[-1]


def get_shape_list(tensor, expected_rank=None, name=None):
  """"""Returns a list of the shape of tensor, preferring static dimensions.
  Args:
    tensor: A tf.Tensor object to find the shape of.
    expected_rank: (optional) int. The expected rank of `tensor`. If this is
      specified and the `tensor` has a different rank, and exception will be
      thrown.
    name: Optional name of the tensor for the error message.
  Returns:
    A list of dimensions of the shape of tensor. All static dimensions will
    be returned as python integers, and dynamic dimensions will be returned
    as tf.Tensor scalars.
  """"""
  if name is None:
    name = tensor.name

  if expected_rank is not None:
    assert_rank(tensor, expected_rank, name)

  shape = tensor.shape.as_list()

  non_static_indexes = []
  for (index, dim) in enumerate(shape):
    if dim is None:
      non_static_indexes.append(index)

  if not non_static_indexes:
    return shape

  dyn_shape = tf.shape(tensor)
  for index in non_static_indexes:
    shape[index] = dyn_shape[index]
  return shape


def reshape_to_matrix(input_tensor):
  """"""Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).""""""
  ndims = input_tensor.shape.ndims
  if ndims < 2:
    raise ValueError(""Input tensor must have at least rank 2. Shape = %s"" %
                     (input_tensor.shape))
  if ndims == 2:
    return input_tensor

  width = input_tensor.shape[-1]
  output_tensor = tf.reshape(input_tensor, [-1, width])
  return output_tensor


def reshape_from_matrix(output_tensor, orig_shape_list):
  """"""Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""""""
  if len(orig_shape_list) == 2:
    return output_tensor

  output_shape = get_shape_list(output_tensor)

  orig_dims = orig_shape_list[0:-1]
  width = output_shape[-1]

  return tf.reshape(output_tensor, orig_dims + [width])


def assert_rank(tensor, expected_rank, name=None):
  """"""Raises an exception if the tensor rank is not of the expected rank.
  Args:
    tensor: A tf.Tensor to check the rank of.
    expected_rank: Python integer or list of integers, expected rank.
    name: Optional name of the tensor for the error message.
  Raises:
    ValueError: If the expected shape doesn't match the actual shape.
  """"""
  if name is None:
    name = tensor.name

  expected_rank_dict = {}
  if isinstance(expected_rank, six.integer_types):
    expected_rank_dict[expected_rank] = True
  else:
    for x in expected_rank:
      expected_rank_dict[x] = True

  actual_rank = tensor.shape.ndims
  if actual_rank not in expected_rank_dict:
    scope_name = tf.get_variable_scope().name
    raise ValueError(
        ""For the tensor `%s` in scope `%s`, the actual rank ""
        ""`%d` (shape = %s) is not equal to the expected rank `%s`"" %
        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))"
Albert,optimization.py,"# coding=utf-8
# Copyright 2018 The Google AI Language Team Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Functions and classes related to optimization (weight updates).""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import re
import tensorflow as tf


def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):
    """"""Creates an optimizer training op.""""""
    global_step = tf.train.get_or_create_global_step()

    learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)

    # Implements linear decay of the learning rate.
    learning_rate = tf.train.polynomial_decay(
        learning_rate,
        global_step,
        num_train_steps,
        end_learning_rate=0.0,
        power=1.0,
        cycle=False)

    # Implements linear warmup. I.e., if global_step < num_warmup_steps, the
    # learning rate will be `global_step/num_warmup_steps * init_lr`.
    if num_warmup_steps:
        global_steps_int = tf.cast(global_step, tf.int32)
        warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)

        global_steps_float = tf.cast(global_steps_int, tf.float32)
        warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)

        warmup_percent_done = global_steps_float / warmup_steps_float
        warmup_learning_rate = init_lr * warmup_percent_done

        is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)
        learning_rate = (
                (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)

    # It is recommended that you use this optimizer for fine tuning, since this
    # is how the model was trained (note that the Adam m/v variables are NOT
    # loaded from init_checkpoint.)
    optimizer = LAMBOptimizer(
        learning_rate=learning_rate,
        weight_decay_rate=0.01,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-6,
        exclude_from_weight_decay=[""LayerNorm"", ""layer_norm"", ""bias""])

    if use_tpu:
        optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)

    tvars = tf.trainable_variables()
    grads = tf.gradients(loss, tvars)

    # This is how the model was pre-trained.
    (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)

    train_op = optimizer.apply_gradients(
        zip(grads, tvars), global_step=global_step)

    # Normally the global step update is done inside of `apply_gradients`.
    # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use
    # a different optimizer, you should probably take this line out.
    new_global_step = global_step + 1
    train_op = tf.group(train_op, [global_step.assign(new_global_step)])
    return train_op


class AdamWeightDecayOptimizer(tf.train.Optimizer):
    """"""A basic Adam optimizer that includes ""correct"" L2 weight decay.""""""

    def __init__(self,
                 learning_rate,
                 weight_decay_rate=0.0,
                 beta_1=0.9,
                 beta_2=0.999,
                 epsilon=1e-6,
                 exclude_from_weight_decay=None,
                 name=""AdamWeightDecayOptimizer""):
        """"""Constructs a AdamWeightDecayOptimizer.""""""
        super(AdamWeightDecayOptimizer, self).__init__(False, name)

        self.learning_rate = learning_rate
        self.weight_decay_rate = weight_decay_rate
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon
        self.exclude_from_weight_decay = exclude_from_weight_decay

    def apply_gradients(self, grads_and_vars, global_step=None, name=None):
        """"""See base class.""""""
        assignments = []
        for (grad, param) in grads_and_vars:
            if grad is None or param is None:
                continue

            param_name = self._get_variable_name(param.name)

            m = tf.get_variable(
                name=param_name + ""/adam_m"",
                shape=param.shape.as_list(),
                dtype=tf.float32,
                trainable=False,
                initializer=tf.zeros_initializer())
            v = tf.get_variable(
                name=param_name + ""/adam_v"",
                shape=param.shape.as_list(),
                dtype=tf.float32,
                trainable=False,
                initializer=tf.zeros_initializer())

            # Standard Adam update.
            next_m = (
                    tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))
            next_v = (
                    tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,
                                                              tf.square(grad)))

            update = next_m / (tf.sqrt(next_v) + self.epsilon)

            # Just adding the square of the weights to the loss function is *not*
            # the correct way of using L2 regularization/weight decay with Adam,
            # since that will interact with the m and v parameters in strange ways.
            #
            # Instead we want ot decay the weights in a manner that doesn't interact
            # with the m/v parameters. This is equivalent to adding the square
            # of the weights to the loss with plain (non-momentum) SGD.
            if self._do_use_weight_decay(param_name):
                update += self.weight_decay_rate * param

            update_with_lr = self.learning_rate * update

            next_param = param - update_with_lr

            assignments.extend(
                [param.assign(next_param),
                 m.assign(next_m),
                 v.assign(next_v)])
        return tf.group(*assignments, name=name)

    def _do_use_weight_decay(self, param_name):
        """"""Whether to use L2 weight decay for `param_name`.""""""
        if not self.weight_decay_rate:
            return False
        if self.exclude_from_weight_decay:
            for r in self.exclude_from_weight_decay:
                if re.search(r, param_name) is not None:
                    return False
        return True

    def _get_variable_name(self, param_name):
        """"""Get the variable name from the tensor name.""""""
        m = re.match(""^(.*):\\d+$"", param_name)
        if m is not None:
            param_name = m.group(1)
        return param_name


#
class LAMBOptimizer(tf.train.Optimizer):
    """"""
    LAMBOptimizer optimizer.
    https://github.com/ymcui/LAMB_Optimizer_TF
    # IMPORTANT NOTE
    - This is NOT an official implementation.
    - LAMB optimizer is changed from arXiv v1 ~ v3.
    - We implement v3 version (which is the latest version on June, 2019.).
    - Our implementation is based on `AdamWeightDecayOptimizer` in BERT (provided by Google).

    # References
    - Large Batch Optimization for Deep Learning: Training BERT in 76 minutes. https://arxiv.org/abs/1904.00962v3
    - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. https://arxiv.org/abs/1810.04805
    # Parameters
    - There is nothing special, just the same as `AdamWeightDecayOptimizer`.
    """"""

    def __init__(self,
                 learning_rate,
                 weight_decay_rate=0.01,
                 beta_1=0.9,
                 beta_2=0.999,
                 epsilon=1e-6,
                 exclude_from_weight_decay=None,
                 name=""LAMBOptimizer""):
        """"""Constructs a LAMBOptimizer.""""""
        super(LAMBOptimizer, self).__init__(False, name)

        self.learning_rate = learning_rate
        self.weight_decay_rate = weight_decay_rate
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon
        self.exclude_from_weight_decay = exclude_from_weight_decay

    def apply_gradients(self, grads_and_vars, global_step=None, name=None):
        """"""See base class.""""""
        assignments = []
        for (grad, param) in grads_and_vars:
            if grad is None or param is None:
                continue

            param_name = self._get_variable_name(param.name)

            m = tf.get_variable(
                name=param_name + ""/lamb_m"",
                shape=param.shape.as_list(),
                dtype=tf.float32,
                trainable=False,
                initializer=tf.zeros_initializer())
            v = tf.get_variable(
                name=param_name + ""/lamb_v"",
                shape=param.shape.as_list(),
                dtype=tf.float32,
                trainable=False,
                initializer=tf.zeros_initializer())

            # Standard Adam update.
            next_m = (
                    tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))
            next_v = (
                    tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,
                                                              tf.square(grad)))

            update = next_m / (tf.sqrt(next_v) + self.epsilon)

            # Just adding the square of the weights to the loss function is *not*
            # the correct way of using L2 regularization/weight decay with Adam,
            # since that will interact with the m and v parameters in strange ways.
            #
            # Instead we want ot decay the weights in a manner that doesn't interact
            # with the m/v parameters. This is equivalent to adding the square
            # of the weights to the loss with plain (non-momentum) SGD.
            if self._do_use_weight_decay(param_name):
                update += self.weight_decay_rate * param

            ############## BELOW ARE THE SPECIFIC PARTS FOR LAMB ##############

            # Note: Here are two choices for scaling function \phi(z)
            # minmax:   \phi(z) = min(max(z, \gamma_l), \gamma_u)
            # identity: \phi(z) = z
            # The authors does not mention what is \gamma_l and \gamma_u
            # UPDATE: after asking authors, they provide me the code below.
            # ratio = array_ops.where(math_ops.greater(w_norm, 0), array_ops.where(
            #      math_ops.greater(g_norm, 0), (w_norm / g_norm), 1.0), 1.0)

            r1 = tf.sqrt(tf.reduce_sum(tf.square(param)))
            r2 = tf.sqrt(tf.reduce_sum(tf.square(update)))

            r = tf.where(tf.greater(r1, 0.0),
                         tf.where(tf.greater(r2, 0.0),
                                  r1 / r2,
                                  1.0),
                         1.0)

            eta = self.learning_rate * r

            update_with_lr = eta * update

            next_param = param - update_with_lr

            assignments.extend(
                [param.assign(next_param),
                 m.assign(next_m),
                 v.assign(next_v)])
        return tf.group(*assignments, name=name)

    def _do_use_weight_decay(self, param_name):
        """"""Whether to use L2 weight decay for `param_name`.""""""
        if not self.weight_decay_rate:
            return False
        if self.exclude_from_weight_decay:
            for r in self.exclude_from_weight_decay:
                if re.search(r, param_name) is not None:
                    return False
        return True

    def _get_variable_name(self, param_name):
        """"""Get the variable name from the tensor name.""""""
        m = re.match(""^(.*):\\d+$"", param_name)
        if m is not None:
            param_name = m.group(1)
        return param_name"
Albert,tokenization.py,"# coding=utf-8
# Copyright 2018 The Google AI Language Team Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Tokenization classes.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import re
import unicodedata
import six
import tensorflow as tf


def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):
  """"""Checks whether the casing config is consistent with the checkpoint name.""""""

  # The casing has to be passed in by the user and there is no explicit check
  # as to whether it matches the checkpoint. The casing information probably
  # should have been stored in the bert_config.json file, but it's not, so
  # we have to heuristically detect it to validate.

  if not init_checkpoint:
    return

  m = re.match(""^.*?([A-Za-z0-9_-]+)/bert_model.ckpt"", init_checkpoint)
  if m is None:
    return

  model_name = m.group(1)

  lower_models = [
      ""uncased_L-24_H-1024_A-16"", ""uncased_L-12_H-768_A-12"",
      ""multilingual_L-12_H-768_A-12"", ""chinese_L-12_H-768_A-12""
  ]

  cased_models = [
      ""cased_L-12_H-768_A-12"", ""cased_L-24_H-1024_A-16"",
      ""multi_cased_L-12_H-768_A-12""
  ]

  is_bad_config = False
  if model_name in lower_models and not do_lower_case:
    is_bad_config = True
    actual_flag = ""False""
    case_name = ""lowercased""
    opposite_flag = ""True""

  if model_name in cased_models and do_lower_case:
    is_bad_config = True
    actual_flag = ""True""
    case_name = ""cased""
    opposite_flag = ""False""

  if is_bad_config:
    raise ValueError(
        ""You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. ""
        ""However, `%s` seems to be a %s model, so you ""
        ""should pass in `--do_lower_case=%s` so that the fine-tuning matches ""
        ""how the model was pre-training. If this error is wrong, please ""
        ""just comment out this check."" % (actual_flag, init_checkpoint,
                                          model_name, case_name, opposite_flag))


def convert_to_unicode(text):
  """"""Converts `text` to Unicode (if it's not already), assuming utf-8 input.""""""
  if six.PY3:
    if isinstance(text, str):
      return text
    elif isinstance(text, bytes):
      return text.decode(""utf-8"", ""ignore"")
    else:
      raise ValueError(""Unsupported string type: %s"" % (type(text)))
  elif six.PY2:
    if isinstance(text, str):
      return text.decode(""utf-8"", ""ignore"")
    elif isinstance(text, unicode):
      return text
    else:
      raise ValueError(""Unsupported string type: %s"" % (type(text)))
  else:
    raise ValueError(""Not running on Python2 or Python 3?"")


def printable_text(text):
  """"""Returns text encoded in a way suitable for print or `tf.logging`.""""""

  # These functions want `str` for both Python2 and Python3, but in one case
  # it's a Unicode string and in the other it's a byte string.
  if six.PY3:
    if isinstance(text, str):
      return text
    elif isinstance(text, bytes):
      return text.decode(""utf-8"", ""ignore"")
    else:
      raise ValueError(""Unsupported string type: %s"" % (type(text)))
  elif six.PY2:
    if isinstance(text, str):
      return text
    elif isinstance(text, unicode):
      return text.encode(""utf-8"")
    else:
      raise ValueError(""Unsupported string type: %s"" % (type(text)))
  else:
    raise ValueError(""Not running on Python2 or Python 3?"")


def load_vocab(vocab_file):
  """"""Loads a vocabulary file into a dictionary.""""""
  vocab = collections.OrderedDict()
  index = 0
  with tf.gfile.GFile(vocab_file, ""r"") as reader:
    while True:
      token = convert_to_unicode(reader.readline())
      if not token:
        break
      token = token.strip()
      vocab[token] = index
      index += 1
  return vocab


def convert_by_vocab(vocab, items):
  """"""Converts a sequence of [tokens|ids] using the vocab.""""""
  output = []
  #print(""items:"",items) #['[CLS]', '日', '##期', '，', '但', '被', '##告', '金', '##东', '##福', '载', '##明', '[MASK]', 'U', '##N', '##K', ']', '保', '##证', '本', '##月', '1', '##4', '[MASK]', '到', '##位', '，', '2', '##0', '##1', '##5', '年', '6', '[MASK]', '1', '##1', '日', '[', 'U', '##N', '##K', ']', '，', '原', '##告', '[MASK]', '认', '##可', '于', '2', '##0', '##1', '##5', '[MASK]', '6', '月', '[MASK]', '[MASK]', '日', '##向', '被', '##告', '主', '##张', '权', '##利', '。', '而', '[MASK]', '[MASK]', '自', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '年', '6', '月', '1', '##1', '日', '[SEP]', '原', '##告', '于', '2', '##0', '##1', '##6', '[MASK]', '6', '[MASK]', '2', '##4', '日', '起', '##诉', '，', '主', '##张', '保', '##证', '责', '##任', '，', '已', '超', '##过', '保', '##证', '期', '##限', '[MASK]', '保', '##证', '人', '依', '##法', '不', '##再', '承', '##担', '保', '##证', '[MASK]', '[MASK]', '[MASK]', '[SEP]']
  for i,item in enumerate(items):
    #print(i,""item:"",item) #  ##期
    output.append(vocab[item])
  return output


def convert_tokens_to_ids(vocab, tokens):
  return convert_by_vocab(vocab, tokens)


def convert_ids_to_tokens(inv_vocab, ids):
  return convert_by_vocab(inv_vocab, ids)


def whitespace_tokenize(text):
  """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""
  text = text.strip()
  if not text:
    return []
  tokens = text.split()
  return tokens


class FullTokenizer(object):
  """"""Runs end-to-end tokenziation.""""""

  def __init__(self, vocab_file, do_lower_case=True):
    self.vocab = load_vocab(vocab_file)
    self.inv_vocab = {v: k for k, v in self.vocab.items()}
    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)
    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)

  def tokenize(self, text):
    split_tokens = []
    for token in self.basic_tokenizer.tokenize(text):
      for sub_token in self.wordpiece_tokenizer.tokenize(token):
        split_tokens.append(sub_token)

    return split_tokens

  def convert_tokens_to_ids(self, tokens):
    return convert_by_vocab(self.vocab, tokens)

  def convert_ids_to_tokens(self, ids):
    return convert_by_vocab(self.inv_vocab, ids)


class BasicTokenizer(object):
  """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""

  def __init__(self, do_lower_case=True):
    """"""Constructs a BasicTokenizer.

    Args:
      do_lower_case: Whether to lower case the input.
    """"""
    self.do_lower_case = do_lower_case

  def tokenize(self, text):
    """"""Tokenizes a piece of text.""""""
    text = convert_to_unicode(text)
    text = self._clean_text(text)

    # This was added on November 1st, 2018 for the multilingual and Chinese
    # models. This is also applied to the English models now, but it doesn't
    # matter since the English models were not trained on any Chinese data
    # and generally don't have any Chinese data in them (there are Chinese
    # characters in the vocabulary because Wikipedia does have some Chinese
    # words in the English Wikipedia.).
    text = self._tokenize_chinese_chars(text)

    orig_tokens = whitespace_tokenize(text)
    split_tokens = []
    for token in orig_tokens:
      if self.do_lower_case:
        token = token.lower()
        token = self._run_strip_accents(token)
      split_tokens.extend(self._run_split_on_punc(token))

    output_tokens = whitespace_tokenize("" "".join(split_tokens))
    return output_tokens

  def _run_strip_accents(self, text):
    """"""Strips accents from a piece of text.""""""
    text = unicodedata.normalize(""NFD"", text)
    output = []
    for char in text:
      cat = unicodedata.category(char)
      if cat == ""Mn"":
        continue
      output.append(char)
    return """".join(output)

  def _run_split_on_punc(self, text):
    """"""Splits punctuation on a piece of text.""""""
    chars = list(text)
    i = 0
    start_new_word = True
    output = []
    while i < len(chars):
      char = chars[i]
      if _is_punctuation(char):
        output.append([char])
        start_new_word = True
      else:
        if start_new_word:
          output.append([])
        start_new_word = False
        output[-1].append(char)
      i += 1

    return ["""".join(x) for x in output]

  def _tokenize_chinese_chars(self, text):
    """"""Adds whitespace around any CJK character.""""""
    output = []
    for char in text:
      cp = ord(char)
      if self._is_chinese_char(cp):
        output.append("" "")
        output.append(char)
        output.append("" "")
      else:
        output.append(char)
    return """".join(output)

  def _is_chinese_char(self, cp):
    """"""Checks whether CP is the codepoint of a CJK character.""""""
    # This defines a ""chinese character"" as anything in the CJK Unicode block:
    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
    #
    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,
    # despite its name. The modern Korean Hangul alphabet is a different block,
    # as is Japanese Hiragana and Katakana. Those alphabets are used to write
    # space-separated words, so they are not treated specially and handled
    # like the all of the other languages.
    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #
        (cp >= 0x3400 and cp <= 0x4DBF) or  #
        (cp >= 0x20000 and cp <= 0x2A6DF) or  #
        (cp >= 0x2A700 and cp <= 0x2B73F) or  #
        (cp >= 0x2B740 and cp <= 0x2B81F) or  #
        (cp >= 0x2B820 and cp <= 0x2CEAF) or
        (cp >= 0xF900 and cp <= 0xFAFF) or  #
        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #
      return True

    return False

  def _clean_text(self, text):
    """"""Performs invalid character removal and whitespace cleanup on text.""""""
    output = []
    for char in text:
      cp = ord(char)
      if cp == 0 or cp == 0xfffd or _is_control(char):
        continue
      if _is_whitespace(char):
        output.append("" "")
      else:
        output.append(char)
    return """".join(output)


class WordpieceTokenizer(object):
  """"""Runs WordPiece tokenziation.""""""

  def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=200):
    self.vocab = vocab
    self.unk_token = unk_token
    self.max_input_chars_per_word = max_input_chars_per_word

  def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.

    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]

    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.

    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + substr
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens


def _is_whitespace(char):
  """"""Checks whether `chars` is a whitespace character.""""""
  # \t, \n, and \r are technically contorl characters but we treat them
  # as whitespace since they are generally considered as such.
  if char == "" "" or char == ""\t"" or char == ""\n"" or char == ""\r"":
    return True
  cat = unicodedata.category(char)
  if cat == ""Zs"":
    return True
  return False


def _is_control(char):
  """"""Checks whether `chars` is a control character.""""""
  # These are technically control characters but we count them as whitespace
  # characters.
  if char == ""\t"" or char == ""\n"" or char == ""\r"":
    return False
  cat = unicodedata.category(char)
  if cat in (""Cc"", ""Cf""):
    return True
  return False


def _is_punctuation(char):
  """"""Checks whether `chars` is a punctuation character.""""""
  cp = ord(char)
  # We treat all non-letter/number ASCII as punctuation.
  # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode
  # Punctuation class but we treat them as punctuation anyways, for
  # consistency.
  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or
      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):
    return True
  cat = unicodedata.category(char)
  if cat.startswith(""P""):
    return True
  return False
"
Albert,lamb_optimizer_google.py,"# coding=utf-8
# Copyright 2019 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Lint as: python2, python3
""""""Functions and classes related to optimization (weight updates).""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import re
import six
import tensorflow as tf

# pylint: disable=g-direct-tensorflow-import
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import linalg_ops
from tensorflow.python.ops import math_ops
# pylint: enable=g-direct-tensorflow-import


class LAMBOptimizer(tf.train.Optimizer):
  """"""LAMB (Layer-wise Adaptive Moments optimizer for Batch training).""""""
  # A new optimizer that includes correct L2 weight decay, adaptive
  # element-wise updating, and layer-wise justification. The LAMB optimizer
  # was proposed by Yang You, Jing Li, Jonathan Hseu, Xiaodan Song,
  # James Demmel, and Cho-Jui Hsieh in a paper titled as Reducing BERT
  # Pre-Training Time from 3 Days to 76 Minutes (arxiv.org/abs/1904.00962)

  def __init__(self,
               learning_rate,
               weight_decay_rate=0.0,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-6,
               exclude_from_weight_decay=None,
               exclude_from_layer_adaptation=None,
               name=""LAMBOptimizer""):
    """"""Constructs a LAMBOptimizer.""""""
    super(LAMBOptimizer, self).__init__(False, name)

    self.learning_rate = learning_rate
    self.weight_decay_rate = weight_decay_rate
    self.beta_1 = beta_1
    self.beta_2 = beta_2
    self.epsilon = epsilon
    self.exclude_from_weight_decay = exclude_from_weight_decay
    # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the
    # arg is None.
    # TODO(jingli): validate if exclude_from_layer_adaptation is necessary.
    if exclude_from_layer_adaptation:
      self.exclude_from_layer_adaptation = exclude_from_layer_adaptation
    else:
      self.exclude_from_layer_adaptation = exclude_from_weight_decay

  def apply_gradients(self, grads_and_vars, global_step=None, name=None):
    """"""See base class.""""""
    assignments = []
    for (grad, param) in grads_and_vars:
      if grad is None or param is None:
        continue

      param_name = self._get_variable_name(param.name)

      m = tf.get_variable(
          name=six.ensure_str(param_name) + ""/adam_m"",
          shape=param.shape.as_list(),
          dtype=tf.float32,
          trainable=False,
          initializer=tf.zeros_initializer())
      v = tf.get_variable(
          name=six.ensure_str(param_name) + ""/adam_v"",
          shape=param.shape.as_list(),
          dtype=tf.float32,
          trainable=False,
          initializer=tf.zeros_initializer())

      # Standard Adam update.
      next_m = (
          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))
      next_v = (
          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,
                                                    tf.square(grad)))

      update = next_m / (tf.sqrt(next_v) + self.epsilon)

      # Just adding the square of the weights to the loss function is *not*
      # the correct way of using L2 regularization/weight decay with Adam,
      # since that will interact with the m and v parameters in strange ways.
      #
      # Instead we want ot decay the weights in a manner that doesn't interact
      # with the m/v parameters. This is equivalent to adding the square
      # of the weights to the loss with plain (non-momentum) SGD.
      if self._do_use_weight_decay(param_name):
        update += self.weight_decay_rate * param

      ratio = 1.0
      if self._do_layer_adaptation(param_name):
        w_norm = linalg_ops.norm(param, ord=2)
        g_norm = linalg_ops.norm(update, ord=2)
        ratio = array_ops.where(math_ops.greater(w_norm, 0), array_ops.where(
            math_ops.greater(g_norm, 0), (w_norm / g_norm), 1.0), 1.0)

      update_with_lr = ratio * self.learning_rate * update

      next_param = param - update_with_lr

      assignments.extend(
          [param.assign(next_param),
           m.assign(next_m),
           v.assign(next_v)])
    return tf.group(*assignments, name=name)

  def _do_use_weight_decay(self, param_name):
    """"""Whether to use L2 weight decay for `param_name`.""""""
    if not self.weight_decay_rate:
      return False
    if self.exclude_from_weight_decay:
      for r in self.exclude_from_weight_decay:
        if re.search(r, param_name) is not None:
          return False
    return True

  def _do_layer_adaptation(self, param_name):
    """"""Whether to do layer-wise learning rate adaptation for `param_name`.""""""
    if self.exclude_from_layer_adaptation:
      for r in self.exclude_from_layer_adaptation:
        if re.search(r, param_name) is not None:
          return False
    return True

  def _get_variable_name(self, param_name):
    """"""Get the variable name from the tensor name.""""""
    m = re.match(""^(.*):\\d+$"", six.ensure_str(param_name))
    if m is not None:
      param_name = m.group(1)
    return param_name
"
Albert,run_pretraining.py,"# coding=utf-8
# Copyright 2018 The Google AI Language Team Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Run masked LM/next sentence masked_lm pre-training for BERT.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import modeling
import optimization
import tensorflow as tf

flags = tf.flags

FLAGS = flags.FLAGS

## Required parameters
flags.DEFINE_string(
    ""bert_config_file"", None,
    ""The config json file corresponding to the pre-trained BERT model. ""
    ""This specifies the model architecture."")

flags.DEFINE_string(
    ""input_file"", None,
    ""Input TF example files (can be a glob or comma separated)."")

flags.DEFINE_string(
    ""output_dir"", None,
    ""The output directory where the model checkpoints will be written."")

## Other parameters
flags.DEFINE_string(
    ""init_checkpoint"", None,
    ""Initial checkpoint (usually from a pre-trained BERT model)."")

flags.DEFINE_integer(
    ""max_seq_length"", 128,
    ""The maximum total input sequence length after WordPiece tokenization. ""
    ""Sequences longer than this will be truncated, and sequences shorter ""
    ""than this will be padded. Must match data generation."")

flags.DEFINE_integer(
    ""max_predictions_per_seq"", 20,
    ""Maximum number of masked LM predictions per sequence. ""
    ""Must match data generation."")

flags.DEFINE_bool(""do_train"", False, ""Whether to run training."")

flags.DEFINE_bool(""do_eval"", False, ""Whether to run eval on the dev set."")

flags.DEFINE_integer(""train_batch_size"", 32, ""Total batch size for training."")

flags.DEFINE_integer(""eval_batch_size"", 8, ""Total batch size for eval."")

flags.DEFINE_float(""learning_rate"", 5e-5, ""The initial learning rate for Adam."")

flags.DEFINE_integer(""num_train_steps"", 100000, ""Number of training steps."")

flags.DEFINE_integer(""num_warmup_steps"", 10000, ""Number of warmup steps."")

flags.DEFINE_integer(""save_checkpoints_steps"", 1000,
                     ""How often to save the model checkpoint."")

flags.DEFINE_integer(""iterations_per_loop"", 1000,
                     ""How many steps to make in each estimator call."")

flags.DEFINE_integer(""max_eval_steps"", 100, ""Maximum number of eval steps."")

flags.DEFINE_bool(""use_tpu"", False, ""Whether to use TPU or GPU/CPU."")

tf.flags.DEFINE_string(
    ""tpu_name"", None,
    ""The Cloud TPU to use for training. This should be either the name ""
    ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 ""
    ""url."")

tf.flags.DEFINE_string(
    ""tpu_zone"", None,
    ""[Optional] GCE zone where the Cloud TPU is located in. If not ""
    ""specified, we will attempt to automatically detect the GCE project from ""
    ""metadata."")

tf.flags.DEFINE_string(
    ""gcp_project"", None,
    ""[Optional] Project name for the Cloud TPU-enabled project. If not ""
    ""specified, we will attempt to automatically detect the GCE project from ""
    ""metadata."")

tf.flags.DEFINE_string(""master"", None, ""[Optional] TensorFlow master URL."")

flags.DEFINE_integer(
    ""num_tpu_cores"", 8,
    ""Only used if `use_tpu` is True. Total number of TPU cores to use."")


def model_fn_builder(bert_config, init_checkpoint, learning_rate,
                     num_train_steps, num_warmup_steps, use_tpu,
                     use_one_hot_embeddings):
  """"""Returns `model_fn` closure for TPUEstimator.""""""

  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
    """"""The `model_fn` for TPUEstimator.""""""

    tf.logging.info(""*** Features ***"")
    for name in sorted(features.keys()):
      tf.logging.info(""  name = %s, shape = %s"" % (name, features[name].shape))

    input_ids = features[""input_ids""]
    input_mask = features[""input_mask""]
    segment_ids = features[""segment_ids""]
    masked_lm_positions = features[""masked_lm_positions""]
    masked_lm_ids = features[""masked_lm_ids""]
    masked_lm_weights = features[""masked_lm_weights""]
    next_sentence_labels = features[""next_sentence_labels""]

    is_training = (mode == tf.estimator.ModeKeys.TRAIN)

    model = modeling.BertModel(
        config=bert_config,
        is_training=is_training,
        input_ids=input_ids,
        input_mask=input_mask,
        token_type_ids=segment_ids,
        use_one_hot_embeddings=use_one_hot_embeddings)

    (masked_lm_loss,
     masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(
         bert_config, model.get_sequence_output(), model.get_embedding_table(),model.get_embedding_table_2(),
         masked_lm_positions, masked_lm_ids, masked_lm_weights)

    (next_sentence_loss, next_sentence_example_loss,
     next_sentence_log_probs) = get_next_sentence_output(
         bert_config, model.get_pooled_output(), next_sentence_labels)

    total_loss = masked_lm_loss + next_sentence_loss

    tvars = tf.trainable_variables()

    initialized_variable_names = {}
    print(""init_checkpoint:"",init_checkpoint)
    scaffold_fn = None
    if init_checkpoint:
      (assignment_map, initialized_variable_names
      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)
      if use_tpu:

        def tpu_scaffold():
          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)
          return tf.train.Scaffold()

        scaffold_fn = tpu_scaffold
      else:
        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)

    tf.logging.info(""**** Trainable Variables ****"")
    for var in tvars:
      init_string = """"
      if var.name in initialized_variable_names:
        init_string = "", *INIT_FROM_CKPT*""
      tf.logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,
                      init_string)

    output_spec = None
    if mode == tf.estimator.ModeKeys.TRAIN:
      train_op = optimization.create_optimizer(
          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)

      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          loss=total_loss,
          train_op=train_op,
          scaffold_fn=scaffold_fn)
    elif mode == tf.estimator.ModeKeys.EVAL:

      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,
                    masked_lm_weights, next_sentence_example_loss,
                    next_sentence_log_probs, next_sentence_labels):
        """"""Computes the loss and accuracy of the model.""""""
        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,[-1, masked_lm_log_probs.shape[-1]])
        masked_lm_predictions = tf.argmax(masked_lm_log_probs, axis=-1, output_type=tf.int32)
        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])
        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])
        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])
        masked_lm_accuracy = tf.metrics.accuracy(
            labels=masked_lm_ids,
            predictions=masked_lm_predictions,
            weights=masked_lm_weights)
        masked_lm_mean_loss = tf.metrics.mean(
            values=masked_lm_example_loss, weights=masked_lm_weights)

        next_sentence_log_probs = tf.reshape(
            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])
        next_sentence_predictions = tf.argmax(
            next_sentence_log_probs, axis=-1, output_type=tf.int32)
        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])
        next_sentence_accuracy = tf.metrics.accuracy(
            labels=next_sentence_labels, predictions=next_sentence_predictions)
        next_sentence_mean_loss = tf.metrics.mean(
            values=next_sentence_example_loss)

        return {
            ""masked_lm_accuracy"": masked_lm_accuracy,
            ""masked_lm_loss"": masked_lm_mean_loss,
            ""next_sentence_accuracy"": next_sentence_accuracy,
            ""next_sentence_loss"": next_sentence_mean_loss,
        }

      # next_sentence_example_loss=0.0 TODO
      # next_sentence_log_probs=0.0 # TODO
      eval_metrics = (metric_fn, [
          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,
          masked_lm_weights, next_sentence_example_loss,
          next_sentence_log_probs, next_sentence_labels
      ])
      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          loss=total_loss,
          eval_metrics=eval_metrics,
          scaffold_fn=scaffold_fn)
    else:
      raise ValueError(""Only TRAIN and EVAL modes are supported: %s"" % (mode))

    return output_spec

  return model_fn


def get_masked_lm_output(bert_config, input_tensor, output_weights,project_weights, positions,
                         label_ids, label_weights):
  """"""Get loss and log probs for the masked LM.""""""
  input_tensor = gather_indexes(input_tensor, positions)

  with tf.variable_scope(""cls/predictions""):
    # We apply one more non-linear transformation before the output layer.
    # This matrix is not used after pre-training.
    with tf.variable_scope(""transform""):
      input_tensor = tf.layers.dense(
          input_tensor,
          units=bert_config.hidden_size,
          activation=modeling.get_activation(bert_config.hidden_act),
          kernel_initializer=modeling.create_initializer(
              bert_config.initializer_range))
      input_tensor = modeling.layer_norm(input_tensor)

    # The output weights are the same as the input embeddings, but there is
    # an output-only bias for each token.
    output_bias = tf.get_variable(
        ""output_bias"",
        shape=[bert_config.vocab_size],
        initializer=tf.zeros_initializer())
    # logits = tf.matmul(input_tensor, output_weights, transpose_b=True)
    # input_tensor=[-1,hidden_size], project_weights=[embedding_size, hidden_size], project_weights_transpose=[hidden_size, embedding_size]--->[-1, embedding_size]
    input_project = tf.matmul(input_tensor, project_weights, transpose_b=True)
    logits = tf.matmul(input_project, output_weights, transpose_b=True)
    #  # input_project=[-1, embedding_size], output_weights=[vocab_size, embedding_size], output_weights_transpose=[embedding_size, vocab_size] ---> [-1, vocab_size]

    logits = tf.nn.bias_add(logits, output_bias)
    log_probs = tf.nn.log_softmax(logits, axis=-1)

    label_ids = tf.reshape(label_ids, [-1])
    label_weights = tf.reshape(label_weights, [-1])

    one_hot_labels = tf.one_hot(label_ids, depth=bert_config.vocab_size, dtype=tf.float32)

    # The `positions` tensor might be zero-padded (if the sequence is too
    # short to have the maximum number of predictions). The `label_weights`
    # tensor has a value of 1.0 for every real prediction and 0.0 for the
    # padding predictions.
    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])
    numerator = tf.reduce_sum(label_weights * per_example_loss)
    denominator = tf.reduce_sum(label_weights) + 1e-5
    loss = numerator / denominator

  return (loss, per_example_loss, log_probs)


def get_next_sentence_output(bert_config, input_tensor, labels):
  """"""Get loss and log probs for the next sentence prediction.""""""

  # Simple binary classification. Note that 0 is ""next sentence"" and 1 is
  # ""random sentence"". This weight matrix is not used after pre-training.
  with tf.variable_scope(""cls/seq_relationship""):
    output_weights = tf.get_variable(
        ""output_weights"",
        shape=[2, bert_config.hidden_size],
        initializer=modeling.create_initializer(bert_config.initializer_range))
    output_bias = tf.get_variable(
        ""output_bias"", shape=[2], initializer=tf.zeros_initializer())

    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)
    logits = tf.nn.bias_add(logits, output_bias)
    log_probs = tf.nn.log_softmax(logits, axis=-1)
    labels = tf.reshape(labels, [-1])
    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)
    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)
    loss = tf.reduce_mean(per_example_loss)
    return (loss, per_example_loss, log_probs)


def gather_indexes(sequence_tensor, positions):
  """"""Gathers the vectors at the specific positions over a minibatch.""""""
  sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)
  batch_size = sequence_shape[0]
  seq_length = sequence_shape[1]
  width = sequence_shape[2]

  flat_offsets = tf.reshape(
      tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])
  flat_positions = tf.reshape(positions + flat_offsets, [-1])
  flat_sequence_tensor = tf.reshape(sequence_tensor,
                                    [batch_size * seq_length, width])
  output_tensor = tf.gather(flat_sequence_tensor, flat_positions)
  return output_tensor


def input_fn_builder(input_files,
                     max_seq_length,
                     max_predictions_per_seq,
                     is_training,
                     num_cpu_threads=4):
  """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""

  def input_fn(params):
    """"""The actual input function.""""""
    batch_size = params[""batch_size""]

    name_to_features = {
        ""input_ids"":
            tf.FixedLenFeature([max_seq_length], tf.int64),
        ""input_mask"":
            tf.FixedLenFeature([max_seq_length], tf.int64),
        ""segment_ids"":
            tf.FixedLenFeature([max_seq_length], tf.int64),
        ""masked_lm_positions"":
            tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
        ""masked_lm_ids"":
            tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
        ""masked_lm_weights"":
            tf.FixedLenFeature([max_predictions_per_seq], tf.float32),
        ""next_sentence_labels"":
            tf.FixedLenFeature([1], tf.int64),
    }

    # For training, we want a lot of parallel reading and shuffling.
    # For eval, we want no shuffling and parallel reading doesn't matter.
    if is_training:
      d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))
      d = d.repeat()
      d = d.shuffle(buffer_size=len(input_files))

      # `cycle_length` is the number of parallel files that get read.
      cycle_length = min(num_cpu_threads, len(input_files))

      # `sloppy` mode means that the interleaving is not exact. This adds
      # even more randomness to the training pipeline.
      d = d.apply(
          tf.contrib.data.parallel_interleave(
              tf.data.TFRecordDataset,
              sloppy=is_training,
              cycle_length=cycle_length))
      d = d.shuffle(buffer_size=100)
    else:
      d = tf.data.TFRecordDataset(input_files)
      # Since we evaluate for a fixed number of steps we don't want to encounter
      # out-of-range exceptions.
      d = d.repeat()

    # We must `drop_remainder` on training because the TPU requires fixed
    # size dimensions. For eval, we assume we are evaluating on the CPU or GPU
    # and we *don't* want to drop the remainder, otherwise we wont cover
    # every sample.
    d = d.apply(
        tf.contrib.data.map_and_batch(
            lambda record: _decode_record(record, name_to_features),
            batch_size=batch_size,
            num_parallel_batches=num_cpu_threads,
            drop_remainder=True))
    return d

  return input_fn


def _decode_record(record, name_to_features):
  """"""Decodes a record to a TensorFlow example.""""""
  example = tf.parse_single_example(record, name_to_features)

  # tf.Example only supports tf.int64, but the TPU only supports tf.int32.
  # So cast all int64 to int32.
  for name in list(example.keys()):
    t = example[name]
    if t.dtype == tf.int64:
      t = tf.to_int32(t)
    example[name] = t

  return example


def main(_):
  tf.logging.set_verbosity(tf.logging.INFO)

  if not FLAGS.do_train and not FLAGS.do_eval: # 必须是训练或验证的类型
    raise ValueError(""At least one of `do_train` or `do_eval` must be True."")

  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file) # 从json文件中获得配置信息

  tf.gfile.MakeDirs(FLAGS.output_dir)

  input_files = [] # 输入可以是多个文件，以“逗号隔开”；可以是一个匹配形式的，如“input_x*”
  for input_pattern in FLAGS.input_file.split("",""):
    input_files.extend(tf.gfile.Glob(input_pattern))

  tf.logging.info(""*** Input Files ***"")
  for input_file in input_files:
    tf.logging.info(""  %s"" % input_file)

  tpu_cluster_resolver = None
  if FLAGS.use_tpu and FLAGS.tpu_name:
      tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver( # TODO
            tpu=FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)

  print(""###tpu_cluster_resolver:"",tpu_cluster_resolver,"";FLAGS.use_tpu:"",FLAGS.use_tpu,"";FLAGS.tpu_name:"",FLAGS.tpu_name,"";FLAGS.tpu_zone:"",FLAGS.tpu_zone)
  # ###tpu_cluster_resolver: <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f4b387b06a0> ;FLAGS.use_tpu: True ;FLAGS.tpu_name: grpc://10.240.1.83:8470

  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2
  run_config = tf.contrib.tpu.RunConfig(
      keep_checkpoint_max=20, # 10
      cluster=tpu_cluster_resolver,
      master=FLAGS.master,
      model_dir=FLAGS.output_dir,
      save_checkpoints_steps=FLAGS.save_checkpoints_steps,
      tpu_config=tf.contrib.tpu.TPUConfig(
          iterations_per_loop=FLAGS.iterations_per_loop,
          num_shards=FLAGS.num_tpu_cores,
          per_host_input_for_training=is_per_host))

  model_fn = model_fn_builder(
      bert_config=bert_config,
      init_checkpoint=FLAGS.init_checkpoint,
      learning_rate=FLAGS.learning_rate,
      num_train_steps=FLAGS.num_train_steps,
      num_warmup_steps=FLAGS.num_warmup_steps,
      use_tpu=FLAGS.use_tpu,
      use_one_hot_embeddings=FLAGS.use_tpu)

  # If TPU is not available, this will fall back to normal Estimator on CPU
  # or GPU.
  estimator = tf.contrib.tpu.TPUEstimator(
      use_tpu=FLAGS.use_tpu,
      model_fn=model_fn,
      config=run_config,
      train_batch_size=FLAGS.train_batch_size,
      eval_batch_size=FLAGS.eval_batch_size)

  if FLAGS.do_train:
    tf.logging.info(""***** Running training *****"")
    tf.logging.info(""  Batch size = %d"", FLAGS.train_batch_size)
    train_input_fn = input_fn_builder(
        input_files=input_files,
        max_seq_length=FLAGS.max_seq_length,
        max_predictions_per_seq=FLAGS.max_predictions_per_seq,
        is_training=True)
    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)

  if FLAGS.do_eval:
    tf.logging.info(""***** Running evaluation *****"")
    tf.logging.info(""  Batch size = %d"", FLAGS.eval_batch_size)

    eval_input_fn = input_fn_builder(
        input_files=input_files,
        max_seq_length=FLAGS.max_seq_length,
        max_predictions_per_seq=FLAGS.max_predictions_per_seq,
        is_training=False)

    result = estimator.evaluate(input_fn=eval_input_fn, steps=FLAGS.max_eval_steps)

    output_eval_file = os.path.join(FLAGS.output_dir, ""eval_results.txt"")
    with tf.gfile.GFile(output_eval_file, ""w"") as writer:
      tf.logging.info(""***** Eval results *****"")
      for key in sorted(result.keys()):
        tf.logging.info(""  %s = %s"", key, str(result[key]))
        writer.write(""%s = %s\n"" % (key, str(result[key])))


if __name__ == ""__main__"":
  flags.mark_flag_as_required(""input_file"")
  flags.mark_flag_as_required(""bert_config_file"")
  flags.mark_flag_as_required(""output_dir"")
  tf.app.run()
"
Albert,run_classifier.py,"# coding=utf-8
# Copyright 2018 The Google AI Language Team Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""BERT finetuning runner.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import csv
import os
import modeling
import optimization_finetuning as optimization
import tokenization
import tensorflow as tf
# from loss import bi_tempered_logistic_loss

flags = tf.flags

FLAGS = flags.FLAGS

## Required parameters
flags.DEFINE_string(
    ""data_dir"", None,
    ""The input data dir. Should contain the .tsv files (or other data files) ""
    ""for the task."")

flags.DEFINE_string(
    ""bert_config_file"", None,
    ""The config json file corresponding to the pre-trained BERT model. ""
    ""This specifies the model architecture."")

flags.DEFINE_string(""task_name"", None, ""The name of the task to train."")

flags.DEFINE_string(""vocab_file"", None,
                    ""The vocabulary file that the BERT model was trained on."")

flags.DEFINE_string(
    ""output_dir"", None,
    ""The output directory where the model checkpoints will be written."")

## Other parameters

flags.DEFINE_string(
    ""init_checkpoint"", None,
    ""Initial checkpoint (usually from a pre-trained BERT model)."")

flags.DEFINE_bool(
    ""do_lower_case"", True,
    ""Whether to lower case the input text. Should be True for uncased ""
    ""models and False for cased models."")

flags.DEFINE_integer(
    ""max_seq_length"", 128,
    ""The maximum total input sequence length after WordPiece tokenization. ""
    ""Sequences longer than this will be truncated, and sequences shorter ""
    ""than this will be padded."")

flags.DEFINE_bool(""do_train"", False, ""Whether to run training."")

flags.DEFINE_bool(""do_eval"", False, ""Whether to run eval on the dev set."")

flags.DEFINE_bool(
    ""do_predict"", False,
    ""Whether to run the model in inference mode on the test set."")

flags.DEFINE_integer(""train_batch_size"", 32, ""Total batch size for training."")

flags.DEFINE_integer(""eval_batch_size"", 8, ""Total batch size for eval."")

flags.DEFINE_integer(""predict_batch_size"", 8, ""Total batch size for predict."")

flags.DEFINE_float(""learning_rate"", 5e-5, ""The initial learning rate for Adam."")

flags.DEFINE_float(""num_train_epochs"", 3.0,
                   ""Total number of training epochs to perform."")

flags.DEFINE_float(
    ""warmup_proportion"", 0.1,
    ""Proportion of training to perform linear learning rate warmup for. ""
    ""E.g., 0.1 = 10% of training."")

flags.DEFINE_integer(""save_checkpoints_steps"", 1000,
                     ""How often to save the model checkpoint."")

flags.DEFINE_integer(""iterations_per_loop"", 1000,
                     ""How many steps to make in each estimator call."")

flags.DEFINE_bool(""use_tpu"", False, ""Whether to use TPU or GPU/CPU."")

tf.flags.DEFINE_string(
    ""tpu_name"", None,
    ""The Cloud TPU to use for training. This should be either the name ""
    ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 ""
    ""url."")

tf.flags.DEFINE_string(
    ""tpu_zone"", None,
    ""[Optional] GCE zone where the Cloud TPU is located in. If not ""
    ""specified, we will attempt to automatically detect the GCE project from ""
    ""metadata."")

tf.flags.DEFINE_string(
    ""gcp_project"", None,
    ""[Optional] Project name for the Cloud TPU-enabled project. If not ""
    ""specified, we will attempt to automatically detect the GCE project from ""
    ""metadata."")

tf.flags.DEFINE_string(""master"", None, ""[Optional] TensorFlow master URL."")

flags.DEFINE_integer(
    ""num_tpu_cores"", 8,
    ""Only used if `use_tpu` is True. Total number of TPU cores to use."")


class InputExample(object):
  """"""A single training/test example for simple sequence classification.""""""

  def __init__(self, guid, text_a, text_b=None, label=None):
    """"""Constructs a InputExample.
    Args:
      guid: Unique id for the example.
      text_a: string. The untokenized text of the first sequence. For single
        sequence tasks, only this sequence must be specified.
      text_b: (Optional) string. The untokenized text of the second sequence.
        Only must be specified for sequence pair tasks.
      label: (Optional) string. The label of the example. This should be
        specified for train and dev examples, but not for test examples.
    """"""
    self.guid = guid
    self.text_a = text_a
    self.text_b = text_b
    self.label = label


class PaddingInputExample(object):
  """"""Fake example so the num input examples is a multiple of the batch size.
  When running eval/predict on the TPU, we need to pad the number of examples
  to be a multiple of the batch size, because the TPU requires a fixed batch
  size. The alternative is to drop the last batch, which is bad because it means
  the entire output data won't be generated.
  We use this class instead of `None` because treating `None` as padding
  battches could cause silent errors.
  """"""


class InputFeatures(object):
  """"""A single set of features of data.""""""

  def __init__(self,
               input_ids,
               input_mask,
               segment_ids,
               label_id,
               is_real_example=True):
    self.input_ids = input_ids
    self.input_mask = input_mask
    self.segment_ids = segment_ids
    self.label_id = label_id
    self.is_real_example = is_real_example


class DataProcessor(object):
  """"""Base class for data converters for sequence classification data sets.""""""

  def get_train_examples(self, data_dir):
    """"""Gets a collection of `InputExample`s for the train set.""""""
    raise NotImplementedError()

  def get_dev_examples(self, data_dir):
    """"""Gets a collection of `InputExample`s for the dev set.""""""
    raise NotImplementedError()

  def get_test_examples(self, data_dir):
    """"""Gets a collection of `InputExample`s for prediction.""""""
    raise NotImplementedError()

  def get_labels(self):
    """"""Gets the list of labels for this data set.""""""
    raise NotImplementedError()

  @classmethod
  def _read_tsv(cls, input_file, quotechar=None):
    """"""Reads a tab separated value file.""""""
    with tf.gfile.Open(input_file, ""r"") as f:
      reader = csv.reader(f, delimiter=""\t"", quotechar=quotechar)
      lines = []
      for line in reader:
        lines.append(line)
      return lines

def convert_single_example(ex_index, example, label_list, max_seq_length,
                           tokenizer):
  """"""Converts a single `InputExample` into a single `InputFeatures`.""""""

  if isinstance(example, PaddingInputExample):
    return InputFeatures(
        input_ids=[0] * max_seq_length,
        input_mask=[0] * max_seq_length,
        segment_ids=[0] * max_seq_length,
        label_id=0,
        is_real_example=False)

  label_map = {}
  for (i, label) in enumerate(label_list):
    label_map[label] = i

  tokens_a = tokenizer.tokenize(example.text_a)
  tokens_b = None
  if example.text_b:
    tokens_b = tokenizer.tokenize(example.text_b)

  if tokens_b:
    # Modifies `tokens_a` and `tokens_b` in place so that the total
    # length is less than the specified length.
    # Account for [CLS], [SEP], [SEP] with ""- 3""
    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)
  else:
    # Account for [CLS] and [SEP] with ""- 2""
    if len(tokens_a) > max_seq_length - 2:
      tokens_a = tokens_a[0:(max_seq_length - 2)]

  # The convention in BERT is:
  # (a) For sequence pairs:
  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]
  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
  # (b) For single sequences:
  #  tokens:   [CLS] the dog is hairy . [SEP]
  #  type_ids: 0     0   0   0  0     0 0
  #
  # Where ""type_ids"" are used to indicate whether this is the first
  # sequence or the second sequence. The embedding vectors for `type=0` and
  # `type=1` were learned during pre-training and are added to the wordpiece
  # embedding vector (and position vector). This is not *strictly* necessary
  # since the [SEP] token unambiguously separates the sequences, but it makes
  # it easier for the model to learn the concept of sequences.
  #
  # For classification tasks, the first vector (corresponding to [CLS]) is
  # used as the ""sentence vector"". Note that this only makes sense because
  # the entire model is fine-tuned.
  tokens = []
  segment_ids = []
  tokens.append(""[CLS]"")
  segment_ids.append(0)
  for token in tokens_a:
    tokens.append(token)
    segment_ids.append(0)
  tokens.append(""[SEP]"")
  segment_ids.append(0)

  if tokens_b:
    for token in tokens_b:
      tokens.append(token)
      segment_ids.append(1)
    tokens.append(""[SEP]"")
    segment_ids.append(1)

  input_ids = tokenizer.convert_tokens_to_ids(tokens)

  # The mask has 1 for real tokens and 0 for padding tokens. Only real
  # tokens are attended to.
  input_mask = [1] * len(input_ids)

  # Zero-pad up to the sequence length.
  while len(input_ids) < max_seq_length:
    input_ids.append(0)
    input_mask.append(0)
    segment_ids.append(0)

  assert len(input_ids) == max_seq_length
  assert len(input_mask) == max_seq_length
  assert len(segment_ids) == max_seq_length

  label_id = label_map[example.label]
  if ex_index < 5:
    tf.logging.info(""*** Example ***"")
    tf.logging.info(""guid: %s"" % (example.guid))
    tf.logging.info(""tokens: %s"" % "" "".join(
        [tokenization.printable_text(x) for x in tokens]))
    tf.logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))
    tf.logging.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))
    tf.logging.info(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))
    tf.logging.info(""label: %s (id = %d)"" % (example.label, label_id))

  feature = InputFeatures(
      input_ids=input_ids,
      input_mask=input_mask,
      segment_ids=segment_ids,
      label_id=label_id,
      is_real_example=True)
  return feature


def file_based_convert_examples_to_features(
    examples, label_list, max_seq_length, tokenizer, output_file):
  """"""Convert a set of `InputExample`s to a TFRecord file.""""""

  writer = tf.python_io.TFRecordWriter(output_file)

  for (ex_index, example) in enumerate(examples):
    if ex_index % 10000 == 0:
      tf.logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))

    feature = convert_single_example(ex_index, example, label_list,
                                     max_seq_length, tokenizer)

    def create_int_feature(values):
      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))
      return f

    features = collections.OrderedDict()
    features[""input_ids""] = create_int_feature(feature.input_ids)
    features[""input_mask""] = create_int_feature(feature.input_mask)
    features[""segment_ids""] = create_int_feature(feature.segment_ids)
    features[""label_ids""] = create_int_feature([feature.label_id])
    features[""is_real_example""] = create_int_feature(
        [int(feature.is_real_example)])

    tf_example = tf.train.Example(features=tf.train.Features(feature=features))
    writer.write(tf_example.SerializeToString())
  writer.close()


def file_based_input_fn_builder(input_file, seq_length, is_training,
                                drop_remainder):
  """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""

  name_to_features = {
      ""input_ids"": tf.FixedLenFeature([seq_length], tf.int64),
      ""input_mask"": tf.FixedLenFeature([seq_length], tf.int64),
      ""segment_ids"": tf.FixedLenFeature([seq_length], tf.int64),
      ""label_ids"": tf.FixedLenFeature([], tf.int64),
      ""is_real_example"": tf.FixedLenFeature([], tf.int64),
  }

  def _decode_record(record, name_to_features):
    """"""Decodes a record to a TensorFlow example.""""""
    example = tf.parse_single_example(record, name_to_features)

    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.
    # So cast all int64 to int32.
    for name in list(example.keys()):
      t = example[name]
      if t.dtype == tf.int64:
        t = tf.to_int32(t)
      example[name] = t

    return example

  def input_fn(params):
    """"""The actual input function.""""""
    batch_size = params[""batch_size""]

    # For training, we want a lot of parallel reading and shuffling.
    # For eval, we want no shuffling and parallel reading doesn't matter.
    d = tf.data.TFRecordDataset(input_file)
    if is_training:
      d = d.repeat()
      d = d.shuffle(buffer_size=100)

    d = d.apply(
        tf.contrib.data.map_and_batch(
            lambda record: _decode_record(record, name_to_features),
            batch_size=batch_size,
            drop_remainder=drop_remainder))

    return d

  return input_fn


def _truncate_seq_pair(tokens_a, tokens_b, max_length):
  """"""Truncates a sequence pair in place to the maximum length.""""""

  # This is a simple heuristic which will always truncate the longer sequence
  # one token at a time. This makes more sense than truncating an equal percent
  # of tokens from each, since if one sequence is very short then each token
  # that's truncated likely contains more information than a longer sequence.
  while True:
    total_length = len(tokens_a) + len(tokens_b)
    if total_length <= max_length:
      break
    if len(tokens_a) > len(tokens_b):
      tokens_a.pop()
    else:
      tokens_b.pop()


def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,
                 labels, num_labels, use_one_hot_embeddings):
  """"""Creates a classification model.""""""
  model = modeling.BertModel(
      config=bert_config,
      is_training=is_training,
      input_ids=input_ids,
      input_mask=input_mask,
      token_type_ids=segment_ids,
      use_one_hot_embeddings=use_one_hot_embeddings)

  # In the demo, we are doing a simple classification task on the entire
  # segment.
  #
  # If you want to use the token-level output, use model.get_sequence_output()
  # instead.
  output_layer = model.get_pooled_output()

  hidden_size = output_layer.shape[-1].value

  output_weights = tf.get_variable(
      ""output_weights"", [num_labels, hidden_size],
      initializer=tf.truncated_normal_initializer(stddev=0.02))

  output_bias = tf.get_variable(
      ""output_bias"", [num_labels], initializer=tf.zeros_initializer())

  with tf.variable_scope(""loss""):
    ln_type = bert_config.ln_type
    if ln_type == 'preln': # add by brightmart, 10-06. if it is preln, we need to an additonal layer: layer normalization as suggested in paper ""ON LAYER NORMALIZATION IN THE TRANSFORMER ARCHITECTURE""
        print(""ln_type is preln. add LN layer."")
        output_layer=layer_norm(output_layer)
    else:
        print(""ln_type is postln or other,do nothing."")

    if is_training:
      # I.e., 0.1 dropout
      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)

    logits = tf.matmul(output_layer, output_weights, transpose_b=True)
    logits = tf.nn.bias_add(logits, output_bias)
    probabilities = tf.nn.softmax(logits, axis=-1)
    log_probs = tf.nn.log_softmax(logits, axis=-1)

    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)

    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1) # todo 08-29 try temp-loss
    ###############bi_tempered_logistic_loss############################################################################
    # print(""##cross entropy loss is used....""); tf.logging.info(""##cross entropy loss is used...."")
    # t1=0.9 #t1=0.90
    # t2=1.05 #t2=1.05
    # per_example_loss=bi_tempered_logistic_loss(log_probs,one_hot_labels,t1,t2,label_smoothing=0.1,num_iters=5) # TODO label_smoothing=0.0
    #tf.logging.info(""per_example_loss:""+str(per_example_loss.shape))
    ##############bi_tempered_logistic_loss#############################################################################

    loss = tf.reduce_mean(per_example_loss)

    return (loss, per_example_loss, logits, probabilities)

def layer_norm(input_tensor, name=None):
  """"""Run layer normalization on the last dimension of the tensor.""""""
  return tf.contrib.layers.layer_norm(
      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)

def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,
                     num_train_steps, num_warmup_steps, use_tpu,
                     use_one_hot_embeddings):
  """"""Returns `model_fn` closure for TPUEstimator.""""""

  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
    """"""The `model_fn` for TPUEstimator.""""""

    tf.logging.info(""*** Features ***"")
    for name in sorted(features.keys()):
      tf.logging.info(""  name = %s, shape = %s"" % (name, features[name].shape))

    input_ids = features[""input_ids""]
    input_mask = features[""input_mask""]
    segment_ids = features[""segment_ids""]
    label_ids = features[""label_ids""]
    is_real_example = None
    if ""is_real_example"" in features:
      is_real_example = tf.cast(features[""is_real_example""], dtype=tf.float32)
    else:
      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)

    is_training = (mode == tf.estimator.ModeKeys.TRAIN)

    (total_loss, per_example_loss, logits, probabilities) = create_model(
        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,
        num_labels, use_one_hot_embeddings)

    tvars = tf.trainable_variables()
    initialized_variable_names = {}
    scaffold_fn = None
    if init_checkpoint:
      (assignment_map, initialized_variable_names
      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)
      if use_tpu:

        def tpu_scaffold():
          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)
          return tf.train.Scaffold()

        scaffold_fn = tpu_scaffold
      else:
        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)

    tf.logging.info(""**** Trainable Variables ****"")
    for var in tvars:
      init_string = """"
      if var.name in initialized_variable_names:
        init_string = "", *INIT_FROM_CKPT*""
      tf.logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,
                      init_string)

    output_spec = None
    if mode == tf.estimator.ModeKeys.TRAIN:

      train_op = optimization.create_optimizer(
          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)

      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          loss=total_loss,
          train_op=train_op,
          scaffold_fn=scaffold_fn)
    elif mode == tf.estimator.ModeKeys.EVAL:

      def metric_fn(per_example_loss, label_ids, logits, is_real_example):
        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)
        accuracy = tf.metrics.accuracy(
            labels=label_ids, predictions=predictions, weights=is_real_example)
        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)
        return {
            ""eval_accuracy"": accuracy,
            ""eval_loss"": loss,
        }

      eval_metrics = (metric_fn,
                      [per_example_loss, label_ids, logits, is_real_example])
      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          loss=total_loss,
          eval_metrics=eval_metrics,
          scaffold_fn=scaffold_fn)
    else:
      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          predictions={""probabilities"": probabilities},
          scaffold_fn=scaffold_fn)
    return output_spec

  return model_fn


# This function is not used by this file but is still used by the Colab and
# people who depend on it.
def input_fn_builder(features, seq_length, is_training, drop_remainder):
  """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""

  all_input_ids = []
  all_input_mask = []
  all_segment_ids = []
  all_label_ids = []

  for feature in features:
    all_input_ids.append(feature.input_ids)
    all_input_mask.append(feature.input_mask)
    all_segment_ids.append(feature.segment_ids)
    all_label_ids.append(feature.label_id)

  def input_fn(params):
    """"""The actual input function.""""""
    batch_size = params[""batch_size""]

    num_examples = len(features)

    # This is for demo purposes and does NOT scale to large data sets. We do
    # not use Dataset.from_generator() because that uses tf.py_func which is
    # not TPU compatible. The right way to load data is with TFRecordReader.
    d = tf.data.Dataset.from_tensor_slices({
        ""input_ids"":
            tf.constant(
                all_input_ids, shape=[num_examples, seq_length],
                dtype=tf.int32),
        ""input_mask"":
            tf.constant(
                all_input_mask,
                shape=[num_examples, seq_length],
                dtype=tf.int32),
        ""segment_ids"":
            tf.constant(
                all_segment_ids,
                shape=[num_examples, seq_length],
                dtype=tf.int32),
        ""label_ids"":
            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),
    })

    if is_training:
      d = d.repeat()
      d = d.shuffle(buffer_size=100)

    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)
    return d

  return input_fn

class LCQMCPairClassificationProcessor(DataProcessor): # TODO NEED CHANGE2
  """"""Processor for the internal data set. sentence pair classification""""""
  def __init__(self):
    self.language = ""zh""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""train.txt"")), ""train"")
    # dev_0827.tsv

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""dev.txt"")), ""dev"")

  def get_test_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""test.txt"")), ""test"")

  def get_labels(self):
    """"""See base class.""""""
    return [""0"", ""1""]
    #return [""-1"",""0"", ""1""]

  def _create_examples(self, lines, set_type):
    """"""Creates examples for the training and dev sets.""""""
    examples = []
    print(""length of lines:"",len(lines))
    for (i, line) in enumerate(lines):
      #print('#i:',i,line)
      if i == 0:
        continue
      guid = ""%s-%s"" % (set_type, i)
      try:
          label = tokenization.convert_to_unicode(line[2])
          text_a = tokenization.convert_to_unicode(line[0])
          text_b = tokenization.convert_to_unicode(line[1])
          examples.append(
              InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
      except Exception:
          print('###error.i:', i, line)
    return examples

class SentencePairClassificationProcessor(DataProcessor):
  """"""Processor for the internal data set. sentence pair classification""""""
  def __init__(self):
    self.language = ""zh""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""train_0827.tsv"")), ""train"")
    # dev_0827.tsv

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""dev_0827.tsv"")), ""dev"")

  def get_test_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""test_0827.tsv"")), ""test"")

  def get_labels(self):
    """"""See base class.""""""
    return [""0"", ""1""]
    #return [""-1"",""0"", ""1""]

  def _create_examples(self, lines, set_type):
    """"""Creates examples for the training and dev sets.""""""
    examples = []
    print(""length of lines:"",len(lines))
    for (i, line) in enumerate(lines):
      #print('#i:',i,line)
      if i == 0:
        continue
      guid = ""%s-%s"" % (set_type, i)
      try:
          label = tokenization.convert_to_unicode(line[0])
          text_a = tokenization.convert_to_unicode(line[1])
          text_b = tokenization.convert_to_unicode(line[2])
          examples.append(
              InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
      except Exception:
          print('###error.i:', i, line)
    return examples

# This function is not used by this file but is still used by the Colab and
# people who depend on it.
def convert_examples_to_features(examples, label_list, max_seq_length,
                                 tokenizer):
  """"""Convert a set of `InputExample`s to a list of `InputFeatures`.""""""

  features = []
  for (ex_index, example) in enumerate(examples):
    if ex_index % 10000 == 0:
      tf.logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))

    feature = convert_single_example(ex_index, example, label_list,
                                     max_seq_length, tokenizer)

    features.append(feature)
  return features


def main(_):
  tf.logging.set_verbosity(tf.logging.INFO)

  processors = {
      ""sentence_pair"": SentencePairClassificationProcessor,
      ""lcqmc_pair"":LCQMCPairClassificationProcessor,
      ""lcqmc"": LCQMCPairClassificationProcessor

  }

  tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case,
                                                FLAGS.init_checkpoint)

  if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:
    raise ValueError(
        ""At least one of `do_train`, `do_eval` or `do_predict' must be True."")

  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)

  if FLAGS.max_seq_length > bert_config.max_position_embeddings:
    raise ValueError(
        ""Cannot use sequence length %d because the BERT model ""
        ""was only trained up to sequence length %d"" %
        (FLAGS.max_seq_length, bert_config.max_position_embeddings))

  tf.gfile.MakeDirs(FLAGS.output_dir)

  task_name = FLAGS.task_name.lower()

  if task_name not in processors:
    raise ValueError(""Task not found: %s"" % (task_name))

  processor = processors[task_name]()

  label_list = processor.get_labels()

  tokenizer = tokenization.FullTokenizer(
      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)

  tpu_cluster_resolver = None
  if FLAGS.use_tpu and FLAGS.tpu_name:
    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)

  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2
  # Cloud TPU: Invalid TPU configuration, ensure ClusterResolver is passed to tpu.
  print(""###tpu_cluster_resolver:"",tpu_cluster_resolver)
  run_config = tf.contrib.tpu.RunConfig(
      cluster=tpu_cluster_resolver,
      master=FLAGS.master,
      model_dir=FLAGS.output_dir,
      save_checkpoints_steps=FLAGS.save_checkpoints_steps,
      tpu_config=tf.contrib.tpu.TPUConfig(
          iterations_per_loop=FLAGS.iterations_per_loop,
          num_shards=FLAGS.num_tpu_cores,
          per_host_input_for_training=is_per_host))

  train_examples = None
  num_train_steps = None
  num_warmup_steps = None
  if FLAGS.do_train:
    train_examples =processor.get_train_examples(FLAGS.data_dir) # TODO
    print(""###length of total train_examples:"",len(train_examples))
    num_train_steps = int(len(train_examples)/ FLAGS.train_batch_size * FLAGS.num_train_epochs)
    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)

  model_fn = model_fn_builder(
      bert_config=bert_config,
      num_labels=len(label_list),
      init_checkpoint=FLAGS.init_checkpoint,
      learning_rate=FLAGS.learning_rate,
      num_train_steps=num_train_steps,
      num_warmup_steps=num_warmup_steps,
      use_tpu=FLAGS.use_tpu,
      use_one_hot_embeddings=FLAGS.use_tpu)

  # If TPU is not available, this will fall back to normal Estimator on CPU
  # or GPU.
  estimator = tf.contrib.tpu.TPUEstimator(
      use_tpu=FLAGS.use_tpu,
      model_fn=model_fn,
      config=run_config,
      train_batch_size=FLAGS.train_batch_size,
      eval_batch_size=FLAGS.eval_batch_size,
      predict_batch_size=FLAGS.predict_batch_size)

  if FLAGS.do_train:
    train_file = os.path.join(FLAGS.output_dir, ""train.tf_record"")
    train_file_exists=os.path.exists(train_file)
    print(""###train_file_exists:"", train_file_exists,"" ;train_file:"",train_file)
    if not train_file_exists: # if tf_record file not exist, convert from raw text file. # TODO
        file_based_convert_examples_to_features(train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)
    tf.logging.info(""***** Running training *****"")
    tf.logging.info(""  Num examples = %d"", len(train_examples))
    tf.logging.info(""  Batch size = %d"", FLAGS.train_batch_size)
    tf.logging.info(""  Num steps = %d"", num_train_steps)
    train_input_fn = file_based_input_fn_builder(
        input_file=train_file,
        seq_length=FLAGS.max_seq_length,
        is_training=True,
        drop_remainder=True)
    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)

  if FLAGS.do_eval:
    eval_examples = processor.get_dev_examples(FLAGS.data_dir)
    num_actual_eval_examples = len(eval_examples)
    if FLAGS.use_tpu:
      # TPU requires a fixed batch size for all batches, therefore the number
      # of examples must be a multiple of the batch size, or else examples
      # will get dropped. So we pad with fake examples which are ignored
      # later on. These do NOT count towards the metric (all tf.metrics
      # support a per-instance weight, and these get a weight of 0.0).
      while len(eval_examples) % FLAGS.eval_batch_size != 0:
        eval_examples.append(PaddingInputExample())

    eval_file = os.path.join(FLAGS.output_dir, ""eval.tf_record"")
    file_based_convert_examples_to_features(
        eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)

    tf.logging.info(""***** Running evaluation *****"")
    tf.logging.info(""  Num examples = %d (%d actual, %d padding)"",
                    len(eval_examples), num_actual_eval_examples,
                    len(eval_examples) - num_actual_eval_examples)
    tf.logging.info(""  Batch size = %d"", FLAGS.eval_batch_size)

    # This tells the estimator to run through the entire set.
    eval_steps = None
    # However, if running eval on the TPU, you will need to specify the
    # number of steps.
    if FLAGS.use_tpu:
      assert len(eval_examples) % FLAGS.eval_batch_size == 0
      eval_steps = int(len(eval_examples) // FLAGS.eval_batch_size)

    eval_drop_remainder = True if FLAGS.use_tpu else False
    eval_input_fn = file_based_input_fn_builder(
        input_file=eval_file,
        seq_length=FLAGS.max_seq_length,
        is_training=False,
        drop_remainder=eval_drop_remainder)

    #######################################################################################################################
    # evaluate all checkpoints; you can use the checkpoint with the best dev accuarcy
    steps_and_files = []
    filenames = tf.gfile.ListDirectory(FLAGS.output_dir)
    for filename in filenames:
        if filename.endswith("".index""):
            ckpt_name = filename[:-6]
            cur_filename = os.path.join(FLAGS.output_dir, ckpt_name)
            global_step = int(cur_filename.split(""-"")[-1])
            tf.logging.info(""Add {} to eval list."".format(cur_filename))
            steps_and_files.append([global_step, cur_filename])
    steps_and_files = sorted(steps_and_files, key=lambda x: x[0])

    output_eval_file = os.path.join(FLAGS.data_dir, ""eval_results_albert_zh.txt"")
    print(""output_eval_file:"",output_eval_file)
    tf.logging.info(""output_eval_file:""+output_eval_file)
    with tf.gfile.GFile(output_eval_file, ""w"") as writer:
        for global_step, filename in sorted(steps_and_files, key=lambda x: x[0]):
            result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps, checkpoint_path=filename)

            tf.logging.info(""***** Eval results %s *****"" % (filename))
            writer.write(""***** Eval results %s *****\n"" % (filename))
            for key in sorted(result.keys()):
                tf.logging.info(""  %s = %s"", key, str(result[key]))
                writer.write(""%s = %s\n"" % (key, str(result[key])))
    #######################################################################################################################

    #result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)
    #
    #output_eval_file = os.path.join(FLAGS.output_dir, ""eval_results.txt"")
    #with tf.gfile.GFile(output_eval_file, ""w"") as writer:
    #  tf.logging.info(""***** Eval results *****"")
    #  for key in sorted(result.keys()):
    #    tf.logging.info(""  %s = %s"", key, str(result[key]))
    #    writer.write(""%s = %s\n"" % (key, str(result[key])))

  if FLAGS.do_predict:
    predict_examples = processor.get_test_examples(FLAGS.data_dir)
    num_actual_predict_examples = len(predict_examples)
    if FLAGS.use_tpu:
      # TPU requires a fixed batch size for all batches, therefore the number
      # of examples must be a multiple of the batch size, or else examples
      # will get dropped. So we pad with fake examples which are ignored
      # later on.
      while len(predict_examples) % FLAGS.predict_batch_size != 0:
        predict_examples.append(PaddingInputExample())

    predict_file = os.path.join(FLAGS.output_dir, ""predict.tf_record"")
    file_based_convert_examples_to_features(predict_examples, label_list,
                                            FLAGS.max_seq_length, tokenizer,
                                            predict_file)

    tf.logging.info(""***** Running prediction*****"")
    tf.logging.info(""  Num examples = %d (%d actual, %d padding)"",
                    len(predict_examples), num_actual_predict_examples,
                    len(predict_examples) - num_actual_predict_examples)
    tf.logging.info(""  Batch size = %d"", FLAGS.predict_batch_size)

    predict_drop_remainder = True if FLAGS.use_tpu else False
    predict_input_fn = file_based_input_fn_builder(
        input_file=predict_file,
        seq_length=FLAGS.max_seq_length,
        is_training=False,
        drop_remainder=predict_drop_remainder)

    result = estimator.predict(input_fn=predict_input_fn)

    output_predict_file = os.path.join(FLAGS.output_dir, ""test_results.tsv"")
    with tf.gfile.GFile(output_predict_file, ""w"") as writer:
      num_written_lines = 0
      tf.logging.info(""***** Predict results *****"")
      for (i, prediction) in enumerate(result):
        probabilities = prediction[""probabilities""]
        if i >= num_actual_predict_examples:
          break
        output_line = ""\t"".join(
            str(class_probability)
            for class_probability in probabilities) + ""\n""
        writer.write(output_line)
        num_written_lines += 1
    assert num_written_lines == num_actual_predict_examples


if __name__ == ""__main__"":
  flags.mark_flag_as_required(""data_dir"")
  flags.mark_flag_as_required(""task_name"")
  flags.mark_flag_as_required(""vocab_file"")
  flags.mark_flag_as_required(""bert_config_file"")
  flags.mark_flag_as_required(""output_dir"")
  tf.app.run()"
Albert,run_classifier_sp_google.py,"# coding=utf-8
# Copyright 2019 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Lint as: python2, python3
""""""BERT finetuning runner with sentence piece tokenization.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import csv
import os

import six
from six.moves import zip
import tensorflow as tf

import modeling_google as modeling
import optimization_google as optimization
import tokenization_google as tokenization
flags = tf.flags

FLAGS = flags.FLAGS

## Required parameters
flags.DEFINE_string(
    ""data_dir"", None,
    ""The input data dir. Should contain the .tsv files (or other data files) ""
    ""for the task."")

flags.DEFINE_string(
    ""albert_config_file"", None,
    ""The config json file corresponding to the pre-trained ALBERT model. ""
    ""This specifies the model architecture."")

flags.DEFINE_string(""task_name"", None, ""The name of the task to train."")

flags.DEFINE_string(
    ""vocab_file"", None,
    ""The vocabulary file that the ALBERT model was trained on."")

flags.DEFINE_string(""spm_model_file"", None,
                    ""The model file for sentence piece tokenization."")

flags.DEFINE_string(
    ""output_dir"", None,
    ""The output directory where the model checkpoints will be written."")

## Other parameters

flags.DEFINE_string(
    ""init_checkpoint"", None,
    ""Initial checkpoint (usually from a pre-trained ALBERT model)."")

flags.DEFINE_bool(
    ""use_pooled_output"", True, ""Whether to use the CLS token outputs"")

flags.DEFINE_bool(
    ""do_lower_case"", True,
    ""Whether to lower case the input text. Should be True for uncased ""
    ""models and False for cased models."")

flags.DEFINE_integer(
    ""max_seq_length"", 512,
    ""The maximum total input sequence length after WordPiece tokenization. ""
    ""Sequences longer than this will be truncated, and sequences shorter ""
    ""than this will be padded."")

flags.DEFINE_bool(""do_train"", False, ""Whether to run training."")

flags.DEFINE_bool(""do_eval"", False, ""Whether to run eval on the dev set."")

flags.DEFINE_bool(
    ""do_predict"", False,
    ""Whether to run the model in inference mode on the test set."")

flags.DEFINE_integer(""train_batch_size"", 32, ""Total batch size for training."")

flags.DEFINE_integer(""eval_batch_size"", 8, ""Total batch size for eval."")

flags.DEFINE_integer(""predict_batch_size"", 8, ""Total batch size for predict."")

flags.DEFINE_float(""learning_rate"", 5e-5, ""The initial learning rate for Adam."")

flags.DEFINE_float(""num_train_epochs"", 3.0,
                   ""Total number of training epochs to perform."")

flags.DEFINE_float(
    ""warmup_proportion"", 0.1,
    ""Proportion of training to perform linear learning rate warmup for. ""
    ""E.g., 0.1 = 10% of training."")

flags.DEFINE_integer(""save_checkpoints_steps"", 1000,
                     ""How often to save the model checkpoint."")

flags.DEFINE_integer(""iterations_per_loop"", 1000,
                     ""How many steps to make in each estimator call."")

flags.DEFINE_bool(""use_tpu"", False, ""Whether to use TPU or GPU/CPU."")

tf.flags.DEFINE_string(
    ""tpu_name"", None,
    ""The Cloud TPU to use for training. This should be either the name ""
    ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 ""
    ""url."")

tf.flags.DEFINE_string(
    ""tpu_zone"", None,
    ""[Optional] GCE zone where the Cloud TPU is located in. If not ""
    ""specified, we will attempt to automatically detect the GCE project from ""
    ""metadata."")

tf.flags.DEFINE_string(
    ""gcp_project"", None,
    ""[Optional] Project name for the Cloud TPU-enabled project. If not ""
    ""specified, we will attempt to automatically detect the GCE project from ""
    ""metadata."")

tf.flags.DEFINE_string(""master"", None, ""[Optional] TensorFlow master URL."")

flags.DEFINE_integer(
    ""num_tpu_cores"", 8,
    ""Only used if `use_tpu` is True. Total number of TPU cores to use."")


class InputExample(object):
  """"""A single training/test example for simple sequence classification.""""""

  def __init__(self, guid, text_a, text_b=None, label=None):
    """"""Constructs a InputExample.
    Args:
      guid: Unique id for the example.
      text_a: string. The untokenized text of the first sequence. For single
        sequence tasks, only this sequence must be specified.
      text_b: (Optional) string. The untokenized text of the second sequence.
        Only must be specified for sequence pair tasks.
      label: (Optional) string. The label of the example. This should be
        specified for train and dev examples, but not for test examples.
    """"""
    self.guid = guid
    self.text_a = text_a
    self.text_b = text_b
    self.label = label


class PaddingInputExample(object):
  """"""Fake example so the num input examples is a multiple of the batch size.
  When running eval/predict on the TPU, we need to pad the number of examples
  to be a multiple of the batch size, because the TPU requires a fixed batch
  size. The alternative is to drop the last batch, which is bad because it means
  the entire output data won't be generated.
  We use this class instead of `None` because treating `None` as padding
  battches could cause silent errors.
  """"""


class InputFeatures(object):
  """"""A single set of features of data.""""""

  def __init__(self,
               input_ids,
               input_mask,
               segment_ids,
               label_id,
               is_real_example=True):
    self.input_ids = input_ids
    self.input_mask = input_mask
    self.segment_ids = segment_ids
    self.label_id = label_id
    self.is_real_example = is_real_example


class DataProcessor(object):
  """"""Base class for data converters for sequence classification data sets.""""""

  def get_train_examples(self, data_dir):
    """"""Gets a collection of `InputExample`s for the train set.""""""
    raise NotImplementedError()

  def get_dev_examples(self, data_dir):
    """"""Gets a collection of `InputExample`s for the dev set.""""""
    raise NotImplementedError()

  def get_test_examples(self, data_dir):
    """"""Gets a collection of `InputExample`s for prediction.""""""
    raise NotImplementedError()

  def get_labels(self):
    """"""Gets the list of labels for this data set.""""""
    raise NotImplementedError()

  @classmethod
  def _read_tsv(cls, input_file, quotechar=None):
    """"""Reads a tab separated value file.""""""
    with tf.gfile.Open(input_file, ""r"") as f:
      reader = csv.reader(f, delimiter=""\t"", quotechar=quotechar)
      lines = []
      for line in reader:
        lines.append(line)
      return lines


class XnliProcessor(DataProcessor):
  """"""Processor for the XNLI data set.""""""

  def __init__(self):
    self.language = ""zh""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    lines = self._read_tsv(
        os.path.join(data_dir, ""multinli"",
                     ""multinli.train.%s.tsv"" % self.language))
    examples = []
    for (i, line) in enumerate(lines):
      if i == 0:
        continue
      guid = ""train-%d"" % (i)
      text_a = tokenization.convert_to_unicode(line[0])
      text_b = tokenization.convert_to_unicode(line[1])
      label = tokenization.convert_to_unicode(line[2])
      if label == tokenization.convert_to_unicode(""contradictory""):
        label = tokenization.convert_to_unicode(""contradiction"")
      examples.append(
          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
    return examples

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    lines = self._read_tsv(os.path.join(data_dir, ""xnli.dev.tsv""))
    examples = []
    for (i, line) in enumerate(lines):
      if i == 0:
        continue
      guid = ""dev-%d"" % (i)
      language = tokenization.convert_to_unicode(line[0])
      if language != tokenization.convert_to_unicode(self.language):
        continue
      text_a = tokenization.convert_to_unicode(line[6])
      text_b = tokenization.convert_to_unicode(line[7])
      label = tokenization.convert_to_unicode(line[1])
      examples.append(
          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
    return examples

  def get_labels(self):
    """"""See base class.""""""
    return [""contradiction"", ""entailment"", ""neutral""]


class MnliProcessor(DataProcessor):
  """"""Processor for the MultiNLI data set (GLUE version).""""""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""dev_matched.tsv"")),
        ""dev_matched"")

  def get_test_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""test_matched.tsv"")), ""test"")

  def get_labels(self):
    """"""See base class.""""""
    return [""contradiction"", ""entailment"", ""neutral""]

  def _create_examples(self, lines, set_type):
    """"""Creates examples for the training and dev sets.""""""
    examples = []
    for (i, line) in enumerate(lines):
      if i == 0:
        continue
    # Note(mingdachen): We will rely on this guid for GLUE submission.
      guid = tokenization.preprocess_text(line[0], lower=FLAGS.do_lower_case)
      text_a = tokenization.preprocess_text(line[8], lower=FLAGS.do_lower_case)
      text_b = tokenization.preprocess_text(line[9], lower=FLAGS.do_lower_case)
      if set_type == ""test"":
        label = ""contradiction""
      else:
        label = tokenization.preprocess_text(line[-1])
      examples.append(
          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
    return examples


class LCQMCPairClassificationProcessor(DataProcessor):
  """"""Processor for the internal data set. sentence pair classification""""""
  def __init__(self):
    self.language = ""zh""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""train.txt"")), ""train"")
    # dev_0827.tsv

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""test.txt"")), ""dev"")

  def get_test_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""test.txt"")), ""test"")

  def get_labels(self):
    """"""See base class.""""""
    return [""0"", ""1""]

  def _create_examples(self, lines, set_type):
    """"""Creates examples for the training and dev sets.""""""
    examples = []
    print(""length of lines:"",len(lines))
    for (i, line) in enumerate(lines):
      if i == 0:
        continue
      guid = ""%s-%s"" % (set_type, i)
      try:
          label = tokenization.convert_to_unicode(line[2])
          text_a = tokenization.convert_to_unicode(line[0])
          text_b = tokenization.convert_to_unicode(line[1])
          examples.append(
              InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
      except Exception:
          print('###error.i:', i, line)
    return examples

class MrpcProcessor(DataProcessor):
  """"""Processor for the MRPC data set (GLUE version).""""""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")

  def get_test_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")

  def get_labels(self):
    """"""See base class.""""""
    return [""0"", ""1""]

  def _create_examples(self, lines, set_type):
    """"""Creates examples for the training and dev sets.""""""
    examples = []
    for (i, line) in enumerate(lines):
      if i == 0:
        continue
      guid = ""%s-%s"" % (set_type, i)
      text_a = tokenization.preprocess_text(line[3], lower=FLAGS.do_lower_case)
      text_b = tokenization.preprocess_text(line[4], lower=FLAGS.do_lower_case)
      if set_type == ""test"":
        guid = line[0]
        label = ""0""
      else:
        label = tokenization.preprocess_text(line[0])
      examples.append(
          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
    return examples


class ColaProcessor(DataProcessor):
  """"""Processor for the CoLA data set (GLUE version).""""""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")

  def get_test_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")

  def get_labels(self):
    """"""See base class.""""""
    return [""0"", ""1""]

  def _create_examples(self, lines, set_type):
    """"""Creates examples for the training and dev sets.""""""
    examples = []
    for (i, line) in enumerate(lines):
      # Only the test set has a header
      if set_type == ""test"" and i == 0:
        continue
      guid = ""%s-%s"" % (set_type, i)
      if set_type == ""test"":
        guid = line[0]
        text_a = tokenization.preprocess_text(
            line[1], lower=FLAGS.do_lower_case)
        label = ""0""
      else:
        text_a = tokenization.preprocess_text(
            line[3], lower=FLAGS.do_lower_case)
        label = tokenization.preprocess_text(line[1])
      examples.append(
          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
    return examples


def convert_single_example(ex_index, example, label_list, max_seq_length,
                           tokenizer):
  """"""Converts a single `InputExample` into a single `InputFeatures`.""""""

  if isinstance(example, PaddingInputExample):
    return InputFeatures(
        input_ids=[0] * max_seq_length,
        input_mask=[0] * max_seq_length,
        segment_ids=[0] * max_seq_length,
        label_id=0,
        is_real_example=False)

  label_map = {}
  for (i, label) in enumerate(label_list):
    label_map[label] = i

  tokens_a = tokenizer.tokenize(example.text_a)
  tokens_b = None
  if example.text_b:
    tokens_b = tokenizer.tokenize(example.text_b)

  if tokens_b:
    # Modifies `tokens_a` and `tokens_b` in place so that the total
    # length is less than the specified length.
    # Account for [CLS], [SEP], [SEP] with ""- 3""
    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)
  else:
    # Account for [CLS] and [SEP] with ""- 2""
    if len(tokens_a) > max_seq_length - 2:
      tokens_a = tokens_a[0:(max_seq_length - 2)]

  # The convention in ALBERT is:
  # (a) For sequence pairs:
  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]
  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
  # (b) For single sequences:
  #  tokens:   [CLS] the dog is hairy . [SEP]
  #  type_ids: 0     0   0   0  0     0 0
  #
  # Where ""type_ids"" are used to indicate whether this is the first
  # sequence or the second sequence. The embedding vectors for `type=0` and
  # `type=1` were learned during pre-training and are added to the wordpiece
  # embedding vector (and position vector). This is not *strictly* necessary
  # since the [SEP] token unambiguously separates the sequences, but it makes
  # it easier for the model to learn the concept of sequences.
  #
  # For classification tasks, the first vector (corresponding to [CLS]) is
  # used as the ""sentence vector"". Note that this only makes sense because
  # the entire model is fine-tuned.
  tokens = []
  segment_ids = []
  tokens.append(""[CLS]"")
  segment_ids.append(0)
  for token in tokens_a:
    tokens.append(token)
    segment_ids.append(0)
  tokens.append(""[SEP]"")
  segment_ids.append(0)

  if tokens_b:
    for token in tokens_b:
      tokens.append(token)
      segment_ids.append(1)
    tokens.append(""[SEP]"")
    segment_ids.append(1)

  input_ids = tokenizer.convert_tokens_to_ids(tokens)

  # The mask has 1 for real tokens and 0 for padding tokens. Only real
  # tokens are attended to.
  input_mask = [1] * len(input_ids)

  # Zero-pad up to the sequence length.
  while len(input_ids) < max_seq_length:
    input_ids.append(0)
    input_mask.append(0)
    segment_ids.append(0)

  assert len(input_ids) == max_seq_length
  assert len(input_mask) == max_seq_length
  assert len(segment_ids) == max_seq_length

  label_id = label_map[example.label]
  if ex_index < 5:
    tf.logging.info(""*** Example ***"")
    tf.logging.info(""guid: %s"" % (example.guid))
    tf.logging.info(""tokens: %s"" % "" "".join(
        [tokenization.printable_text(x) for x in tokens]))
    tf.logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))
    tf.logging.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))
    tf.logging.info(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))
    tf.logging.info(""label: %s (id = %d)"" % (example.label, label_id))

  feature = InputFeatures(
      input_ids=input_ids,
      input_mask=input_mask,
      segment_ids=segment_ids,
      label_id=label_id,
      is_real_example=True)
  return feature


def file_based_convert_examples_to_features(
    examples, label_list, max_seq_length, tokenizer, output_file):
  """"""Convert a set of `InputExample`s to a TFRecord file.""""""

  writer = tf.python_io.TFRecordWriter(output_file)

  for (ex_index, example) in enumerate(examples):
    if ex_index % 10000 == 0:
      tf.logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))

    feature = convert_single_example(ex_index, example, label_list,
                                     max_seq_length, tokenizer)

    def create_int_feature(values):
      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))
      return f

    features = collections.OrderedDict()
    features[""input_ids""] = create_int_feature(feature.input_ids)
    features[""input_mask""] = create_int_feature(feature.input_mask)
    features[""segment_ids""] = create_int_feature(feature.segment_ids)
    features[""label_ids""] = create_int_feature([feature.label_id])
    features[""is_real_example""] = create_int_feature(
        [int(feature.is_real_example)])

    tf_example = tf.train.Example(features=tf.train.Features(feature=features))
    writer.write(tf_example.SerializeToString())
  writer.close()


def file_based_input_fn_builder(input_file, seq_length, is_training,
                                drop_remainder):
  """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""

  name_to_features = {
      ""input_ids"": tf.FixedLenFeature([seq_length], tf.int64),
      ""input_mask"": tf.FixedLenFeature([seq_length], tf.int64),
      ""segment_ids"": tf.FixedLenFeature([seq_length], tf.int64),
      ""label_ids"": tf.FixedLenFeature([], tf.int64),
      ""is_real_example"": tf.FixedLenFeature([], tf.int64),
  }

  def _decode_record(record, name_to_features):
    """"""Decodes a record to a TensorFlow example.""""""
    example = tf.parse_single_example(record, name_to_features)

    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.
    # So cast all int64 to int32.
    for name in list(example.keys()):
      t = example[name]
      if t.dtype == tf.int64:
        t = tf.to_int32(t)
      example[name] = t

    return example

  def input_fn(params):
    """"""The actual input function.""""""
    batch_size = params[""batch_size""]

    # For training, we want a lot of parallel reading and shuffling.
    # For eval, we want no shuffling and parallel reading doesn't matter.
    d = tf.data.TFRecordDataset(input_file)
    if is_training:
      d = d.repeat()
      d = d.shuffle(buffer_size=100)

    d = d.apply(
        tf.contrib.data.map_and_batch(
            lambda record: _decode_record(record, name_to_features),
            batch_size=batch_size,
            drop_remainder=drop_remainder))

    return d

  return input_fn


def _truncate_seq_pair(tokens_a, tokens_b, max_length):
  """"""Truncates a sequence pair in place to the maximum length.""""""

  # This is a simple heuristic which will always truncate the longer sequence
  # one token at a time. This makes more sense than truncating an equal percent
  # of tokens from each, since if one sequence is very short then each token
  # that's truncated likely contains more information than a longer sequence.
  while True:
    total_length = len(tokens_a) + len(tokens_b)
    if total_length <= max_length:
      break
    if len(tokens_a) > len(tokens_b):
      tokens_a.pop()
    else:
      tokens_b.pop()


def create_model(albert_config, is_training, input_ids, input_mask, segment_ids,
                 labels, num_labels, use_one_hot_embeddings):
  """"""Creates a classification model.""""""
  model = modeling.AlbertModel(
      config=albert_config,
      is_training=is_training,
      input_ids=input_ids,
      input_mask=input_mask,
      token_type_ids=segment_ids,
      use_one_hot_embeddings=use_one_hot_embeddings)

  # In the demo, we are doing a simple classification task on the entire
  # segment.
  #
  # If you want to use the token-level output, use model.get_sequence_output()
  # instead.
  if FLAGS.use_pooled_output:
    tf.logging.info(""using pooled output"")
    output_layer = model.get_pooled_output()
  else:
    tf.logging.info(""using meaned output"")
    output_layer = tf.reduce_mean(model.get_sequence_output(), axis=1)

  hidden_size = output_layer.shape[-1].value

  output_weights = tf.get_variable(
      ""output_weights"", [num_labels, hidden_size],
      initializer=tf.truncated_normal_initializer(stddev=0.02))

  output_bias = tf.get_variable(
      ""output_bias"", [num_labels], initializer=tf.zeros_initializer())

  with tf.variable_scope(""loss""):
    if is_training:
      # I.e., 0.1 dropout
      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)

    logits = tf.matmul(output_layer, output_weights, transpose_b=True)
    logits = tf.nn.bias_add(logits, output_bias)
    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)
    probabilities = tf.nn.softmax(logits, axis=-1)
    log_probs = tf.nn.log_softmax(logits, axis=-1)

    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)

    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)
    loss = tf.reduce_mean(per_example_loss)

    return (loss, per_example_loss, probabilities, predictions)


def model_fn_builder(albert_config, num_labels, init_checkpoint, learning_rate,
                     num_train_steps, num_warmup_steps, use_tpu,
                     use_one_hot_embeddings):
  """"""Returns `model_fn` closure for TPUEstimator.""""""

  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
    """"""The `model_fn` for TPUEstimator.""""""

    tf.logging.info(""*** Features ***"")
    for name in sorted(features.keys()):
      tf.logging.info(""  name = %s, shape = %s"" % (name, features[name].shape))

    input_ids = features[""input_ids""]
    input_mask = features[""input_mask""]
    segment_ids = features[""segment_ids""]
    label_ids = features[""label_ids""]
    is_real_example = None
    if ""is_real_example"" in features:
      is_real_example = tf.cast(features[""is_real_example""], dtype=tf.float32)
    else:
      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)

    is_training = (mode == tf.estimator.ModeKeys.TRAIN)

    (total_loss, per_example_loss, probabilities, predictions) = \
        create_model(albert_config, is_training, input_ids, input_mask,
                     segment_ids, label_ids, num_labels, use_one_hot_embeddings)

    tvars = tf.trainable_variables()
    initialized_variable_names = {}
    scaffold_fn = None
    if init_checkpoint:
      (assignment_map, initialized_variable_names
      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)
      if use_tpu:

        def tpu_scaffold():
          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)
          return tf.train.Scaffold()

        scaffold_fn = tpu_scaffold
      else:
        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)

    tf.logging.info(""**** Trainable Variables ****"")
    for var in tvars:
      init_string = """"
      if var.name in initialized_variable_names:
        init_string = "", *INIT_FROM_CKPT*""
      tf.logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,
                      init_string)

    output_spec = None
    if mode == tf.estimator.ModeKeys.TRAIN:

      train_op = optimization.create_optimizer(
          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)

      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          loss=total_loss,
          train_op=train_op,
          scaffold_fn=scaffold_fn)
    elif mode == tf.estimator.ModeKeys.EVAL:

      def metric_fn(per_example_loss, label_ids, predictions, is_real_example):
        accuracy = tf.metrics.accuracy(
            labels=label_ids, predictions=predictions, weights=is_real_example)
        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)
        return {
            ""eval_accuracy"": accuracy,
            ""eval_loss"": loss,
        }

      eval_metrics = (metric_fn,
                      [per_example_loss, label_ids,
                       predictions, is_real_example])
      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          loss=total_loss,
          eval_metrics=eval_metrics,
          scaffold_fn=scaffold_fn)
    else:
      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          predictions={""probabilities"": probabilities,
                       ""predictions"": predictions},
          scaffold_fn=scaffold_fn)
    return output_spec

  return model_fn


# This function is not used by this file but is still used by the Colab and
# people who depend on it.
def input_fn_builder(features, seq_length, is_training, drop_remainder):
  """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""

  all_input_ids = []
  all_input_mask = []
  all_segment_ids = []
  all_label_ids = []

  for feature in features:
    all_input_ids.append(feature.input_ids)
    all_input_mask.append(feature.input_mask)
    all_segment_ids.append(feature.segment_ids)
    all_label_ids.append(feature.label_id)

  def input_fn(params):
    """"""The actual input function.""""""
    batch_size = params[""batch_size""]

    num_examples = len(features)

    # This is for demo purposes and does NOT scale to large data sets. We do
    # not use Dataset.from_generator() because that uses tf.py_func which is
    # not TPU compatible. The right way to load data is with TFRecordReader.
    d = tf.data.Dataset.from_tensor_slices({
        ""input_ids"":
            tf.constant(
                all_input_ids, shape=[num_examples, seq_length],
                dtype=tf.int32),
        ""input_mask"":
            tf.constant(
                all_input_mask,
                shape=[num_examples, seq_length],
                dtype=tf.int32),
        ""segment_ids"":
            tf.constant(
                all_segment_ids,
                shape=[num_examples, seq_length],
                dtype=tf.int32),
        ""label_ids"":
            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),
    })

    if is_training:
      d = d.repeat()
      d = d.shuffle(buffer_size=100)

    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)
    return d

  return input_fn


# This function is not used by this file but is still used by the Colab and
# people who depend on it.
def convert_examples_to_features(examples, label_list, max_seq_length,
                                 tokenizer):
  """"""Convert a set of `InputExample`s to a list of `InputFeatures`.""""""

  features = []
  for (ex_index, example) in enumerate(examples):
    if ex_index % 10000 == 0:
      tf.logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))

    feature = convert_single_example(ex_index, example, label_list,
                                     max_seq_length, tokenizer)

    features.append(feature)
  return features


def main(_):
  tf.logging.set_verbosity(tf.logging.INFO)

  processors = {
      ""cola"": ColaProcessor,
      ""mnli"": MnliProcessor,
      ""mrpc"": MrpcProcessor,
      ""xnli"": XnliProcessor,
      ""lcqmc_pair"": LCQMCPairClassificationProcessor

  }

  tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case,
                                                FLAGS.init_checkpoint)

  if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:
    raise ValueError(
        ""At least one of `do_train`, `do_eval` or `do_predict' must be True."")

  albert_config = modeling.AlbertConfig.from_json_file(FLAGS.albert_config_file)

  if FLAGS.max_seq_length > albert_config.max_position_embeddings:
    raise ValueError(
        ""Cannot use sequence length %d because the ALBERT model ""
        ""was only trained up to sequence length %d"" %
        (FLAGS.max_seq_length, albert_config.max_position_embeddings))

  tf.gfile.MakeDirs(FLAGS.output_dir)

  task_name = FLAGS.task_name.lower()

  if task_name not in processors:
    raise ValueError(""Task not found: %s"" % (task_name))

  processor = processors[task_name]()

  label_list = processor.get_labels()

  tokenizer = tokenization.FullTokenizer(
      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case,
      spm_model_file=FLAGS.spm_model_file)

  tpu_cluster_resolver = None
  if FLAGS.use_tpu and FLAGS.tpu_name:
    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)

  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2
  run_config = tf.contrib.tpu.RunConfig(
      cluster=tpu_cluster_resolver,
      master=FLAGS.master,
      model_dir=FLAGS.output_dir,
      save_checkpoints_steps=FLAGS.save_checkpoints_steps,
      tpu_config=tf.contrib.tpu.TPUConfig(
          iterations_per_loop=FLAGS.iterations_per_loop,
          num_shards=FLAGS.num_tpu_cores,
          per_host_input_for_training=is_per_host))

  train_examples = None
  num_train_steps = None
  num_warmup_steps = None
  if FLAGS.do_train:
    train_examples = processor.get_train_examples(FLAGS.data_dir)
    num_train_steps = int(
        len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)
    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)

  model_fn = model_fn_builder(
      albert_config=albert_config,
      num_labels=len(label_list),
      init_checkpoint=FLAGS.init_checkpoint,
      learning_rate=FLAGS.learning_rate,
      num_train_steps=num_train_steps,
      num_warmup_steps=num_warmup_steps,
      use_tpu=FLAGS.use_tpu,
      use_one_hot_embeddings=FLAGS.use_tpu)

  # If TPU is not available, this will fall back to normal Estimator on CPU
  # or GPU.
  estimator = tf.contrib.tpu.TPUEstimator(
      use_tpu=FLAGS.use_tpu,
      model_fn=model_fn,
      config=run_config,
      train_batch_size=FLAGS.train_batch_size,
      eval_batch_size=FLAGS.eval_batch_size,
      predict_batch_size=FLAGS.predict_batch_size)

  if FLAGS.do_train:
    train_file = os.path.join(FLAGS.output_dir, ""train.tf_record"")
    file_based_convert_examples_to_features(
        train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)
    tf.logging.info(""***** Running training *****"")
    tf.logging.info(""  Num examples = %d"", len(train_examples))
    tf.logging.info(""  Batch size = %d"", FLAGS.train_batch_size)
    tf.logging.info(""  Num steps = %d"", num_train_steps)
    train_input_fn = file_based_input_fn_builder(
        input_file=train_file,
        seq_length=FLAGS.max_seq_length,
        is_training=True,
        drop_remainder=True)
    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)

  if FLAGS.do_eval:
    eval_examples = processor.get_dev_examples(FLAGS.data_dir)
    num_actual_eval_examples = len(eval_examples)
    if FLAGS.use_tpu:
      # TPU requires a fixed batch size for all batches, therefore the number
      # of examples must be a multiple of the batch size, or else examples
      # will get dropped. So we pad with fake examples which are ignored
      # later on. These do NOT count towards the metric (all tf.metrics
      # support a per-instance weight, and these get a weight of 0.0).
      while len(eval_examples) % FLAGS.eval_batch_size != 0:
        eval_examples.append(PaddingInputExample())

    eval_file = os.path.join(FLAGS.output_dir, ""eval.tf_record"")
    file_based_convert_examples_to_features(
        eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)

    tf.logging.info(""***** Running evaluation *****"")
    tf.logging.info(""  Num examples = %d (%d actual, %d padding)"",
                    len(eval_examples), num_actual_eval_examples,
                    len(eval_examples) - num_actual_eval_examples)
    tf.logging.info(""  Batch size = %d"", FLAGS.eval_batch_size)

    # This tells the estimator to run through the entire set.
    eval_steps = None
    # However, if running eval on the TPU, you will need to specify the
    # number of steps.
    if FLAGS.use_tpu:
      assert len(eval_examples) % FLAGS.eval_batch_size == 0
      eval_steps = int(len(eval_examples) // FLAGS.eval_batch_size)

    eval_drop_remainder = True if FLAGS.use_tpu else False
    eval_input_fn = file_based_input_fn_builder(
        input_file=eval_file,
        seq_length=FLAGS.max_seq_length,
        is_training=False,
        drop_remainder=eval_drop_remainder)

    #######################################################################################################################
    # evaluate all checkpoints; you can use the checkpoint with the best dev accuarcy
    steps_and_files = []
    filenames = tf.gfile.ListDirectory(FLAGS.output_dir)
    for filename in filenames:
        if filename.endswith("".index""):
            ckpt_name = filename[:-6]
            cur_filename = os.path.join(FLAGS.output_dir, ckpt_name)
            global_step = int(cur_filename.split(""-"")[-1])
            tf.logging.info(""Add {} to eval list."".format(cur_filename))
            steps_and_files.append([global_step, cur_filename])
    steps_and_files = sorted(steps_and_files, key=lambda x: x[0])

    output_eval_file = os.path.join(FLAGS.data_dir, ""eval_results_albert_zh.txt"")
    print(""output_eval_file:"",output_eval_file)
    tf.logging.info(""output_eval_file:""+output_eval_file)
    with tf.gfile.GFile(output_eval_file, ""w"") as writer:
        for global_step, filename in sorted(steps_and_files, key=lambda x: x[0]):
            result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps, checkpoint_path=filename)

            tf.logging.info(""***** Eval results %s *****"" % (filename))
            writer.write(""***** Eval results %s *****\n"" % (filename))
            for key in sorted(result.keys()):
                tf.logging.info(""  %s = %s"", key, str(result[key]))
                writer.write(""%s = %s\n"" % (key, str(result[key])))
    #######################################################################################################################
    # result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)
    # output_eval_file = os.path.join(FLAGS.output_dir, ""eval_results.txt"")
    # with tf.gfile.GFile(output_eval_file, ""w"") as writer:
    #  tf.logging.info(""***** Eval results *****"")
    #  for key in sorted(result.keys()):
    #    tf.logging.info(""  %s = %s"", key, str(result[key]))
    #    writer.write(""%s = %s\n"" % (key, str(result[key])))

  if FLAGS.do_predict:
    predict_examples = processor.get_test_examples(FLAGS.data_dir)
    num_actual_predict_examples = len(predict_examples)
    if FLAGS.use_tpu:
      # TPU requires a fixed batch size for all batches, therefore the number
      # of examples must be a multiple of the batch size, or else examples
      # will get dropped. So we pad with fake examples which are ignored
      # later on.
      while len(predict_examples) % FLAGS.predict_batch_size != 0:
        predict_examples.append(PaddingInputExample())

    predict_file = os.path.join(FLAGS.output_dir, ""predict.tf_record"")
    file_based_convert_examples_to_features(predict_examples, label_list,
                                            FLAGS.max_seq_length, tokenizer,
                                            predict_file)

    tf.logging.info(""***** Running prediction*****"")
    tf.logging.info(""  Num examples = %d (%d actual, %d padding)"",
                    len(predict_examples), num_actual_predict_examples,
                    len(predict_examples) - num_actual_predict_examples)
    tf.logging.info(""  Batch size = %d"", FLAGS.predict_batch_size)

    predict_drop_remainder = True if FLAGS.use_tpu else False
    predict_input_fn = file_based_input_fn_builder(
        input_file=predict_file,
        seq_length=FLAGS.max_seq_length,
        is_training=False,
        drop_remainder=predict_drop_remainder)

    result = estimator.predict(input_fn=predict_input_fn)

    output_predict_file = os.path.join(FLAGS.output_dir, ""test_results.tsv"")
    output_submit_file = os.path.join(FLAGS.output_dir, ""submit_results.tsv"")
    with tf.gfile.GFile(output_predict_file, ""w"") as pred_writer,\
        tf.gfile.GFile(output_submit_file, ""w"") as sub_writer:
      num_written_lines = 0
      tf.logging.info(""***** Predict results *****"")
      for (i, (example, prediction)) in\
          enumerate(zip(predict_examples, result)):
        probabilities = prediction[""probabilities""]
        if i >= num_actual_predict_examples:
          break
        output_line = ""\t"".join(
            str(class_probability)
            for class_probability in probabilities) + ""\n""
        pred_writer.write(output_line)

        actual_label = label_list[int(prediction[""predictions""])]
        sub_writer.write(
            six.ensure_str(example.guid) + ""\t"" + actual_label + ""\n"")
        num_written_lines += 1
    assert num_written_lines == num_actual_predict_examples


if __name__ == ""__main__"":
  flags.mark_flag_as_required(""data_dir"")
  flags.mark_flag_as_required(""task_name"")
  flags.mark_flag_as_required(""vocab_file"")
  flags.mark_flag_as_required(""albert_config_file"")
  flags.mark_flag_as_required(""output_dir"")
  tf.app.run()"
Albert,optimization_google.py,"# coding=utf-8
# Copyright 2019 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Lint as: python2, python3
""""""Functions and classes related to optimization (weight updates).""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import re

import six
from six.moves import zip
import tensorflow as tf

import lamb_optimizer_google as lamb_optimizer


def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu,
                     optimizer=""adamw"", poly_power=1.0, start_warmup_step=0):
  """"""Creates an optimizer training op.""""""
  global_step = tf.train.get_or_create_global_step()

  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)

  # Implements linear decay of the learning rate.
  learning_rate = tf.train.polynomial_decay(
      learning_rate,
      global_step,
      num_train_steps,
      end_learning_rate=0.0,
      power=poly_power,
      cycle=False)

  # Implements linear warmup. I.e., if global_step - start_warmup_step <
  # num_warmup_steps, the learning rate will be
  # `(global_step - start_warmup_step)/num_warmup_steps * init_lr`.
  if num_warmup_steps:
    tf.logging.info(""++++++ warmup starts at step "" + str(start_warmup_step)
                    + "", for "" + str(num_warmup_steps) + "" steps ++++++"")
    global_steps_int = tf.cast(global_step, tf.int32)
    start_warm_int = tf.constant(start_warmup_step, dtype=tf.int32)
    global_steps_int = global_steps_int - start_warm_int
    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)

    global_steps_float = tf.cast(global_steps_int, tf.float32)
    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)

    warmup_percent_done = global_steps_float / warmup_steps_float
    warmup_learning_rate = init_lr * warmup_percent_done

    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)
    learning_rate = (
        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)

  # It is OK that you use this optimizer for finetuning, since this
  # is how the model was trained (note that the Adam m/v variables are NOT
  # loaded from init_checkpoint.)
  # It is OK to use AdamW in the finetuning even the model is trained by LAMB.
  # As report in the Bert pulic github, the learning rate for SQuAD 1.1 finetune
  # is 3e-5, 4e-5 or 5e-5. For LAMB, the users can use 3e-4, 4e-4,or 5e-4 for a
  # batch size of 64 in the finetune.
  if optimizer == ""adamw"":
    tf.logging.info(""using adamw"")
    optimizer = AdamWeightDecayOptimizer(
        learning_rate=learning_rate,
        weight_decay_rate=0.01,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-6,
        exclude_from_weight_decay=[""LayerNorm"", ""layer_norm"", ""bias""])
  elif optimizer == ""lamb"":
    tf.logging.info(""using lamb"")
    optimizer = lamb_optimizer.LAMBOptimizer(
        learning_rate=learning_rate,
        weight_decay_rate=0.01,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-6,
        exclude_from_weight_decay=[""LayerNorm"", ""layer_norm"", ""bias""])
  else:
    raise ValueError(""Not supported optimizer: "", optimizer)

  if use_tpu:
    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)

  tvars = tf.trainable_variables()
  grads = tf.gradients(loss, tvars)

  # This is how the model was pre-trained.
  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)

  train_op = optimizer.apply_gradients(
      list(zip(grads, tvars)), global_step=global_step)

  # Normally the global step update is done inside of `apply_gradients`.
  # However, neither `AdamWeightDecayOptimizer` nor `LAMBOptimizer` do this.
  # But if you use a different optimizer, you should probably take this line
  # out.
  new_global_step = global_step + 1
  train_op = tf.group(train_op, [global_step.assign(new_global_step)])
  return train_op


class AdamWeightDecayOptimizer(tf.train.Optimizer):
  """"""A basic Adam optimizer that includes ""correct"" L2 weight decay.""""""

  def __init__(self,
               learning_rate,
               weight_decay_rate=0.0,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-6,
               exclude_from_weight_decay=None,
               name=""AdamWeightDecayOptimizer""):
    """"""Constructs a AdamWeightDecayOptimizer.""""""
    super(AdamWeightDecayOptimizer, self).__init__(False, name)

    self.learning_rate = learning_rate
    self.weight_decay_rate = weight_decay_rate
    self.beta_1 = beta_1
    self.beta_2 = beta_2
    self.epsilon = epsilon
    self.exclude_from_weight_decay = exclude_from_weight_decay

  def apply_gradients(self, grads_and_vars, global_step=None, name=None):
    """"""See base class.""""""
    assignments = []
    for (grad, param) in grads_and_vars:
      if grad is None or param is None:
        continue

      param_name = self._get_variable_name(param.name)

      m = tf.get_variable(
          name=six.ensure_str(param_name) + ""/adam_m"",
          shape=param.shape.as_list(),
          dtype=tf.float32,
          trainable=False,
          initializer=tf.zeros_initializer())
      v = tf.get_variable(
          name=six.ensure_str(param_name) + ""/adam_v"",
          shape=param.shape.as_list(),
          dtype=tf.float32,
          trainable=False,
          initializer=tf.zeros_initializer())

      # Standard Adam update.
      next_m = (
          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))
      next_v = (
          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,
                                                    tf.square(grad)))

      update = next_m / (tf.sqrt(next_v) + self.epsilon)

      # Just adding the square of the weights to the loss function is *not*
      # the correct way of using L2 regularization/weight decay with Adam,
      # since that will interact with the m and v parameters in strange ways.
      #
      # Instead we want ot decay the weights in a manner that doesn't interact
      # with the m/v parameters. This is equivalent to adding the square
      # of the weights to the loss with plain (non-momentum) SGD.
      if self._do_use_weight_decay(param_name):
        update += self.weight_decay_rate * param

      update_with_lr = self.learning_rate * update

      next_param = param - update_with_lr

      assignments.extend(
          [param.assign(next_param),
           m.assign(next_m),
           v.assign(next_v)])
    return tf.group(*assignments, name=name)

  def _do_use_weight_decay(self, param_name):
    """"""Whether to use L2 weight decay for `param_name`.""""""
    if not self.weight_decay_rate:
      return False
    if self.exclude_from_weight_decay:
      for r in self.exclude_from_weight_decay:
        if re.search(r, param_name) is not None:
          return False
    return True

  def _get_variable_name(self, param_name):
    """"""Get the variable name from the tensor name.""""""
    m = re.match(""^(.*):\\d+$"", six.ensure_str(param_name))
    if m is not None:
      param_name = m.group(1)
    return param_name
"
Albert,run_pretraining_google_fast.py,"# coding=utf-8
# Copyright 2019 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Lint as: python2, python3
""""""Run masked LM/next sentence masked_lm pre-training for ALBERT.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import time

from six.moves import range
import tensorflow as tf

import modeling_google_fast as modeling
import optimization_google as optimization

flags = tf.flags

FLAGS = flags.FLAGS

## Required parameters
flags.DEFINE_string(
    ""albert_config_file"", None,
    ""The config json file corresponding to the pre-trained ALBERT model. ""
    ""This specifies the model architecture."")

flags.DEFINE_string(
    ""input_file"", None,
    ""Input TF example files (can be a glob or comma separated)."")

flags.DEFINE_string(
    ""output_dir"", None,
    ""The output directory where the model checkpoints will be written."")

flags.DEFINE_string(
    ""export_dir"", None,
    ""The output directory where the saved models will be written."")
## Other parameters
flags.DEFINE_string(
    ""init_checkpoint"", None,
    ""Initial checkpoint (usually from a pre-trained ALBERT model)."")

flags.DEFINE_integer(
    ""max_seq_length"", 512,
    ""The maximum total input sequence length after WordPiece tokenization. ""
    ""Sequences longer than this will be truncated, and sequences shorter ""
    ""than this will be padded. Must match data generation."")

flags.DEFINE_integer(
    ""max_predictions_per_seq"", 20,
    ""Maximum number of masked LM predictions per sequence. ""
    ""Must match data generation."")

flags.DEFINE_bool(""do_train"", True, ""Whether to run training."")

flags.DEFINE_bool(""do_eval"", False, ""Whether to run eval on the dev set."")

flags.DEFINE_integer(""train_batch_size"", 4096, ""Total batch size for training."")

flags.DEFINE_integer(""eval_batch_size"", 64, ""Total batch size for eval."")

flags.DEFINE_enum(""optimizer"", ""lamb"", [""adamw"", ""lamb""],
                  ""The optimizer for training."")

flags.DEFINE_float(""learning_rate"", 0.00176, ""The initial learning rate."")

flags.DEFINE_float(""poly_power"", 1.0, ""The power of poly decay."")

flags.DEFINE_integer(""num_train_steps"", 125000, ""Number of training steps."")

flags.DEFINE_integer(""num_warmup_steps"", 3125, ""Number of warmup steps."")

flags.DEFINE_integer(""start_warmup_step"", 0, ""The starting step of warmup."")

flags.DEFINE_integer(""save_checkpoints_steps"", 5000,
                     ""How often to save the model checkpoint."")

flags.DEFINE_integer(""iterations_per_loop"", 1000,
                     ""How many steps to make in each estimator call."")

flags.DEFINE_integer(""max_eval_steps"", 100, ""Maximum number of eval steps."")

flags.DEFINE_bool(""use_tpu"", False, ""Whether to use TPU or GPU/CPU."")

flags.DEFINE_bool(""init_from_group0"", False, ""Whether to initialize""
                  ""parameters of other groups from group 0"")

tf.flags.DEFINE_string(
    ""tpu_name"", None,
    ""The Cloud TPU to use for training. This should be either the name ""
    ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 ""
    ""url."")

tf.flags.DEFINE_string(
    ""tpu_zone"", None,
    ""[Optional] GCE zone where the Cloud TPU is located in. If not ""
    ""specified, we will attempt to automatically detect the GCE project from ""
    ""metadata."")

tf.flags.DEFINE_string(
    ""gcp_project"", None,
    ""[Optional] Project name for the Cloud TPU-enabled project. If not ""
    ""specified, we will attempt to automatically detect the GCE project from ""
    ""metadata."")

tf.flags.DEFINE_string(""master"", None, ""[Optional] TensorFlow master URL."")

flags.DEFINE_integer(
    ""num_tpu_cores"", 8,
    ""Only used if `use_tpu` is True. Total number of TPU cores to use."")

flags.DEFINE_float(
    ""masked_lm_budget"", 0,
    ""If >0, the ratio of masked ngrams to unmasked ngrams. Default 0,""
    ""for offline masking"")


def model_fn_builder(albert_config, init_checkpoint, learning_rate,
                     num_train_steps, num_warmup_steps, use_tpu,
                     use_one_hot_embeddings, optimizer, poly_power,
                     start_warmup_step):
  """"""Returns `model_fn` closure for TPUEstimator.""""""

  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
    """"""The `model_fn` for TPUEstimator.""""""

    tf.logging.info(""*** Features ***"")
    for name in sorted(features.keys()):
      tf.logging.info(""  name = %s, shape = %s"" % (name, features[name].shape))

    input_ids = features[""input_ids""]
    input_mask = features[""input_mask""]
    segment_ids = features[""segment_ids""]
    masked_lm_positions = features[""masked_lm_positions""]
    masked_lm_ids = features[""masked_lm_ids""]
    masked_lm_weights = features[""masked_lm_weights""]
    # Note: We keep this feature name `next_sentence_labels` to be compatible
    # with the original data created by lanzhzh@. However, in the ALBERT case
    # it does represent sentence_order_labels.
    sentence_order_labels = features[""next_sentence_labels""]

    is_training = (mode == tf.estimator.ModeKeys.TRAIN)

    model = modeling.AlbertModel(
        config=albert_config,
        is_training=is_training,
        input_ids=input_ids,
        input_mask=input_mask,
        token_type_ids=segment_ids,
        use_one_hot_embeddings=use_one_hot_embeddings)

    (masked_lm_loss, masked_lm_example_loss,
     masked_lm_log_probs) = get_masked_lm_output(albert_config,
                                                 model.get_sequence_output(),
                                                 model.get_embedding_table(),
                                                 masked_lm_positions,
                                                 masked_lm_ids,
                                                 masked_lm_weights)

    (sentence_order_loss, sentence_order_example_loss,
     sentence_order_log_probs) = get_sentence_order_output(
         albert_config, model.get_pooled_output(), sentence_order_labels)

    total_loss = masked_lm_loss + sentence_order_loss

    tvars = tf.trainable_variables()

    initialized_variable_names = {}
    scaffold_fn = None
    if init_checkpoint:
      tf.logging.info(""number of hidden group %d to initialize"",
                      albert_config.num_hidden_groups)
      num_of_initialize_group = 1
      if FLAGS.init_from_group0:
        num_of_initialize_group = albert_config.num_hidden_groups
        if albert_config.net_structure_type > 0:
          num_of_initialize_group = albert_config.num_hidden_layers
      (assignment_map, initialized_variable_names
      ) = modeling.get_assignment_map_from_checkpoint(
              tvars, init_checkpoint, num_of_initialize_group)
      if use_tpu:

        def tpu_scaffold():
          for gid in range(num_of_initialize_group):
            tf.logging.info(""initialize the %dth layer"", gid)
            tf.logging.info(assignment_map[gid])
            tf.train.init_from_checkpoint(init_checkpoint, assignment_map[gid])
          return tf.train.Scaffold()

        scaffold_fn = tpu_scaffold
      else:
        for gid in range(num_of_initialize_group):
          tf.logging.info(""initialize the %dth layer"", gid)
          tf.logging.info(assignment_map[gid])
          tf.train.init_from_checkpoint(init_checkpoint, assignment_map[gid])

    tf.logging.info(""**** Trainable Variables ****"")
    for var in tvars:
      init_string = """"
      if var.name in initialized_variable_names:
        init_string = "", *INIT_FROM_CKPT*""
      tf.logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,
                      init_string)

    output_spec = None
    if mode == tf.estimator.ModeKeys.TRAIN:
      train_op = optimization.create_optimizer(
          total_loss, learning_rate, num_train_steps, num_warmup_steps,
          use_tpu, optimizer, poly_power, start_warmup_step)

      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          loss=total_loss,
          train_op=train_op,
          scaffold_fn=scaffold_fn)
    elif mode == tf.estimator.ModeKeys.EVAL:

      def metric_fn(*args):
        """"""Computes the loss and accuracy of the model.""""""
        (masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,
         masked_lm_weights, sentence_order_example_loss,
         sentence_order_log_probs, sentence_order_labels) = args[:7]


        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,
                                         [-1, masked_lm_log_probs.shape[-1]])
        masked_lm_predictions = tf.argmax(
            masked_lm_log_probs, axis=-1, output_type=tf.int32)
        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])
        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])
        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])
        masked_lm_accuracy = tf.metrics.accuracy(
            labels=masked_lm_ids,
            predictions=masked_lm_predictions,
            weights=masked_lm_weights)
        masked_lm_mean_loss = tf.metrics.mean(
            values=masked_lm_example_loss, weights=masked_lm_weights)

        metrics = {
            ""masked_lm_accuracy"": masked_lm_accuracy,
            ""masked_lm_loss"": masked_lm_mean_loss,
        }

        sentence_order_log_probs = tf.reshape(
            sentence_order_log_probs, [-1, sentence_order_log_probs.shape[-1]])
        sentence_order_predictions = tf.argmax(
            sentence_order_log_probs, axis=-1, output_type=tf.int32)
        sentence_order_labels = tf.reshape(sentence_order_labels, [-1])
        sentence_order_accuracy = tf.metrics.accuracy(
            labels=sentence_order_labels,
            predictions=sentence_order_predictions)
        sentence_order_mean_loss = tf.metrics.mean(
            values=sentence_order_example_loss)
        metrics.update({
            ""sentence_order_accuracy"": sentence_order_accuracy,
            ""sentence_order_loss"": sentence_order_mean_loss
        })
        return metrics

      metric_values = [
          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,
          masked_lm_weights, sentence_order_example_loss,
          sentence_order_log_probs, sentence_order_labels
      ]

      eval_metrics = (metric_fn, metric_values)

      output_spec = tf.contrib.tpu.TPUEstimatorSpec(
          mode=mode,
          loss=total_loss,
          eval_metrics=eval_metrics,
          scaffold_fn=scaffold_fn)
    else:
      raise ValueError(""Only TRAIN and EVAL modes are supported: %s"" % (mode))

    return output_spec

  return model_fn


def get_masked_lm_output(albert_config, input_tensor, output_weights, positions,
                         label_ids, label_weights):
  """"""Get loss and log probs for the masked LM.""""""
  input_tensor = gather_indexes(input_tensor, positions)


  with tf.variable_scope(""cls/predictions""):
    # We apply one more non-linear transformation before the output layer.
    # This matrix is not used after pre-training.
    with tf.variable_scope(""transform""):
      input_tensor = tf.layers.dense(
          input_tensor,
          units=albert_config.embedding_size,
          activation=modeling.get_activation(albert_config.hidden_act),
          kernel_initializer=modeling.create_initializer(
              albert_config.initializer_range))
      input_tensor = modeling.layer_norm(input_tensor)

    # The output weights are the same as the input embeddings, but there is
    # an output-only bias for each token.
    output_bias = tf.get_variable(
        ""output_bias"",
        shape=[albert_config.vocab_size],
        initializer=tf.zeros_initializer())
    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)
    logits = tf.nn.bias_add(logits, output_bias)
    log_probs = tf.nn.log_softmax(logits, axis=-1)

    label_ids = tf.reshape(label_ids, [-1])
    label_weights = tf.reshape(label_weights, [-1])

    one_hot_labels = tf.one_hot(
        label_ids, depth=albert_config.vocab_size, dtype=tf.float32)

    # The `positions` tensor might be zero-padded (if the sequence is too
    # short to have the maximum number of predictions). The `label_weights`
    # tensor has a value of 1.0 for every real prediction and 0.0 for the
    # padding predictions.
    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])
    numerator = tf.reduce_sum(label_weights * per_example_loss)
    denominator = tf.reduce_sum(label_weights) + 1e-5
    loss = numerator / denominator

  return (loss, per_example_loss, log_probs)


def get_sentence_order_output(albert_config, input_tensor, labels):
  """"""Get loss and log probs for the next sentence prediction.""""""

  # Simple binary classification. Note that 0 is ""next sentence"" and 1 is
  # ""random sentence"". This weight matrix is not used after pre-training.
  with tf.variable_scope(""cls/seq_relationship""):
    output_weights = tf.get_variable(
        ""output_weights"",
        shape=[2, albert_config.hidden_size],
        initializer=modeling.create_initializer(
            albert_config.initializer_range))
    output_bias = tf.get_variable(
        ""output_bias"", shape=[2], initializer=tf.zeros_initializer())

    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)
    logits = tf.nn.bias_add(logits, output_bias)
    log_probs = tf.nn.log_softmax(logits, axis=-1)
    labels = tf.reshape(labels, [-1])
    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)
    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)
    loss = tf.reduce_mean(per_example_loss)
    return (loss, per_example_loss, log_probs)


def gather_indexes(sequence_tensor, positions):
  """"""Gathers the vectors at the specific positions over a minibatch.""""""
  sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)
  batch_size = sequence_shape[0]
  seq_length = sequence_shape[1]
  width = sequence_shape[2]

  flat_offsets = tf.reshape(
      tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])
  flat_positions = tf.reshape(positions + flat_offsets, [-1])
  flat_sequence_tensor = tf.reshape(sequence_tensor,
                                    [batch_size * seq_length, width])
  output_tensor = tf.gather(flat_sequence_tensor, flat_positions)
  return output_tensor


def input_fn_builder(input_files,
                     max_seq_length,
                     max_predictions_per_seq,
                     is_training,
                     num_cpu_threads=4):
  """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""

  def input_fn(params):
    """"""The actual input function.""""""
    batch_size = params[""batch_size""]

    name_to_features = {
        ""input_ids"": tf.FixedLenFeature([max_seq_length], tf.int64),
        ""input_mask"": tf.FixedLenFeature([max_seq_length], tf.int64),
        ""segment_ids"": tf.FixedLenFeature([max_seq_length], tf.int64),
        # Note: We keep this feature name `next_sentence_labels` to be
        # compatible with the original data created by lanzhzh@. However, in
        # the ALBERT case it does represent sentence_order_labels.
        ""next_sentence_labels"": tf.FixedLenFeature([1], tf.int64),
    }

    if FLAGS.masked_lm_budget:
      name_to_features.update({
          ""token_boundary"":
              tf.FixedLenFeature([max_seq_length], tf.int64)})
    else:
      name_to_features.update({
          ""masked_lm_positions"":
              tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
          ""masked_lm_ids"":
              tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
          ""masked_lm_weights"":
              tf.FixedLenFeature([max_predictions_per_seq], tf.float32)})

    # For training, we want a lot of parallel reading and shuffling.
    # For eval, we want no shuffling and parallel reading doesn't matter.
    if is_training:
      d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))
      d = d.repeat()
      d = d.shuffle(buffer_size=len(input_files))

      # `cycle_length` is the number of parallel files that get read.
      cycle_length = min(num_cpu_threads, len(input_files))

      # `sloppy` mode means that the interleaving is not exact. This adds
      # even more randomness to the training pipeline.
      d = d.apply(
          tf.contrib.data.parallel_interleave(
              tf.data.TFRecordDataset,
              sloppy=is_training,
              cycle_length=cycle_length))
      d = d.shuffle(buffer_size=100)
    else:
      d = tf.data.TFRecordDataset(input_files)
      # Since we evaluate for a fixed number of steps we don't want to encounter
      # out-of-range exceptions.
      d = d.repeat()

    # We must `drop_remainder` on training because the TPU requires fixed
    # size dimensions. For eval, we assume we are evaluating on the CPU or GPU
    # and we *don't* want to drop the remainder, otherwise we wont cover
    # every sample.
    d = d.apply(
        tf.data.experimental.map_and_batch_with_legacy_function(
            lambda record: _decode_record(record, name_to_features),
            batch_size=batch_size,
            num_parallel_batches=num_cpu_threads,
            drop_remainder=True))
    tf.logging.info(d)
    return d

  return input_fn


def _decode_record(record, name_to_features):
  """"""Decodes a record to a TensorFlow example.""""""
  example = tf.parse_single_example(record, name_to_features)

  # tf.Example only supports tf.int64, but the TPU only supports tf.int32.
  # So cast all int64 to int32.
  for name in list(example.keys()):
    t = example[name]
    if t.dtype == tf.int64:
      t = tf.to_int32(t)
    example[name] = t

  return example


def main(_):
  tf.logging.set_verbosity(tf.logging.INFO)

  if not FLAGS.do_train and not FLAGS.do_eval:
    raise ValueError(""At least one of `do_train` or `do_eval` must be True."")

  albert_config = modeling.AlbertConfig.from_json_file(FLAGS.albert_config_file)

  tf.gfile.MakeDirs(FLAGS.output_dir)

  input_files = []
  for input_pattern in FLAGS.input_file.split("",""):
    input_files.extend(tf.gfile.Glob(input_pattern))

  tf.logging.info(""*** Input Files ***"")
  for input_file in input_files:
    tf.logging.info(""  %s"" % input_file)

  tpu_cluster_resolver = None
  if FLAGS.use_tpu and FLAGS.tpu_name:
    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)

  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2
  run_config = tf.contrib.tpu.RunConfig(
      cluster=tpu_cluster_resolver,
      master=FLAGS.master,
      model_dir=FLAGS.output_dir,
      save_checkpoints_steps=FLAGS.save_checkpoints_steps,
      tpu_config=tf.contrib.tpu.TPUConfig(
          iterations_per_loop=FLAGS.iterations_per_loop,
          num_shards=FLAGS.num_tpu_cores,
          per_host_input_for_training=is_per_host))

  model_fn = model_fn_builder(
      albert_config=albert_config,
      init_checkpoint=FLAGS.init_checkpoint,
      learning_rate=FLAGS.learning_rate,
      num_train_steps=FLAGS.num_train_steps,
      num_warmup_steps=FLAGS.num_warmup_steps,
      use_tpu=FLAGS.use_tpu,
      use_one_hot_embeddings=FLAGS.use_tpu,
      optimizer=FLAGS.optimizer,
      poly_power=FLAGS.poly_power,
      start_warmup_step=FLAGS.start_warmup_step)

  # If TPU is not available, this will fall back to normal Estimator on CPU
  # or GPU.
  estimator = tf.contrib.tpu.TPUEstimator(
      use_tpu=FLAGS.use_tpu,
      model_fn=model_fn,
      config=run_config,
      train_batch_size=FLAGS.train_batch_size,
      eval_batch_size=FLAGS.eval_batch_size)

  if FLAGS.do_train:
    tf.logging.info(""***** Running training *****"")
    tf.logging.info(""  Batch size = %d"", FLAGS.train_batch_size)
    train_input_fn = input_fn_builder(
        input_files=input_files,
        max_seq_length=FLAGS.max_seq_length,
        max_predictions_per_seq=FLAGS.max_predictions_per_seq,
        is_training=True)
    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)

  if FLAGS.do_eval:
    tf.logging.info(""***** Running evaluation *****"")
    tf.logging.info(""  Batch size = %d"", FLAGS.eval_batch_size)
    global_step = -1
    output_eval_file = os.path.join(FLAGS.output_dir, ""eval_results.txt"")
    writer = tf.gfile.GFile(output_eval_file, ""w"")
    tf.gfile.MakeDirs(FLAGS.export_dir)
    eval_input_fn = input_fn_builder(
        input_files=input_files,
        max_seq_length=FLAGS.max_seq_length,
        max_predictions_per_seq=FLAGS.max_predictions_per_seq,
        is_training=False)
    while global_step < FLAGS.num_train_steps:
      if estimator.latest_checkpoint() is None:
        tf.logging.info(""No checkpoint found yet. Sleeping."")
        time.sleep(1)
      else:
        result = estimator.evaluate(
            input_fn=eval_input_fn, steps=FLAGS.max_eval_steps)
        global_step = result[""global_step""]
        tf.logging.info(""***** Eval results *****"")
        for key in sorted(result.keys()):
          tf.logging.info(""  %s = %s"", key, str(result[key]))
          writer.write(""%s = %s\n"" % (key, str(result[key])))

if __name__ == ""__main__"":
  flags.mark_flag_as_required(""input_file"")
  flags.mark_flag_as_required(""albert_config_file"")
  flags.mark_flag_as_required(""output_dir"")
  tf.app.run()"
Albert,modeling_google_fast.py,"# coding=utf-8
# Copyright 2019 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Lint as: python2, python3
""""""The main ALBERT model and related functions.
For a description of the algorithm, see https://arxiv.org/abs/1909.11942.
""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import copy
import json
import math
import re
import numpy as np
import six
from six.moves import range
import tensorflow as tf


class AlbertConfig(object):
  """"""Configuration for `AlbertModel`.
  The default settings match the configuration of model `albert_xxlarge`.
  """"""

  def __init__(self,
               vocab_size,
               embedding_size=128,
               hidden_size=4096,
               num_hidden_layers=12,
               num_hidden_groups=1,
               num_attention_heads=64,
               intermediate_size=16384,
               inner_group_num=1,
               down_scale_factor=1,
               hidden_act=""gelu"",
               hidden_dropout_prob=0,
               attention_probs_dropout_prob=0,
               max_position_embeddings=512,
               type_vocab_size=2,
               initializer_range=0.02):
    """"""Constructs AlbertConfig.
    Args:
      vocab_size: Vocabulary size of `inputs_ids` in `AlbertModel`.
      embedding_size: size of voc embeddings.
      hidden_size: Size of the encoder layers and the pooler layer.
      num_hidden_layers: Number of hidden layers in the Transformer encoder.
      num_hidden_groups: Number of group for the hidden layers, parameters in
        the same group are shared.
      num_attention_heads: Number of attention heads for each attention layer in
        the Transformer encoder.
      intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)
        layer in the Transformer encoder.
      inner_group_num: int, number of inner repetition of attention and ffn.
      down_scale_factor: float, the scale to apply
      hidden_act: The non-linear activation function (function or string) in the
        encoder and pooler.
      hidden_dropout_prob: The dropout probability for all fully connected
        layers in the embeddings, encoder, and pooler.
      attention_probs_dropout_prob: The dropout ratio for the attention
        probabilities.
      max_position_embeddings: The maximum sequence length that this model might
        ever be used with. Typically set this to something large just in case
        (e.g., 512 or 1024 or 2048).
      type_vocab_size: The vocabulary size of the `token_type_ids` passed into
        `AlbertModel`.
      initializer_range: The stdev of the truncated_normal_initializer for
        initializing all weight matrices.
    """"""
    self.vocab_size = vocab_size
    self.embedding_size = embedding_size
    self.hidden_size = hidden_size
    self.num_hidden_layers = num_hidden_layers
    self.num_hidden_groups = num_hidden_groups
    self.num_attention_heads = num_attention_heads
    self.inner_group_num = inner_group_num
    self.down_scale_factor = down_scale_factor
    self.hidden_act = hidden_act
    self.intermediate_size = intermediate_size
    self.hidden_dropout_prob = hidden_dropout_prob
    self.attention_probs_dropout_prob = attention_probs_dropout_prob
    self.max_position_embeddings = max_position_embeddings
    self.type_vocab_size = type_vocab_size
    self.initializer_range = initializer_range

  @classmethod
  def from_dict(cls, json_object):
    """"""Constructs a `AlbertConfig` from a Python dictionary of parameters.""""""
    config = AlbertConfig(vocab_size=None)
    for (key, value) in six.iteritems(json_object):
      config.__dict__[key] = value
    return config

  @classmethod
  def from_json_file(cls, json_file):
    """"""Constructs a `AlbertConfig` from a json file of parameters.""""""
    with tf.gfile.GFile(json_file, ""r"") as reader:
      text = reader.read()
    return cls.from_dict(json.loads(text))

  def to_dict(self):
    """"""Serializes this instance to a Python dictionary.""""""
    output = copy.deepcopy(self.__dict__)
    return output

  def to_json_string(self):
    """"""Serializes this instance to a JSON string.""""""
    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\n""


class AlbertModel(object):
  """"""BERT model (""Bidirectional Encoder Representations from Transformers"").
  Example usage:
  ```python
  # Already been converted from strings into ids
  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])
  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])
  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])
  config = modeling.AlbertConfig(vocab_size=32000, hidden_size=512,
    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)
  model = modeling.AlbertModel(config=config, is_training=True,
    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)
  label_embeddings = tf.get_variable(...)
  pooled_output = model.get_pooled_output()
  logits = tf.matmul(pooled_output, label_embeddings)
  ...
  ```
  """"""

  def __init__(self,
               config,
               is_training,
               input_ids,
               input_mask=None,
               token_type_ids=None,
               use_one_hot_embeddings=False,
               scope=None):
    """"""Constructor for AlbertModel.
    Args:
      config: `AlbertConfig` instance.
      is_training: bool. true for training model, false for eval model. Controls
        whether dropout will be applied.
      input_ids: int32 Tensor of shape [batch_size, seq_length].
      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].
      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].
      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word
        embeddings or tf.embedding_lookup() for the word embeddings.
      scope: (optional) variable scope. Defaults to ""bert"".
    Raises:
      ValueError: The config is invalid or one of the input tensor shapes
        is invalid.
    """"""
    config = copy.deepcopy(config)
    if not is_training:
      config.hidden_dropout_prob = 0.0
      config.attention_probs_dropout_prob = 0.0

    input_shape = get_shape_list(input_ids, expected_rank=2)
    batch_size = input_shape[0]
    seq_length = input_shape[1]

    if input_mask is None:
      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)

    if token_type_ids is None:
      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)

    with tf.variable_scope(scope, default_name=""bert""):
      with tf.variable_scope(""embeddings""):
        # Perform embedding lookup on the word ids.
        (self.word_embedding_output,
         self.output_embedding_table) = embedding_lookup(
            input_ids=input_ids,
            vocab_size=config.vocab_size,
            embedding_size=config.embedding_size,
            initializer_range=config.initializer_range,
            word_embedding_name=""word_embeddings"",
            use_one_hot_embeddings=use_one_hot_embeddings)

        # Add positional embeddings and token type embeddings, then layer
        # normalize and perform dropout.
        self.embedding_output = embedding_postprocessor(
            input_tensor=self.word_embedding_output,
            use_token_type=True,
            token_type_ids=token_type_ids,
            token_type_vocab_size=config.type_vocab_size,
            token_type_embedding_name=""token_type_embeddings"",
            use_position_embeddings=True,
            position_embedding_name=""position_embeddings"",
            initializer_range=config.initializer_range,
            max_position_embeddings=config.max_position_embeddings,
            dropout_prob=config.hidden_dropout_prob)

      with tf.variable_scope(""encoder""):

        # Run the stacked transformer.
        # `sequence_output` shape = [batch_size, seq_length, hidden_size].
        self.all_encoder_layers = transformer_model(
            input_tensor=self.embedding_output,
            attention_mask=input_mask,
            hidden_size=config.hidden_size,
            num_hidden_layers=config.num_hidden_layers,
            num_hidden_groups=config.num_hidden_groups,
            num_attention_heads=config.num_attention_heads,
            intermediate_size=config.intermediate_size,
            inner_group_num=config.inner_group_num,
            intermediate_act_fn=get_activation(config.hidden_act),
            hidden_dropout_prob=config.hidden_dropout_prob,
            attention_probs_dropout_prob=config.attention_probs_dropout_prob,
            initializer_range=config.initializer_range,
            do_return_all_layers=True)

      self.sequence_output = self.all_encoder_layers[-1]
      # The ""pooler"" converts the encoded sequence tensor of shape
      # [batch_size, seq_length, hidden_size] to a tensor of shape
      # [batch_size, hidden_size]. This is necessary for segment-level
      # (or segment-pair-level) classification tasks where we need a fixed
      # dimensional representation of the segment.
      with tf.variable_scope(""pooler""):
        # We ""pool"" the model by simply taking the hidden state corresponding
        # to the first token. We assume that this has been pre-trained
        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)
        self.pooled_output = tf.layers.dense(
            first_token_tensor,
            config.hidden_size,
            activation=tf.tanh,
            kernel_initializer=create_initializer(config.initializer_range))

  def get_pooled_output(self):
    return self.pooled_output

  def get_sequence_output(self):
    """"""Gets final hidden layer of encoder.
    Returns:
      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
      to the final hidden of the transformer encoder.
    """"""
    return self.sequence_output

  def get_all_encoder_layers(self):
    return self.all_encoder_layers

  def get_word_embedding_output(self):
    """"""Get output of the word(piece) embedding lookup.
    This is BEFORE positional embeddings and token type embeddings have been
    added.
    Returns:
      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
      to the output of the word(piece) embedding layer.
    """"""
    return self.word_embedding_output

  def get_embedding_output(self):
    """"""Gets output of the embedding lookup (i.e., input to the transformer).
    Returns:
      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
      to the output of the embedding layer, after summing the word
      embeddings with the positional embeddings and the token type embeddings,
      then performing layer normalization. This is the input to the transformer.
    """"""
    return self.embedding_output

  def get_embedding_table(self):
    return self.output_embedding_table


def gelu(x):
  """"""Gaussian Error Linear Unit.
  This is a smoother version of the RELU.
  Original paper: https://arxiv.org/abs/1606.08415
  Args:
    x: float Tensor to perform activation.
  Returns:
    `x` with the GELU activation applied.
  """"""
  cdf = 0.5 * (1.0 + tf.tanh(
      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))
  return x * cdf


def get_activation(activation_string):
  """"""Maps a string to a Python function, e.g., ""relu"" => `tf.nn.relu`.
  Args:
    activation_string: String name of the activation function.
  Returns:
    A Python function corresponding to the activation function. If
    `activation_string` is None, empty, or ""linear"", this will return None.
    If `activation_string` is not a string, it will return `activation_string`.
  Raises:
    ValueError: The `activation_string` does not correspond to a known
      activation.
  """"""

  # We assume that anything that""s not a string is already an activation
  # function, so we just return it.
  if not isinstance(activation_string, six.string_types):
    return activation_string

  if not activation_string:
    return None

  act = activation_string.lower()
  if act == ""linear"":
    return None
  elif act == ""relu"":
    return tf.nn.relu
  elif act == ""gelu"":
    return gelu
  elif act == ""tanh"":
    return tf.tanh
  elif act == ""swish"":
    return lambda x: x * tf.sigmoid(x)
  else:
    raise ValueError(""Unsupported activation: %s"" % act)


def get_assignment_map_from_checkpoint(tvars, init_checkpoint, num_of_group=0):
  """"""Compute the union of the current variables and checkpoint variables.""""""
  assignment_map = {}
  initialized_variable_names = {}

  name_to_variable = collections.OrderedDict()
  for var in tvars:
    name = var.name
    m = re.match(""^(.*):\\d+$"", name)
    if m is not None:
      name = m.group(1)
    name_to_variable[name] = var
  init_vars = tf.train.list_variables(init_checkpoint)
  init_vars_name = [name for (name, _) in init_vars]

  if num_of_group > 0:
    assignment_map = []
    for gid in range(num_of_group):
      assignment_map.append(collections.OrderedDict())
  else:
    assignment_map = collections.OrderedDict()

  for name in name_to_variable:
    if name in init_vars_name:
      tvar_name = name
    elif (re.sub(r""/group_\d+/"", ""/group_0/"",
                 six.ensure_str(name)) in init_vars_name and
          num_of_group > 1):
      tvar_name = re.sub(r""/group_\d+/"", ""/group_0/"", six.ensure_str(name))
    elif (re.sub(r""/ffn_\d+/"", ""/ffn_1/"", six.ensure_str(name))
          in init_vars_name and num_of_group > 1):
      tvar_name = re.sub(r""/ffn_\d+/"", ""/ffn_1/"", six.ensure_str(name))
    elif (re.sub(r""/attention_\d+/"", ""/attention_1/"", six.ensure_str(name))
          in init_vars_name and num_of_group > 1):
      tvar_name = re.sub(r""/attention_\d+/"", ""/attention_1/"",
                         six.ensure_str(name))
    else:
      tf.logging.info(""name %s does not get matched"", name)
      continue
    tf.logging.info(""name %s match to %s"", name, tvar_name)
    if num_of_group > 0:
      group_matched = False
      for gid in range(1, num_of_group):
        if ((""/group_"" + str(gid) + ""/"" in name) or
            (""/ffn_"" + str(gid) + ""/"" in name) or
            (""/attention_"" + str(gid) + ""/"" in name)):
          group_matched = True
          tf.logging.info(""%s belongs to %dth"", name, gid)
          assignment_map[gid][tvar_name] = name
      if not group_matched:
        assignment_map[0][tvar_name] = name
    else:
      assignment_map[tvar_name] = name
    initialized_variable_names[name] = 1
    initialized_variable_names[six.ensure_str(name) + "":0""] = 1

  return (assignment_map, initialized_variable_names)


def dropout(input_tensor, dropout_prob):
  """"""Perform dropout.
  Args:
    input_tensor: float Tensor.
    dropout_prob: Python float. The probability of dropping out a value (NOT of
      *keeping* a dimension as in `tf.nn.dropout`).
  Returns:
    A version of `input_tensor` with dropout applied.
  """"""
  if dropout_prob is None or dropout_prob == 0.0:
    return input_tensor

  output = tf.nn.dropout(input_tensor, rate=dropout_prob)
  return output


def layer_norm(input_tensor, name=None):
  """"""Run layer normalization on the last dimension of the tensor.""""""
  return tf.contrib.layers.layer_norm(
      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)


def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):
  """"""Runs layer normalization followed by dropout.""""""
  output_tensor = layer_norm(input_tensor, name)
  output_tensor = dropout(output_tensor, dropout_prob)
  return output_tensor


def create_initializer(initializer_range=0.02):
  """"""Creates a `truncated_normal_initializer` with the given range.""""""
  return tf.truncated_normal_initializer(stddev=initializer_range)


def get_timing_signal_1d_given_position(channels,
                                        position,
                                        min_timescale=1.0,
                                        max_timescale=1.0e4):
  """"""Get sinusoids of diff frequencies, with timing position given.
  Adapted from add_timing_signal_1d_given_position in
  //third_party/py/tensor2tensor/layers/common_attention.py
  Args:
    channels: scalar, size of timing embeddings to create. The number of
        different timescales is equal to channels / 2.
    position: a Tensor with shape [batch, seq_len]
    min_timescale: a float
    max_timescale: a float
  Returns:
    a Tensor of timing signals [batch, seq_len, channels]
  """"""
  num_timescales = channels // 2
  log_timescale_increment = (
      math.log(float(max_timescale) / float(min_timescale)) /
      (tf.to_float(num_timescales) - 1))
  inv_timescales = min_timescale * tf.exp(
      tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)
  scaled_time = (
      tf.expand_dims(tf.to_float(position), 2) * tf.expand_dims(
          tf.expand_dims(inv_timescales, 0), 0))
  signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=2)
  signal = tf.pad(signal, [[0, 0], [0, 0], [0, tf.mod(channels, 2)]])
  return signal


def embedding_lookup(input_ids,
                     vocab_size,
                     embedding_size=128,
                     initializer_range=0.02,
                     word_embedding_name=""word_embeddings"",
                     use_one_hot_embeddings=False):
  """"""Looks up words embeddings for id tensor.
  Args:
    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word
      ids.
    vocab_size: int. Size of the embedding vocabulary.
    embedding_size: int. Width of the word embeddings.
    initializer_range: float. Embedding initialization range.
    word_embedding_name: string. Name of the embedding table.
    use_one_hot_embeddings: bool. If True, use one-hot method for word
      embeddings. If False, use `tf.nn.embedding_lookup()`.
  Returns:
    float Tensor of shape [batch_size, seq_length, embedding_size].
  """"""
  # This function assumes that the input is of shape [batch_size, seq_length,
  # num_inputs].
  #
  # If the input is a 2D tensor of shape [batch_size, seq_length], we
  # reshape to [batch_size, seq_length, 1].
  if input_ids.shape.ndims == 2:
    input_ids = tf.expand_dims(input_ids, axis=[-1])

  embedding_table = tf.get_variable(
      name=word_embedding_name,
      shape=[vocab_size, embedding_size],
      initializer=create_initializer(initializer_range))

  if use_one_hot_embeddings:
    flat_input_ids = tf.reshape(input_ids, [-1])
    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)
    output = tf.matmul(one_hot_input_ids, embedding_table)
  else:
    output = tf.nn.embedding_lookup(embedding_table, input_ids)

  input_shape = get_shape_list(input_ids)

  output = tf.reshape(output,
                      input_shape[0:-1] + [input_shape[-1] * embedding_size])
  return (output, embedding_table)


def embedding_postprocessor(input_tensor,
                            use_token_type=False,
                            token_type_ids=None,
                            token_type_vocab_size=16,
                            token_type_embedding_name=""token_type_embeddings"",
                            use_position_embeddings=True,
                            position_embedding_name=""position_embeddings"",
                            initializer_range=0.02,
                            max_position_embeddings=512,
                            dropout_prob=0.1):
  """"""Performs various post-processing on a word embedding tensor.
  Args:
    input_tensor: float Tensor of shape [batch_size, seq_length,
      embedding_size].
    use_token_type: bool. Whether to add embeddings for `token_type_ids`.
    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].
      Must be specified if `use_token_type` is True.
    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.
    token_type_embedding_name: string. The name of the embedding table variable
      for token type ids.
    use_position_embeddings: bool. Whether to add position embeddings for the
      position of each token in the sequence.
    position_embedding_name: string. The name of the embedding table variable
      for positional embeddings.
    initializer_range: float. Range of the weight initialization.
    max_position_embeddings: int. Maximum sequence length that might ever be
      used with this model. This can be longer than the sequence length of
      input_tensor, but cannot be shorter.
    dropout_prob: float. Dropout probability applied to the final output tensor.
  Returns:
    float tensor with same shape as `input_tensor`.
  Raises:
    ValueError: One of the tensor shapes or input values is invalid.
  """"""
  input_shape = get_shape_list(input_tensor, expected_rank=3)
  batch_size = input_shape[0]
  seq_length = input_shape[1]
  width = input_shape[2]

  output = input_tensor

  if use_token_type:
    if token_type_ids is None:
      raise ValueError(""`token_type_ids` must be specified if""
                       ""`use_token_type` is True."")
    token_type_table = tf.get_variable(
        name=token_type_embedding_name,
        shape=[token_type_vocab_size, width],
        initializer=create_initializer(initializer_range))
    # This vocab will be small so we always do one-hot here, since it is always
    # faster for a small vocabulary.
    flat_token_type_ids = tf.reshape(token_type_ids, [-1])
    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)
    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)
    token_type_embeddings = tf.reshape(token_type_embeddings,
                                       [batch_size, seq_length, width])
    output += token_type_embeddings

  if use_position_embeddings:
    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)
    with tf.control_dependencies([assert_op]):
      full_position_embeddings = tf.get_variable(
          name=position_embedding_name,
          shape=[max_position_embeddings, width],
          initializer=create_initializer(initializer_range))
      # Since the position embedding table is a learned variable, we create it
      # using a (long) sequence length `max_position_embeddings`. The actual
      # sequence length might be shorter than this, for faster training of
      # tasks that do not have long sequences.
      #
      # So `full_position_embeddings` is effectively an embedding table
      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current
      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just
      # perform a slice.
      position_embeddings = tf.slice(full_position_embeddings, [0, 0],
                                     [seq_length, -1])
      num_dims = len(output.shape.as_list())

      # Only the last two dimensions are relevant (`seq_length` and `width`), so
      # we broadcast among the first dimensions, which is typically just
      # the batch size.
      position_broadcast_shape = []
      for _ in range(num_dims - 2):
        position_broadcast_shape.append(1)
      position_broadcast_shape.extend([seq_length, width])
      position_embeddings = tf.reshape(position_embeddings,
                                       position_broadcast_shape)
      output += position_embeddings

  output = layer_norm_and_dropout(output, dropout_prob)
  return output


def dense_layer_3d(input_tensor,
                   num_attention_heads,
                   head_size,
                   initializer,
                   activation,
                   name=None):
  """"""A dense layer with 3D kernel.
  Args:
    input_tensor: float Tensor of shape [batch, seq_length, hidden_size].
    num_attention_heads: Number of attention heads.
    head_size: The size per attention head.
    initializer: Kernel initializer.
    activation: Actication function.
    name: The name scope of this layer.
  Returns:
    float logits Tensor.
  """"""

  input_shape = get_shape_list(input_tensor)
  hidden_size = input_shape[2]

  with tf.variable_scope(name):
    w = tf.get_variable(
        name=""kernel"",
        shape=[hidden_size, num_attention_heads * head_size],
        initializer=initializer)
    w = tf.reshape(w, [hidden_size, num_attention_heads, head_size])
    b = tf.get_variable(
        name=""bias"",
        shape=[num_attention_heads * head_size],
        initializer=tf.zeros_initializer)
    b = tf.reshape(b, [num_attention_heads, head_size])
    ret = tf.einsum(""BFH,HND->BFND"", input_tensor, w)
    ret += b
  if activation is not None:
    return activation(ret)
  else:
    return ret


def dense_layer_3d_proj(input_tensor,
                        hidden_size,
                        head_size,
                        initializer,
                        activation,
                        name=None):
  """"""A dense layer with 3D kernel for projection.
  Args:
    input_tensor: float Tensor of shape [batch,from_seq_length,
      num_attention_heads, size_per_head].
    hidden_size: The size of hidden layer.
    num_attention_heads: The size of output dimension.
    head_size: The size of head.
    initializer: Kernel initializer.
    activation: Actication function.
    name: The name scope of this layer.
  Returns:
    float logits Tensor.
  """"""
  input_shape = get_shape_list(input_tensor)
  num_attention_heads= input_shape[2]
  with tf.variable_scope(name):
    w = tf.get_variable(
        name=""kernel"",
        shape=[num_attention_heads * head_size, hidden_size],
        initializer=initializer)
    w = tf.reshape(w, [num_attention_heads, head_size, hidden_size])
    b = tf.get_variable(
        name=""bias"", shape=[hidden_size], initializer=tf.zeros_initializer)
    ret = tf.einsum(""BFND,NDH->BFH"", input_tensor, w)
    ret += b
  if activation is not None:
    return activation(ret)
  else:
    return ret

def dense_layer_2d(input_tensor,
                   output_size,
                   initializer,
                   activation,
                   num_attention_heads=1,
                   name=None,
                   num_groups=1):
  """"""A dense layer with 2D kernel.
  Args:
    input_tensor: Float tensor with rank 3.
    output_size: The size of output dimension.
    initializer: Kernel initializer.
    activation: Activation function.
    num_groups: number of groups in dense layer
    num_attention_heads: number of attention head in attention layer.
    name: The name scope of this layer.
  Returns:
    float logits Tensor.
  """"""
  del num_attention_heads  # unused
  input_shape = get_shape_list(input_tensor)
  hidden_size = input_shape[2]
  if num_groups == 1:
    with tf.variable_scope(name):
      w = tf.get_variable(
          name=""kernel"",
          shape=[hidden_size, output_size],
          initializer=initializer)
      b = tf.get_variable(
          name=""bias"", shape=[output_size], initializer=tf.zeros_initializer)
      ret = tf.einsum(""BFH,HO->BFO"", input_tensor, w)
      ret += b
  else:
    assert hidden_size % num_groups == 0
    assert output_size % num_groups == 0
    with tf.variable_scope(name):
      w = tf.get_variable(
          name=""kernel"",
          shape=[hidden_size//num_groups, output_size//num_groups, num_groups],
          initializer=initializer)
      b = tf.get_variable(
          name=""bias"", shape=[output_size], initializer=tf.zeros_initializer)
      input_tensor = tf.reshape(input_tensor, input_shape[:2] + [hidden_size//num_groups, num_groups])
      ret = tf.einsum(""BFHG,HOG->BFGO"", input_tensor, w)
      ret = tf.reshape(ret, input_shape[:2] + [output_size])
      ret += b
  if activation is not None:
    return activation(ret)
  else:
    return ret

def dense_layer_2d_old(input_tensor,
                   output_size,
                   initializer,
                   activation,
                   num_attention_heads=1,
                   name=None,
                   num_groups=1):
  """"""A dense layer with 2D kernel. 添加分组全连接的方式
  Args:
    input_tensor: Float tensor with rank 3. [ batch_size,sequence_length, hidden_size]
    output_size: The size of output dimension.
    initializer: Kernel initializer.
    activation: Activation function.
    num_groups: number of groups in dense layer
    num_attention_heads: number of attention head in attention layer.
    name: The name scope of this layer.
  Returns:
    float logits Tensor.
  """"""
  del num_attention_heads  # unused
  input_shape = get_shape_list(input_tensor)
  # print(""#dense_layer_2d.1.input_shape of input_tensor:"",input_shape)  # e.g. [2, 512, 768] = [ batch_size,sequence_length, hidden_size]
  hidden_size = input_shape[2]
  if num_groups == 1:
    with tf.variable_scope(name):
      w = tf.get_variable(
          name=""kernel"",
          shape=[hidden_size, output_size],
          initializer=initializer)
      b = tf.get_variable(
          name=""bias"", shape=[output_size], initializer=tf.zeros_initializer)
      ret = tf.einsum(""BFH,HO->BFO"", input_tensor, w)
      ret += b
  else: # e.g. input_shape = [2, 512, 768] = [ batch_size,sequence_length, hidden_size]
    assert hidden_size % num_groups == 0
    assert output_size % num_groups == 0
    # print(""#dense_layer_2d.output_size:"",output_size,"";hidden_size:"",hidden_size) # output_size = 3072; hidden_size = 768
    with tf.variable_scope(name):
      w = tf.get_variable(
          name=""kernel"",
          shape=[num_groups, hidden_size//num_groups, output_size//num_groups],
          initializer=initializer)
      # print(""#dense_layer_2d.2'w:"",w.shape) # (16, 48, 192)
      b = tf.get_variable(
          name=""bias"", shape=[num_groups, output_size//num_groups], initializer=tf.zeros_initializer)
      # input_tensor = [ batch_size,sequence_length, hidden_size].
      # input_shape[:2] + [hidden_size//num_groups, num_groups] = [batch_size, sequence_length, hidden_size/num_groups, num_groups]
      input_tensor = tf.reshape(input_tensor, input_shape[:2] + [hidden_size//num_groups, num_groups])
      # print(""#dense_layer_2d.2.input_shape of input_tensor:"", input_tensor.shape)
      input_tensor = tf.transpose(input_tensor, [3, 0, 1, 2]) # [num_groups, batch_size, sequence_length, hidden_size/num_groups]
      # print(""#dense_layer_2d.3.input_shape of input_tensor:"", input_tensor.shape) #  input_tensor=(16, 2, 512, 192)
      # input_tensor=[num_groups, batch_size, sequence_length, hidden_size/num_groups], w=[num_groups, hidden_size/num_groups, output_size/num_groups]

      ret = tf.einsum(""GBFH,GHO->GBFO"", input_tensor, w)
      # print(""#dense_layer_2d.4. shape of ret:"", ret.shape) #  (16, 2, 512, 48) = [num_groups, batch_size, sequence_length ,output_size]
      b = tf.expand_dims(b, 1)
      b = tf.expand_dims(b, 1)
      # print(""#dense_layer_2d.4.2.b:"",b.shape) #  (16, 1, 1, 48)
      ret += b
      ret = tf.transpose(ret, [1, 2, 0, 3]) #  (2, 512, 16, 48)
      # print(""#dense_layer_2d.5. shape of ret:"", ret.shape)
      ret = tf.reshape(ret, input_shape[:2] + [output_size]) # [2, 512, 768]
  if activation is not None:
    return activation(ret)
  else:
    return ret


def dot_product_attention(q, k, v, bias, dropout_rate=0.0):
  """"""Dot-product attention.
  Args:
    q: Tensor with shape [..., length_q, depth_k].
    k: Tensor with shape [..., length_kv, depth_k]. Leading dimensions must
      match with q.
    v: Tensor with shape [..., length_kv, depth_v] Leading dimensions must
      match with q.
    bias: bias Tensor (see attention_bias())
    dropout_rate: a float.
  Returns:
    Tensor with shape [..., length_q, depth_v].
  """"""
  logits = tf.matmul(q, k, transpose_b=True)  # [..., length_q, length_kv]
  logits = tf.multiply(logits, 1.0 / math.sqrt(float(get_shape_list(q)[-1])))
  if bias is not None:
    # `attention_mask` = [B, T]
    from_shape = get_shape_list(q)
    if len(from_shape) == 4:
      broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], 1], tf.float32)
    elif len(from_shape) == 5:
      # from_shape = [B, N, Block_num, block_size, depth]#
      broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], from_shape[3],
                                1], tf.float32)

    bias = tf.matmul(broadcast_ones,
                     tf.cast(bias, tf.float32), transpose_b=True)

    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
    # masked positions, this operation will create a tensor which is 0.0 for
    # positions we want to attend and -10000.0 for masked positions.
    adder = (1.0 - bias) * -10000.0

    # Since we are adding it to the raw scores before the softmax, this is
    # effectively the same as removing these entirely.
    logits += adder
  else:
    adder = 0.0

  attention_probs = tf.nn.softmax(logits, name=""attention_probs"")
  attention_probs = dropout(attention_probs, dropout_rate)
  return tf.matmul(attention_probs, v)


def attention_layer(from_tensor,
                    to_tensor,
                    attention_mask=None,
                    num_attention_heads=1,
                    query_act=None,
                    key_act=None,
                    value_act=None,
                    attention_probs_dropout_prob=0.0,
                    initializer_range=0.02,
                    batch_size=None,
                    from_seq_length=None,
                    to_seq_length=None):
  """"""Performs multi-headed attention from `from_tensor` to `to_tensor`.
  Args:
    from_tensor: float Tensor of shape [batch_size, from_seq_length,
      from_width].
    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].
    attention_mask: (optional) int32 Tensor of shape [batch_size,
      from_seq_length, to_seq_length]. The values should be 1 or 0. The
      attention scores will effectively be set to -infinity for any positions in
      the mask that are 0, and will be unchanged for positions that are 1.
    num_attention_heads: int. Number of attention heads.
    query_act: (optional) Activation function for the query transform.
    key_act: (optional) Activation function for the key transform.
    value_act: (optional) Activation function for the value transform.
    attention_probs_dropout_prob: (optional) float. Dropout probability of the
      attention probabilities.
    initializer_range: float. Range of the weight initializer.
    batch_size: (Optional) int. If the input is 2D, this might be the batch size
      of the 3D version of the `from_tensor` and `to_tensor`.
    from_seq_length: (Optional) If the input is 2D, this might be the seq length
      of the 3D version of the `from_tensor`.
    to_seq_length: (Optional) If the input is 2D, this might be the seq length
      of the 3D version of the `to_tensor`.
  Returns:
    float Tensor of shape [batch_size, from_seq_length, num_attention_heads,
      size_per_head].
  Raises:
    ValueError: Any of the arguments or tensor shapes are invalid.
  """"""
  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])
  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])
  size_per_head = int(from_shape[2]/num_attention_heads)

  if len(from_shape) != len(to_shape):
    raise ValueError(
        ""The rank of `from_tensor` must match the rank of `to_tensor`."")

  if len(from_shape) == 3:
    batch_size = from_shape[0]
    from_seq_length = from_shape[1]
    to_seq_length = to_shape[1]
  elif len(from_shape) == 2:
    if (batch_size is None or from_seq_length is None or to_seq_length is None):
      raise ValueError(
          ""When passing in rank 2 tensors to attention_layer, the values ""
          ""for `batch_size`, `from_seq_length`, and `to_seq_length` ""
          ""must all be specified."")

  # Scalar dimensions referenced here:
  #   B = batch size (number of sequences)
  #   F = `from_tensor` sequence length
  #   T = `to_tensor` sequence length
  #   N = `num_attention_heads`
  #   H = `size_per_head`

  # `query_layer` = [B, F, N, H]
  q = dense_layer_3d(from_tensor, num_attention_heads, size_per_head,
                     create_initializer(initializer_range), query_act, ""query"")

  # `key_layer` = [B, T, N, H]
  k = dense_layer_3d(to_tensor, num_attention_heads, size_per_head,
                     create_initializer(initializer_range), key_act, ""key"")
  # `value_layer` = [B, T, N, H]
  v = dense_layer_3d(to_tensor, num_attention_heads, size_per_head,
                     create_initializer(initializer_range), value_act, ""value"")
  q = tf.transpose(q, [0, 2, 1, 3])
  k = tf.transpose(k, [0, 2, 1, 3])
  v = tf.transpose(v, [0, 2, 1, 3])
  if attention_mask is not None:
    attention_mask = tf.reshape(
        attention_mask, [batch_size, 1, to_seq_length, 1])
    # 'new_embeddings = [B, N, F, H]'
  new_embeddings = dot_product_attention(q, k, v, attention_mask,
                                         attention_probs_dropout_prob)

  return tf.transpose(new_embeddings, [0, 2, 1, 3])


def attention_ffn_block(layer_input,
                        hidden_size=768,
                        attention_mask=None,
                        num_attention_heads=1,
                        attention_head_size=64,
                        attention_probs_dropout_prob=0.0,
                        intermediate_size=3072,
                        intermediate_act_fn=None,
                        initializer_range=0.02,
                        hidden_dropout_prob=0.0):
  """"""A network with attention-ffn as sub-block.
  Args:
    layer_input: float Tensor of shape [batch_size, from_seq_length,
      from_width].
    hidden_size: (optional) int, size of hidden layer.
    attention_mask: (optional) int32 Tensor of shape [batch_size,
      from_seq_length, to_seq_length]. The values should be 1 or 0. The
      attention scores will effectively be set to -infinity for any positions in
      the mask that are 0, and will be unchanged for positions that are 1.
    num_attention_heads: int. Number of attention heads.
    attention_head_size: int. Size of attention head.
    attention_probs_dropout_prob: float. dropout probability for attention_layer
    intermediate_size: int. Size of intermediate hidden layer.
    intermediate_act_fn: (optional) Activation function for the intermediate
      layer.
    initializer_range: float. Range of the weight initializer.
    hidden_dropout_prob: (optional) float. Dropout probability of the hidden
      layer.
  Returns:
    layer output
  """"""

  with tf.variable_scope(""attention_1""):
    with tf.variable_scope(""self""):
      attention_output = attention_layer(
          from_tensor=layer_input,
          to_tensor=layer_input,
          attention_mask=attention_mask,
          num_attention_heads=num_attention_heads,
          attention_probs_dropout_prob=attention_probs_dropout_prob,
          initializer_range=initializer_range)

    # Run a linear projection of `hidden_size` then add a residual
    # with `layer_input`.
    with tf.variable_scope(""output""):
      attention_output = dense_layer_3d_proj(
          attention_output,
          hidden_size,
          attention_head_size,
          create_initializer(initializer_range),
          None,
          name=""dense"")
      attention_output = dropout(attention_output, hidden_dropout_prob)
  attention_output = layer_norm(attention_output + layer_input)
  with tf.variable_scope(""ffn_1""):
    with tf.variable_scope(""intermediate""):
      intermediate_output = dense_layer_2d(
          attention_output,
          intermediate_size,
          create_initializer(initializer_range),
          intermediate_act_fn,
          num_attention_heads=num_attention_heads,
          name=""dense"",
          num_groups=16)
      with tf.variable_scope(""output""):
        ffn_output = dense_layer_2d(
            intermediate_output,
            hidden_size,
            create_initializer(initializer_range),
            None,
            num_attention_heads=num_attention_heads,
            name=""dense"",
            num_groups=16)
      ffn_output = dropout(ffn_output, hidden_dropout_prob)
  ffn_output = layer_norm(ffn_output + attention_output)
  return ffn_output


def transformer_model(input_tensor,
                      attention_mask=None,
                      hidden_size=768,
                      num_hidden_layers=12,
                      num_hidden_groups=12,
                      num_attention_heads=12,
                      intermediate_size=3072,
                      inner_group_num=1,
                      intermediate_act_fn=""gelu"",
                      hidden_dropout_prob=0.1,
                      attention_probs_dropout_prob=0.1,
                      initializer_range=0.02,
                      do_return_all_layers=False):
  """"""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".
  This is almost an exact implementation of the original Transformer encoder.
  See the original paper:
  https://arxiv.org/abs/1706.03762
  Also see:
  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py
  Args:
    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].
    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,
      seq_length], with 1 for positions that can be attended to and 0 in
      positions that should not be.
    hidden_size: int. Hidden size of the Transformer.
    num_hidden_layers: int. Number of layers (blocks) in the Transformer.
    num_hidden_groups: int. Number of group for the hidden layers, parameters
      in the same group are shared.
    num_attention_heads: int. Number of attention heads in the Transformer.
    intermediate_size: int. The size of the ""intermediate"" (a.k.a., feed
      forward) layer.
    inner_group_num: int, number of inner repetition of attention and ffn.
    intermediate_act_fn: function. The non-linear activation function to apply
      to the output of the intermediate/feed-forward layer.
    hidden_dropout_prob: float. Dropout probability for the hidden layers.
    attention_probs_dropout_prob: float. Dropout probability of the attention
      probabilities.
    initializer_range: float. Range of the initializer (stddev of truncated
      normal).
    do_return_all_layers: Whether to also return all layers or just the final
      layer.
  Returns:
    float Tensor of shape [batch_size, seq_length, hidden_size], the final
    hidden layer of the Transformer.
  Raises:
    ValueError: A Tensor shape or parameter is invalid.
  """"""
  if hidden_size % num_attention_heads != 0:
    raise ValueError(
        ""The hidden size (%d) is not a multiple of the number of attention ""
        ""heads (%d)"" % (hidden_size, num_attention_heads))

  attention_head_size = hidden_size // num_attention_heads
  input_shape = get_shape_list(input_tensor, expected_rank=3)
  input_width = input_shape[2]

  all_layer_outputs = []
  if input_width != hidden_size:
    prev_output = dense_layer_2d(
        input_tensor, hidden_size, create_initializer(initializer_range),
        None, name=""embedding_hidden_mapping_in"")
  else:
    prev_output = input_tensor
  with tf.variable_scope(""transformer"", reuse=tf.AUTO_REUSE):
    for layer_idx in range(num_hidden_layers):
      group_idx = int(layer_idx / num_hidden_layers * num_hidden_groups)
      with tf.variable_scope(""group_%d"" % group_idx):
        with tf.name_scope(""layer_%d"" % layer_idx):
          layer_output = prev_output
          for inner_group_idx in range(inner_group_num):
            with tf.variable_scope(""inner_group_%d"" % inner_group_idx):
              layer_output = attention_ffn_block(
                  layer_output, hidden_size, attention_mask,
                  num_attention_heads, attention_head_size,
                  attention_probs_dropout_prob, intermediate_size,
                  intermediate_act_fn, initializer_range, hidden_dropout_prob)
              prev_output = layer_output
              all_layer_outputs.append(layer_output)
  if do_return_all_layers:
    return all_layer_outputs
  else:
    return all_layer_outputs[-1]


def get_shape_list(tensor, expected_rank=None, name=None):
  """"""Returns a list of the shape of tensor, preferring static dimensions.
  Args:
    tensor: A tf.Tensor object to find the shape of.
    expected_rank: (optional) int. The expected rank of `tensor`. If this is
      specified and the `tensor` has a different rank, and exception will be
      thrown.
    name: Optional name of the tensor for the error message.
  Returns:
    A list of dimensions of the shape of tensor. All static dimensions will
    be returned as python integers, and dynamic dimensions will be returned
    as tf.Tensor scalars.
  """"""
  if name is None:
    name = tensor.name

  if expected_rank is not None:
    assert_rank(tensor, expected_rank, name)

  shape = tensor.shape.as_list()

  non_static_indexes = []
  for (index, dim) in enumerate(shape):
    if dim is None:
      non_static_indexes.append(index)

  if not non_static_indexes:
    return shape

  dyn_shape = tf.shape(tensor)
  for index in non_static_indexes:
    shape[index] = dyn_shape[index]
  return shape


def reshape_to_matrix(input_tensor):
  """"""Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).""""""
  ndims = input_tensor.shape.ndims
  if ndims < 2:
    raise ValueError(""Input tensor must have at least rank 2. Shape = %s"" %
                     (input_tensor.shape))
  if ndims == 2:
    return input_tensor

  width = input_tensor.shape[-1]
  output_tensor = tf.reshape(input_tensor, [-1, width])
  return output_tensor


def reshape_from_matrix(output_tensor, orig_shape_list):
  """"""Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""""""
  if len(orig_shape_list) == 2:
    return output_tensor

  output_shape = get_shape_list(output_tensor)

  orig_dims = orig_shape_list[0:-1]
  width = output_shape[-1]

  return tf.reshape(output_tensor, orig_dims + [width])


def assert_rank(tensor, expected_rank, name=None):
  """"""Raises an exception if the tensor rank is not of the expected rank.
  Args:
    tensor: A tf.Tensor to check the rank of.
    expected_rank: Python integer or list of integers, expected rank.
    name: Optional name of the tensor for the error message.
  Raises:
    ValueError: If the expected shape doesn't match the actual shape.
  """"""
  if name is None:
    name = tensor.name

  expected_rank_dict = {}
  if isinstance(expected_rank, six.integer_types):
    expected_rank_dict[expected_rank] = True
  else:
    for x in expected_rank:
      expected_rank_dict[x] = True

  actual_rank = tensor.shape.ndims
  if actual_rank not in expected_rank_dict:
    scope_name = tf.get_variable_scope().name
    raise ValueError(
        ""For the tensor `%s` in scope `%s`, the actual rank ""
        ""`%d` (shape = %s) is not equal to the expected rank `%s`"" %
        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))
"
Albert,create_pretraining_data_google.py,"# coding=utf-8
# Copyright 2019 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Lint as: python2, python3
# coding=utf-8
""""""Create masked LM/next sentence masked_lm TF examples for ALBERT.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import random

import numpy as np
import six
from six.moves import range
from six.moves import zip
import tensorflow as tf

from albert import tokenization

flags = tf.flags

FLAGS = flags.FLAGS

flags.DEFINE_string(""input_file"", None,
                    ""Input raw text file (or comma-separated list of files)."")

flags.DEFINE_string(
    ""output_file"", None,
    ""Output TF example file (or comma-separated list of files)."")

flags.DEFINE_string(
    ""vocab_file"", None,
    ""The vocabulary file that the ALBERT model was trained on."")

flags.DEFINE_string(""spm_model_file"", None,
                    ""The model file for sentence piece tokenization."")

flags.DEFINE_bool(
    ""do_lower_case"", True,
    ""Whether to lower case the input text. Should be True for uncased ""
    ""models and False for cased models."")

flags.DEFINE_bool(
    ""do_whole_word_mask"", True,
    ""Whether to use whole word masking rather than per-xWordPiece masking."")

flags.DEFINE_bool(
    ""do_permutation"", False,
    ""Whether to do the permutation training."")

flags.DEFINE_bool(
    ""favor_shorter_ngram"", False,
    ""Whether to set higher probabilities for sampling shorter ngrams."")

flags.DEFINE_bool(
    ""random_next_sentence"", False,
    ""Whether to use the sentence that's right before the current sentence ""
    ""as the negative sample for next sentence prection, rather than using ""
    ""sentences from other random documents."")

flags.DEFINE_integer(""max_seq_length"", 512, ""Maximum sequence length."")

flags.DEFINE_integer(""ngram"", 3, ""Maximum number of ngrams to mask."")

flags.DEFINE_integer(""max_predictions_per_seq"", 20,
                     ""Maximum number of masked LM predictions per sequence."")

flags.DEFINE_integer(""random_seed"", 12345, ""Random seed for data generation."")

flags.DEFINE_integer(
    ""dupe_factor"", 10,
    ""Number of times to duplicate the input data (with different masks)."")

flags.DEFINE_float(""masked_lm_prob"", 0.15, ""Masked LM probability."")

flags.DEFINE_float(
    ""short_seq_prob"", 0.1,
    ""Probability of creating sequences which are shorter than the ""
    ""maximum length."")


class TrainingInstance(object):
  """"""A single training instance (sentence pair).""""""

  def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,
               is_random_next, token_boundary):
    self.tokens = tokens
    self.segment_ids = segment_ids
    self.is_random_next = is_random_next
    self.token_boundary = token_boundary
    self.masked_lm_positions = masked_lm_positions
    self.masked_lm_labels = masked_lm_labels

  def __str__(self):
    s = """"
    s += ""tokens: %s\n"" % ("" "".join(
        [tokenization.printable_text(x) for x in self.tokens]))
    s += ""segment_ids: %s\n"" % ("" "".join([str(x) for x in self.segment_ids]))
    s += ""token_boundary: %s\n"" % ("" "".join(
        [str(x) for x in self.token_boundary]))
    s += ""is_random_next: %s\n"" % self.is_random_next
    s += ""masked_lm_positions: %s\n"" % ("" "".join(
        [str(x) for x in self.masked_lm_positions]))
    s += ""masked_lm_labels: %s\n"" % ("" "".join(
        [tokenization.printable_text(x) for x in self.masked_lm_labels]))
    s += ""\n""
    return s

  def __repr__(self):
    return self.__str__()


def write_instance_to_example_files(instances, tokenizer, max_seq_length,
                                    max_predictions_per_seq, output_files):
  """"""Create TF example files from `TrainingInstance`s.""""""
  writers = []
  for output_file in output_files:
    writers.append(tf.python_io.TFRecordWriter(output_file))

  writer_index = 0

  total_written = 0
  for (inst_index, instance) in enumerate(instances):
    input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)
    input_mask = [1] * len(input_ids)
    segment_ids = list(instance.segment_ids)
    token_boundary = list(instance.token_boundary)
    assert len(input_ids) <= max_seq_length

    while len(input_ids) < max_seq_length:
      input_ids.append(0)
      input_mask.append(0)
      segment_ids.append(0)
      token_boundary.append(0)

    assert len(input_ids) == max_seq_length
    assert len(input_mask) == max_seq_length
    assert len(segment_ids) == max_seq_length

    masked_lm_positions = list(instance.masked_lm_positions)
    masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)
    masked_lm_weights = [1.0] * len(masked_lm_ids)

    multiplier = 1 + int(FLAGS.do_permutation)
    while len(masked_lm_positions) < max_predictions_per_seq * multiplier:
      masked_lm_positions.append(0)
      masked_lm_ids.append(0)
      masked_lm_weights.append(0.0)

    sentence_order_label = 1 if instance.is_random_next else 0

    features = collections.OrderedDict()
    features[""input_ids""] = create_int_feature(input_ids)
    features[""input_mask""] = create_int_feature(input_mask)
    features[""segment_ids""] = create_int_feature(segment_ids)
    features[""token_boundary""] = create_int_feature(token_boundary)
    features[""masked_lm_positions""] = create_int_feature(masked_lm_positions)
    features[""masked_lm_ids""] = create_int_feature(masked_lm_ids)
    features[""masked_lm_weights""] = create_float_feature(masked_lm_weights)
    # Note: We keep this feature name `next_sentence_labels` to be compatible
    # with the original data created by lanzhzh@. However, in the ALBERT case
    # it does contain sentence_order_label.
    features[""next_sentence_labels""] = create_int_feature(
        [sentence_order_label])

    tf_example = tf.train.Example(features=tf.train.Features(feature=features))

    writers[writer_index].write(tf_example.SerializeToString())
    writer_index = (writer_index + 1) % len(writers)

    total_written += 1

    if inst_index < 6:
      tf.logging.info(""*** Example ***"")
      tf.logging.info(""tokens: %s"" % "" "".join(
          [tokenization.printable_text(x) for x in instance.tokens]))

      for feature_name in features.keys():
        feature = features[feature_name]
        values = []
        if feature.int64_list.value:
          values = feature.int64_list.value
        elif feature.float_list.value:
          values = feature.float_list.value
        tf.logging.info(
            ""%s: %s"" % (feature_name, "" "".join([str(x) for x in values])))

  for writer in writers:
    writer.close()

  tf.logging.info(""Wrote %d total instances"", total_written)


def create_int_feature(values):
  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))
  return feature


def create_float_feature(values):
  feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))
  return feature


def create_training_instances(input_files, tokenizer, max_seq_length,
                              dupe_factor, short_seq_prob, masked_lm_prob,
                              max_predictions_per_seq, rng):
  """"""Create `TrainingInstance`s from raw text.""""""
  all_documents = [[]]

  # Input file format:
  # (1) One sentence per line. These should ideally be actual sentences, not
  # entire paragraphs or arbitrary spans of text. (Because we use the
  # sentence boundaries for the ""next sentence prediction"" task).
  # (2) Blank lines between documents. Document boundaries are needed so
  # that the ""next sentence prediction"" task doesn't span between documents.
  for input_file in input_files:
    with tf.gfile.GFile(input_file, ""r"") as reader:
      while True:
        line = reader.readline()
        if not FLAGS.spm_model_file:
          line = tokenization.convert_to_unicode(line)
        if not line:
          break
        if FLAGS.spm_model_file:
          line = tokenization.preprocess_text(line, lower=FLAGS.do_lower_case)
        else:
          line = line.strip()

        # Empty lines are used as document delimiters
        if not line:
          all_documents.append([])
        tokens = tokenizer.tokenize(line)
        if tokens:
          all_documents[-1].append(tokens)

  # Remove empty documents
  all_documents = [x for x in all_documents if x]
  rng.shuffle(all_documents)

  vocab_words = list(tokenizer.vocab.keys())
  instances = []
  for _ in range(dupe_factor):
    for document_index in range(len(all_documents)):
      instances.extend(
          create_instances_from_document(
              all_documents, document_index, max_seq_length, short_seq_prob,
              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))

  rng.shuffle(instances)
  return instances


def create_instances_from_document(
    all_documents, document_index, max_seq_length, short_seq_prob,
    masked_lm_prob, max_predictions_per_seq, vocab_words, rng):
  """"""Creates `TrainingInstance`s for a single document.""""""
  document = all_documents[document_index]

  # Account for [CLS], [SEP], [SEP]
  max_num_tokens = max_seq_length - 3

  # We *usually* want to fill up the entire sequence since we are padding
  # to `max_seq_length` anyways, so short sequences are generally wasted
  # computation. However, we *sometimes*
  # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter
  # sequences to minimize the mismatch between pre-training and fine-tuning.
  # The `target_seq_length` is just a rough target however, whereas
  # `max_seq_length` is a hard limit.
  target_seq_length = max_num_tokens
  if rng.random() < short_seq_prob:
    target_seq_length = rng.randint(2, max_num_tokens)

  # We DON'T just concatenate all of the tokens from a document into a long
  # sequence and choose an arbitrary split point because this would make the
  # next sentence prediction task too easy. Instead, we split the input into
  # segments ""A"" and ""B"" based on the actual ""sentences"" provided by the user
  # input.
  instances = []
  current_chunk = []
  current_length = 0
  i = 0
  while i < len(document):
    segment = document[i]
    current_chunk.append(segment)
    current_length += len(segment)
    if i == len(document) - 1 or current_length >= target_seq_length:
      if current_chunk:
        # `a_end` is how many segments from `current_chunk` go into the `A`
        # (first) sentence.
        a_end = 1
        if len(current_chunk) >= 2:
          a_end = rng.randint(1, len(current_chunk) - 1)

        tokens_a = []
        for j in range(a_end):
          tokens_a.extend(current_chunk[j])

        tokens_b = []
        # Random next
        is_random_next = False
        if len(current_chunk) == 1 or \
            (FLAGS.random_next_sentence and rng.random() < 0.5):
          is_random_next = True
          target_b_length = target_seq_length - len(tokens_a)

          # This should rarely go for more than one iteration for large
          # corpora. However, just to be careful, we try to make sure that
          # the random document is not the same as the document
          # we're processing.
          for _ in range(10):
            random_document_index = rng.randint(0, len(all_documents) - 1)
            if random_document_index != document_index:
              break

          random_document = all_documents[random_document_index]
          random_start = rng.randint(0, len(random_document) - 1)
          for j in range(random_start, len(random_document)):
            tokens_b.extend(random_document[j])
            if len(tokens_b) >= target_b_length:
              break
          # We didn't actually use these segments so we ""put them back"" so
          # they don't go to waste.
          num_unused_segments = len(current_chunk) - a_end
          i -= num_unused_segments
        elif not FLAGS.random_next_sentence and rng.random() < 0.5:
          is_random_next = True
          for j in range(a_end, len(current_chunk)):
            tokens_b.extend(current_chunk[j])
          # Note(mingdachen): in this case, we just swap tokens_a and tokens_b
          tokens_a, tokens_b = tokens_b, tokens_a
        # Actual next
        else:
          is_random_next = False
          for j in range(a_end, len(current_chunk)):
            tokens_b.extend(current_chunk[j])
        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)

        assert len(tokens_a) >= 1
        assert len(tokens_b) >= 1

        tokens = []
        segment_ids = []
        tokens.append(""[CLS]"")
        segment_ids.append(0)
        for token in tokens_a:
          tokens.append(token)
          segment_ids.append(0)

        tokens.append(""[SEP]"")
        segment_ids.append(0)

        for token in tokens_b:
          tokens.append(token)
          segment_ids.append(1)
        tokens.append(""[SEP]"")
        segment_ids.append(1)

        (tokens, masked_lm_positions,
         masked_lm_labels, token_boundary) = create_masked_lm_predictions(
             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)
        instance = TrainingInstance(
            tokens=tokens,
            segment_ids=segment_ids,
            is_random_next=is_random_next,
            token_boundary=token_boundary,
            masked_lm_positions=masked_lm_positions,
            masked_lm_labels=masked_lm_labels)
        instances.append(instance)
      current_chunk = []
      current_length = 0
    i += 1

  return instances


MaskedLmInstance = collections.namedtuple(""MaskedLmInstance"",
                                          [""index"", ""label""])


def _is_start_piece_sp(piece):
  """"""Check if the current word piece is the starting piece (sentence piece).""""""
  special_pieces = set(list('!""#$%&\""()*+,-./:;?@[\\]^_`{|}~'))
  special_pieces.add(u""€"".encode(""utf-8""))
  special_pieces.add(u""£"".encode(""utf-8""))
  # Note(mingdachen):
  # For foreign characters, we always treat them as a whole piece.
  english_chars = set(list(""abcdefghijklmnopqrstuvwhyz""))
  if (six.ensure_str(piece).startswith(""▁"") or
      six.ensure_str(piece).startswith(""<"") or piece in special_pieces or
      not all([i.lower() in english_chars.union(special_pieces)
               for i in piece])):
    return True
  else:
    return False


def _is_start_piece_bert(piece):
  """"""Check if the current word piece is the starting piece (BERT).""""""
  # When a word has been split into
  # WordPieces, the first token does not have any marker and any subsequence
  # tokens are prefixed with ##. So whenever we see the ## token, we
  # append it to the previous set of word indexes.
  return not six.ensure_str(piece).startswith(""##"")


def is_start_piece(piece):
  if FLAGS.spm_model_file:
    return _is_start_piece_sp(piece)
  else:
    return _is_start_piece_bert(piece)


def create_masked_lm_predictions(tokens, masked_lm_prob,
                                 max_predictions_per_seq, vocab_words, rng):
  """"""Creates the predictions for the masked LM objective.""""""

  cand_indexes = []
  # Note(mingdachen): We create a list for recording if the piece is
  # the starting piece of current token, where 1 means true, so that
  # on-the-fly whole word masking is possible.
  token_boundary = [0] * len(tokens)

  for (i, token) in enumerate(tokens):
    if token == ""[CLS]"" or token == ""[SEP]"":
      token_boundary[i] = 1
      continue
    # Whole Word Masking means that if we mask all of the wordpieces
    # corresponding to an original word.
    #
    # Note that Whole Word Masking does *not* change the training code
    # at all -- we still predict each WordPiece independently, softmaxed
    # over the entire vocabulary.
    if (FLAGS.do_whole_word_mask and len(cand_indexes) >= 1 and
        not is_start_piece(token)):
      cand_indexes[-1].append(i)
    else:
      cand_indexes.append([i])
      if is_start_piece(token):
        token_boundary[i] = 1

  output_tokens = list(tokens)

  masked_lm_positions = []
  masked_lm_labels = []

  if masked_lm_prob == 0:
    return (output_tokens, masked_lm_positions,
            masked_lm_labels, token_boundary)

  num_to_predict = min(max_predictions_per_seq,
                       max(1, int(round(len(tokens) * masked_lm_prob))))

  # Note(mingdachen):
  # By default, we set the probilities to favor longer ngram sequences.
  ngrams = np.arange(1, FLAGS.ngram + 1, dtype=np.int64)
  pvals = 1. / np.arange(1, FLAGS.ngram + 1)
  pvals /= pvals.sum(keepdims=True)

  if FLAGS.favor_shorter_ngram:
    pvals = pvals[::-1]

  ngram_indexes = []
  for idx in range(len(cand_indexes)):
    ngram_index = []
    for n in ngrams:
      ngram_index.append(cand_indexes[idx:idx+n])
    ngram_indexes.append(ngram_index)

  rng.shuffle(ngram_indexes)

  masked_lms = []
  covered_indexes = set()
  for cand_index_set in ngram_indexes:
    if len(masked_lms) >= num_to_predict:
      break
    if not cand_index_set:
      continue
    # Note(mingdachen):
    # Skip current piece if they are covered in lm masking or previous ngrams.
    for index_set in cand_index_set[0]:
      for index in index_set:
        if index in covered_indexes:
          continue

    n = np.random.choice(ngrams[:len(cand_index_set)],
                         p=pvals[:len(cand_index_set)] /
                         pvals[:len(cand_index_set)].sum(keepdims=True))
    index_set = sum(cand_index_set[n - 1], [])
    n -= 1
    # Note(mingdachen):
    # Repeatedly looking for a candidate that does not exceed the
    # maximum number of predictions by trying shorter ngrams.
    while len(masked_lms) + len(index_set) > num_to_predict:
      if n == 0:
        break
      index_set = sum(cand_index_set[n - 1], [])
      n -= 1
    # If adding a whole-word mask would exceed the maximum number of
    # predictions, then just skip this candidate.
    if len(masked_lms) + len(index_set) > num_to_predict:
      continue
    is_any_index_covered = False
    for index in index_set:
      if index in covered_indexes:
        is_any_index_covered = True
        break
    if is_any_index_covered:
      continue
    for index in index_set:
      covered_indexes.add(index)

      masked_token = None
      # 80% of the time, replace with [MASK]
      if rng.random() < 0.8:
        masked_token = ""[MASK]""
      else:
        # 10% of the time, keep original
        if rng.random() < 0.5:
          masked_token = tokens[index]
        # 10% of the time, replace with random word
        else:
          masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]

      output_tokens[index] = masked_token

      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))
  assert len(masked_lms) <= num_to_predict

  rng.shuffle(ngram_indexes)

  select_indexes = set()
  if FLAGS.do_permutation:
    for cand_index_set in ngram_indexes:
      if len(select_indexes) >= num_to_predict:
        break
      if not cand_index_set:
        continue
      # Note(mingdachen):
      # Skip current piece if they are covered in lm masking or previous ngrams.
      for index_set in cand_index_set[0]:
        for index in index_set:
          if index in covered_indexes or index in select_indexes:
            continue

      n = np.random.choice(ngrams[:len(cand_index_set)],
                           p=pvals[:len(cand_index_set)] /
                           pvals[:len(cand_index_set)].sum(keepdims=True))
      index_set = sum(cand_index_set[n - 1], [])
      n -= 1

      while len(select_indexes) + len(index_set) > num_to_predict:
        if n == 0:
          break
        index_set = sum(cand_index_set[n - 1], [])
        n -= 1
      # If adding a whole-word mask would exceed the maximum number of
      # predictions, then just skip this candidate.
      if len(select_indexes) + len(index_set) > num_to_predict:
        continue
      is_any_index_covered = False
      for index in index_set:
        if index in covered_indexes or index in select_indexes:
          is_any_index_covered = True
          break
      if is_any_index_covered:
        continue
      for index in index_set:
        select_indexes.add(index)
    assert len(select_indexes) <= num_to_predict

    select_indexes = sorted(select_indexes)
    permute_indexes = list(select_indexes)
    rng.shuffle(permute_indexes)
    orig_token = list(output_tokens)

    for src_i, tgt_i in zip(select_indexes, permute_indexes):
      output_tokens[src_i] = orig_token[tgt_i]
      masked_lms.append(MaskedLmInstance(index=src_i, label=orig_token[src_i]))

  masked_lms = sorted(masked_lms, key=lambda x: x.index)

  for p in masked_lms:
    masked_lm_positions.append(p.index)
    masked_lm_labels.append(p.label)
  return (output_tokens, masked_lm_positions, masked_lm_labels, token_boundary)


def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):
  """"""Truncates a pair of sequences to a maximum sequence length.""""""
  while True:
    total_length = len(tokens_a) + len(tokens_b)
    if total_length <= max_num_tokens:
      break

    trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b
    assert len(trunc_tokens) >= 1

    # We want to sometimes truncate from the front and sometimes from the
    # back to add more randomness and avoid biases.
    if rng.random() < 0.5:
      del trunc_tokens[0]
    else:
      trunc_tokens.pop()


def main(_):
  tf.logging.set_verbosity(tf.logging.INFO)

  tokenizer = tokenization.FullTokenizer(
      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case,
      spm_model_file=FLAGS.spm_model_file)

  input_files = []
  for input_pattern in FLAGS.input_file.split("",""):
    input_files.extend(tf.gfile.Glob(input_pattern))

  tf.logging.info(""*** Reading from input files ***"")
  for input_file in input_files:
    tf.logging.info(""  %s"", input_file)

  rng = random.Random(FLAGS.random_seed)
  instances = create_training_instances(
      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,
      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,
      rng)

  tf.logging.info(""number of instances: %i"", len(instances))

  output_files = FLAGS.output_file.split("","")
  tf.logging.info(""*** Writing to output files ***"")
  for output_file in output_files:
    tf.logging.info(""  %s"", output_file)

  write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,
                                  FLAGS.max_predictions_per_seq, output_files)


if __name__ == ""__main__"":
  flags.mark_flag_as_required(""input_file"")
  flags.mark_flag_as_required(""output_file"")
  flags.mark_flag_as_required(""vocab_file"")
  tf.app.run()"
Albert,create_pretraining_data.py,"# coding=utf-8
# Copyright 2018 The Google AI Language Team Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Create masked LM/next sentence masked_lm TF examples for BERT.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import random
import tokenization
import tensorflow as tf
import jieba
import re
flags = tf.flags

FLAGS = flags.FLAGS

flags.DEFINE_string(""input_file"", None,
                    ""Input raw text file (or comma-separated list of files)."")

flags.DEFINE_string(
    ""output_file"", None,
    ""Output TF example file (or comma-separated list of files)."")

flags.DEFINE_string(""vocab_file"", None,
                    ""The vocabulary file that the BERT model was trained on."")

flags.DEFINE_bool(
    ""do_lower_case"", True,
    ""Whether to lower case the input text. Should be True for uncased ""
    ""models and False for cased models."")

flags.DEFINE_bool(
    ""do_whole_word_mask"", False,
    ""Whether to use whole word masking rather than per-WordPiece masking."")

flags.DEFINE_integer(""max_seq_length"", 128, ""Maximum sequence length."")

flags.DEFINE_integer(""max_predictions_per_seq"", 20,
                     ""Maximum number of masked LM predictions per sequence."")

flags.DEFINE_integer(""random_seed"", 12345, ""Random seed for data generation."")

flags.DEFINE_integer(
    ""dupe_factor"", 10,
    ""Number of times to duplicate the input data (with different masks)."")

flags.DEFINE_float(""masked_lm_prob"", 0.15, ""Masked LM probability."")

flags.DEFINE_float(
    ""short_seq_prob"", 0.1,
    ""Probability of creating sequences which are shorter than the ""
    ""maximum length."")

flags.DEFINE_bool(""non_chinese"", False,""manually set this to True if you are not doing chinese pre-train task."")


class TrainingInstance(object):
  """"""A single training instance (sentence pair).""""""

  def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,
               is_random_next):
    self.tokens = tokens
    self.segment_ids = segment_ids
    self.is_random_next = is_random_next
    self.masked_lm_positions = masked_lm_positions
    self.masked_lm_labels = masked_lm_labels

  def __str__(self):
    s = """"
    s += ""tokens: %s\n"" % ("" "".join(
        [tokenization.printable_text(x) for x in self.tokens]))
    s += ""segment_ids: %s\n"" % ("" "".join([str(x) for x in self.segment_ids]))
    s += ""is_random_next: %s\n"" % self.is_random_next
    s += ""masked_lm_positions: %s\n"" % ("" "".join(
        [str(x) for x in self.masked_lm_positions]))
    s += ""masked_lm_labels: %s\n"" % ("" "".join(
        [tokenization.printable_text(x) for x in self.masked_lm_labels]))
    s += ""\n""
    return s

  def __repr__(self):
    return self.__str__()


def write_instance_to_example_files(instances, tokenizer, max_seq_length,
                                    max_predictions_per_seq, output_files):
  """"""Create TF example files from `TrainingInstance`s.""""""
  writers = []
  for output_file in output_files:
    writers.append(tf.python_io.TFRecordWriter(output_file))

  writer_index = 0

  total_written = 0
  for (inst_index, instance) in enumerate(instances):
    input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)
    input_mask = [1] * len(input_ids)
    segment_ids = list(instance.segment_ids)
    assert len(input_ids) <= max_seq_length

    while len(input_ids) < max_seq_length:
      input_ids.append(0)
      input_mask.append(0)
      segment_ids.append(0)

    assert len(input_ids) == max_seq_length
    assert len(input_mask) == max_seq_length
    assert len(segment_ids) == max_seq_length

    masked_lm_positions = list(instance.masked_lm_positions)
    masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)
    masked_lm_weights = [1.0] * len(masked_lm_ids)

    while len(masked_lm_positions) < max_predictions_per_seq:
      masked_lm_positions.append(0)
      masked_lm_ids.append(0)
      masked_lm_weights.append(0.0)

    next_sentence_label = 1 if instance.is_random_next else 0

    features = collections.OrderedDict()
    features[""input_ids""] = create_int_feature(input_ids)
    features[""input_mask""] = create_int_feature(input_mask)
    features[""segment_ids""] = create_int_feature(segment_ids)
    features[""masked_lm_positions""] = create_int_feature(masked_lm_positions)
    features[""masked_lm_ids""] = create_int_feature(masked_lm_ids)
    features[""masked_lm_weights""] = create_float_feature(masked_lm_weights)
    features[""next_sentence_labels""] = create_int_feature([next_sentence_label])

    tf_example = tf.train.Example(features=tf.train.Features(feature=features))

    writers[writer_index].write(tf_example.SerializeToString())
    writer_index = (writer_index + 1) % len(writers)

    total_written += 1

    if inst_index < 20:
      tf.logging.info(""*** Example ***"")
      tf.logging.info(""tokens: %s"" % "" "".join(
          [tokenization.printable_text(x) for x in instance.tokens]))

      for feature_name in features.keys():
        feature = features[feature_name]
        values = []
        if feature.int64_list.value:
          values = feature.int64_list.value
        elif feature.float_list.value:
          values = feature.float_list.value
        tf.logging.info(
            ""%s: %s"" % (feature_name, "" "".join([str(x) for x in values])))

  for writer in writers:
    writer.close()

  tf.logging.info(""Wrote %d total instances"", total_written)


def create_int_feature(values):
  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))
  return feature


def create_float_feature(values):
  feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))
  return feature


def create_training_instances(input_files, tokenizer, max_seq_length,
                              dupe_factor, short_seq_prob, masked_lm_prob,
                              max_predictions_per_seq, rng):
  """"""Create `TrainingInstance`s from raw text.""""""
  all_documents = [[]]

  # Input file format:
  # (1) One sentence per line. These should ideally be actual sentences, not
  # entire paragraphs or arbitrary spans of text. (Because we use the
  # sentence boundaries for the ""next sentence prediction"" task).
  # (2) Blank lines between documents. Document boundaries are needed so
  # that the ""next sentence prediction"" task doesn't span between documents.
  for input_file in input_files:
    with tf.gfile.GFile(input_file, ""r"") as reader:
      while True:
        strings=reader.readline()
        strings=strings.replace(""   "","" "").replace(""  "","" "") # 如果有两个或三个空格，替换为一个空格
        line = tokenization.convert_to_unicode(strings)
        if not line:
          break
        line = line.strip()

        # Empty lines are used as document delimiters
        if not line:
          all_documents.append([])
        tokens = tokenizer.tokenize(line)
        if tokens:
          all_documents[-1].append(tokens)

  # Remove empty documents
  all_documents = [x for x in all_documents if x]
  rng.shuffle(all_documents)

  vocab_words = list(tokenizer.vocab.keys())
  instances = []
  for _ in range(dupe_factor):
    for document_index in range(len(all_documents)):
      instances.extend(
        create_instances_from_document_albert( # change to albert style for sentence order prediction(SOP), 2019-08-28, brightmart
              all_documents, document_index, max_seq_length, short_seq_prob,
              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))

  rng.shuffle(instances)
  return instances

def get_new_segment(segment):  # 新增的方法 ####
    """"""
    输入一句话，返回一句经过处理的话: 为了支持中文全称mask，将被分开的词，将上特殊标记(""#"")，使得后续处理模块，能够知道哪些字是属于同一个词的。
    :param segment: 一句话. e.g.  ['悬', '灸', '技', '术', '培', '训', '专', '家', '教', '你', '艾', '灸', '降', '血', '糖', '，', '为', '爸', '妈', '收', '好', '了', '！']
    :return: 一句处理过的话 e.g.    ['悬', '##灸', '技', '术', '培', '训', '专', '##家', '教', '你', '艾', '##灸', '降', '##血', '##糖', '，', '为', '爸', '##妈', '收', '##好', '了', '！']
    """"""
    seq_cws = jieba.lcut("""".join(segment)) # 分词
    seq_cws_dict = {x: 1 for x in seq_cws} # 分词后的词加入到词典dict
    new_segment = []
    i = 0
    while i < len(segment): # 从句子的第一个字开始处理，知道处理完整个句子
      if len(re.findall('[\u4E00-\u9FA5]', segment[i])) == 0:  # 如果找不到中文的，原文加进去即不用特殊处理。
        new_segment.append(segment[i])
        i += 1
        continue

      has_add = False
      for length in range(3, 0, -1):
        if i + length > len(segment):
          continue
        if ''.join(segment[i:i + length]) in seq_cws_dict:
          new_segment.append(segment[i])
          for l in range(1, length):
            new_segment.append('##' + segment[i + l])
          i += length
          has_add = True
          break
      if not has_add:
        new_segment.append(segment[i])
        i += 1
    # print(""get_new_segment.wwm.get_new_segment:"",new_segment)
    return new_segment

def create_instances_from_document_albert(
    all_documents, document_index, max_seq_length, short_seq_prob,
    masked_lm_prob, max_predictions_per_seq, vocab_words, rng):
  """"""Creates `TrainingInstance`s for a single document.
     This method is changed to create sentence-order prediction (SOP) followed by idea from paper of ALBERT, 2019-08-28, brightmart
  """"""
  document = all_documents[document_index] # 得到一个文档

  # Account for [CLS], [SEP], [SEP]
  max_num_tokens = max_seq_length - 3

  # We *usually* want to fill up the entire sequence since we are padding
  # to `max_seq_length` anyways, so short sequences are generally wasted
  # computation. However, we *sometimes*
  # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter
  # sequences to minimize the mismatch between pre-training and fine-tuning.
  # The `target_seq_length` is just a rough target however, whereas
  # `max_seq_length` is a hard limit.
  target_seq_length = max_num_tokens
  if rng.random() < short_seq_prob: # 有一定的比例，如10%的概率，我们使用比较短的序列长度，以缓解预训练的长序列和调优阶段（可能的）短序列的不一致情况
    target_seq_length = rng.randint(2, max_num_tokens)

  # We DON'T just concatenate all of the tokens from a document into a long
  # sequence and choose an arbitrary split point because this would make the
  # next sentence prediction task too easy. Instead, we split the input into
  # segments ""A"" and ""B"" based on the actual ""sentences"" provided by the user
  # input.
  # 设法使用实际的句子，而不是任意的截断句子，从而更好的构造句子连贯性预测的任务
  instances = []
  current_chunk = [] # 当前处理的文本段，包含多个句子
  current_length = 0
  i = 0
  # print(""###document:"",document) # 一个document可以是一整篇文章、新闻、词条等. document:[['是', '爷', '们', '，', '就', '得', '给', '媳', '妇', '幸', '福'], ['关', '注', '【', '晨', '曦', '教', '育', '】', '，', '获', '取', '育', '儿', '的', '智', '慧', '，', '与', '孩', '子', '一', '同', '成', '长', '！'], ['方', '法', ':', '打', '开', '微', '信', '→', '添', '加', '朋', '友', '→', '搜', '号', '→', '##he', '##bc', '##x', '##jy', '##→', '关', '注', '!', '我', '是', '一', '个', '爷', '们', '，', '孝', '顺', '是', '做', '人', '的', '第', '一', '准', '则', '。'], ['甭', '管', '小', '时', '候', '怎', '么', '跟', '家', '长', '犯', '混', '蛋', '，', '长', '大', '了', '，', '就', '底', '报', '答', '父', '母', '，', '以', '后', '我', '媳', '妇', '也', '必', '须', '孝', '顺', '。'], ['我', '是', '一', '个', '爷', '们', '，', '可', '以', '花', '心', '，', '可', '以', '好', '玩', '。'], ['但', '我', '一', '定', '会', '找', '一', '个', '管', '的', '住', '我', '的', '女', '人', '，', '和', '我', '一', '起', '生', '活', '。'], ['28', '岁', '以', '前', '在', '怎', '么', '玩', '都', '行', '，', '但', '我', '最', '后', '一', '定', '会', '找', '一', '个', '勤', '俭', '持', '家', '的', '女', '人', '。'], ['我', '是', '一', '爷', '们', '，', '我', '不', '会', '让', '自', '己', '的', '女', '人', '受', '一', '点', '委', '屈', '，', '每', '次', '把', '她', '抱', '在', '怀', '里', '，', '看', '她', '洋', '溢', '着', '幸', '福', '的', '脸', '，', '我', '都', '会', '引', '以', '为', '傲', '，', '这', '特', '么', '就', '是', '我', '的', '女', '人', '。'], ['我', '是', '一', '爷', '们', '，', '干', '什', '么', '也', '不', '能', '忘', '了', '自', '己', '媳', '妇', '，', '就', '算', '和', '哥', '们', '一', '起', '喝', '酒', '，', '喝', '到', '很', '晚', '，', '也', '要', '提', '前', '打', '电', '话', '告', '诉', '她', '，', '让', '她', '早', '点', '休', '息', '。'], ['我', '是', '一', '爷', '们', '，', '我', '媳', '妇', '绝', '对', '不', '能', '抽', '烟', '，', '喝', '酒', '还', '勉', '强', '过', '得', '去', '，', '不', '过', '该', '喝', '的', '时', '候', '喝', '，', '不', '该', '喝', '的', '时', '候', '，', '少', '扯', '纳', '极', '薄', '蛋', '。'], ['我', '是', '一', '爷', '们', '，', '我', '媳', '妇', '必', '须', '听', '我', '话', '，', '在', '人', '前', '一', '定', '要', '给', '我', '面', '子', '，', '回', '家', '了', '咱', '什', '么', '都', '好', '说', '。'], ['我', '是', '一', '爷', '们', '，', '就', '算', '难', '的', '吃', '不', '上', '饭', '了', '，', '都', '不', '张', '口', '跟', '媳', '妇', '要', '一', '分', '钱', '。'], ['我', '是', '一', '爷', '们', '，', '不', '管', '上', '学', '还', '是', '上', '班', '，', '我', '都', '会', '送', '媳', '妇', '回', '家', '。'], ['我', '是', '一', '爷', '们', '，', '交', '往', '不', '到', '1', '年', '，', '绝', '对', '不', '会', '和', '媳', '妇', '提', '过', '分', '的', '要', '求', '，', '我', '会', '尊', '重', '她', '。'], ['我', '是', '一', '爷', '们', '，', '游', '戏', '永', '远', '比', '不', '上', '我', '媳', '妇', '重', '要', '，', '只', '要', '媳', '妇', '发', '话', '，', '我', '绝', '对', '唯', '命', '是', '从', '。'], ['我', '是', '一', '爷', '们', '，', '上', 'q', '绝', '对', '是', '为', '了', '等', '媳', '妇', '，', '所', '有', '暧', '昧', '的', '心', '情', '只', '为', '她', '一', '个', '女', '人', '而', '写', '，', '我', '不', '一', '定', '会', '经', '常', '写', '日', '志', '，', '可', '是', '我', '会', '告', '诉', '全', '世', '界', '，', '我', '很', '爱', '她', '。'], ['我', '是', '一', '爷', '们', '，', '不', '一', '定', '要', '经', '常', '制', '造', '浪', '漫', '、', '偶', '尔', '过', '个', '节', '日', '也', '要', '送', '束', '玫', '瑰', '花', '给', '媳', '妇', '抱', '回', '家', '。'], ['我', '是', '一', '爷', '们', '，', '手', '机', '会', '24', '小', '时', '为', '她', '开', '机', '，', '让', '她', '半', '夜', '痛', '经', '的', '时', '候', '，', '做', '恶', '梦', '的', '时', '候', '，', '随', '时', '可', '以', '联', '系', '到', '我', '。'], ['我', '是', '一', '爷', '们', '，', '我', '会', '经', '常', '带', '媳', '妇', '出', '去', '玩', '，', '她', '不', '一', '定', '要', '和', '我', '所', '有', '的', '哥', '们', '都', '认', '识', '，', '但', '见', '面', '能', '说', '的', '上', '话', '就', '行', '。'], ['我', '是', '一', '爷', '们', '，', '我', '会', '和', '媳', '妇', '的', '姐', '妹', '哥', '们', '搞', '好', '关', '系', '，', '让', '她', '们', '相', '信', '我', '一', '定', '可', '以', '给', '我', '媳', '妇', '幸', '福', '。'], ['我', '是', '一', '爷', '们', '，', '吵', '架', '后', '、', '也', '要', '主', '动', '打', '电', '话', '关', '心', '她', '，', '咱', '是', '一', '爷', '们', '，', '给', '媳', '妇', '服', '个', '软', '，', '道', '个', '歉', '怎', '么', '了', '？'], ['我', '是', '一', '爷', '们', '，', '绝', '对', '不', '会', '嫌', '弃', '自', '己', '媳', '妇', '，', '拿', '她', '和', '别', '人', '比', '，', '说', '她', '这', '不', '如', '人', '家', '，', '纳', '不', '如', '人', '家', '的', '。'], ['我', '是', '一', '爷', '们', '，', '陪', '媳', '妇', '逛', '街', '时', '，', '碰', '见', '熟', '人', '，', '无', '论', '我', '媳', '妇', '长', '的', '好', '看', '与', '否', '，', '我', '都', '会', '大', '方', '的', '介', '绍', '。'], ['谁', '让', '咱', '爷', '们', '就', '好', '这', '口', '呢', '。'], ['我', '是', '一', '爷', '们', '，', '我', '想', '我', '会', '给', '我', '媳', '妇', '最', '好', '的', '幸', '福', '。'], ['【', '我', '们', '重', '在', '分', '享', '。'], ['所', '有', '文', '字', '和', '美', '图', '，', '来', '自', '网', '络', '，', '晨', '欣', '教', '育', '整', '理', '。'], ['对', '原', '文', '作', '者', '，', '表', '示', '敬', '意', '。'], ['】', '关', '注', '晨', '曦', '教', '育', '[UNK]', '[UNK]', '晨', '曦', '教', '育', '（', '微', '信', '号', '：', 'he', '##bc', '##x', '##jy', '）', '。'], ['打', '开', '微', '信', '，', '扫', '描', '二', '维', '码', '，', '关', '注', '[UNK]', '晨', '曦', '教', '育', '[UNK]', '，', '获', '取', '更', '多', '育', '儿', '资', '源', '。'], ['点', '击', '下', '面', '订', '阅', '按', '钮', '订', '阅', '，', '会', '有', '更', '多', '惊', '喜', '哦', '！']]
  while i < len(document): # 从文档的第一个位置开始，按个往下看
    segment = document[i] # segment是列表，代表的是按字分开的一个完整句子，如 segment=['我', '是', '一', '爷', '们', '，', '我', '想', '我', '会', '给', '我', '媳', '妇', '最', '好', '的', '幸', '福', '。']
    if FLAGS.non_chinese==False: # if non chinese is False, that means it is chinese, then do something to make chinese whole word mask works.
      segment = get_new_segment(segment)  # whole word mask for chinese: 结合分词的中文的whole mask设置即在需要的地方加上“##”

    current_chunk.append(segment) # 将一个独立的句子加入到当前的文本块中
    current_length += len(segment) # 累计到为止位置接触到句子的总长度
    if i == len(document) - 1 or current_length >= target_seq_length:
      # 如果累计的序列长度达到了目标的长度，或当前走到了文档结尾==>构造并添加到“A[SEP]B“中的A和B中；
      if current_chunk: # 如果当前块不为空
        # `a_end` is how many segments from `current_chunk` go into the `A`
        # (first) sentence.
        a_end = 1
        if len(current_chunk) >= 2: # 当前块，如果包含超过两个句子，取当前块的一部分作为“A[SEP]B“中的A部分
          a_end = rng.randint(1, len(current_chunk) - 1)
        # 将当前文本段中选取出来的前半部分，赋值给A即tokens_a
        tokens_a = []
        for j in range(a_end):
          tokens_a.extend(current_chunk[j])

        # 构造“A[SEP]B“中的B部分(有一部分是正常的当前文档中的后半部;在原BERT的实现中一部分是随机的从另一个文档中选取的，）
        tokens_b = []
        for j in range(a_end, len(current_chunk)):
          tokens_b.extend(current_chunk[j])

        # 有百分之50%的概率交换一下tokens_a和tokens_b的位置
        # print(""tokens_a length1:"",len(tokens_a))
        # print(""tokens_b length1:"",len(tokens_b)) # len(tokens_b) = 0

        if len(tokens_a) == 0 or len(tokens_b) == 0: i += 1; continue
        if rng.random() < 0.5: # 交换一下tokens_a和tokens_b
          is_random_next=True
          temp=tokens_a
          tokens_a=tokens_b
          tokens_b=temp
        else:
          is_random_next=False

        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)

        assert len(tokens_a) >= 1
        assert len(tokens_b) >= 1

        # 把tokens_a & tokens_b加入到按照bert的风格，即以[CLS]tokens_a[SEP]tokens_b[SEP]的形式，结合到一起，作为最终的tokens; 也带上segment_ids，前面部分segment_ids的值是0，后面部分的值是1.
        tokens = []
        segment_ids = []
        tokens.append(""[CLS]"")
        segment_ids.append(0)
        for token in tokens_a:
          tokens.append(token)
          segment_ids.append(0)

        tokens.append(""[SEP]"")
        segment_ids.append(0)

        for token in tokens_b:
          tokens.append(token)
          segment_ids.append(1)
        tokens.append(""[SEP]"")
        segment_ids.append(1)

        # 创建masked LM的任务的数据 Creates the predictions for the masked LM objective
        (tokens, masked_lm_positions,
         masked_lm_labels) = create_masked_lm_predictions(
             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)
        instance = TrainingInstance( # 创建训练实例的对象
            tokens=tokens,
            segment_ids=segment_ids,
            is_random_next=is_random_next,
            masked_lm_positions=masked_lm_positions,
            masked_lm_labels=masked_lm_labels)
        instances.append(instance)
      current_chunk = [] # 清空当前块
      current_length = 0 # 重置当前文本块的长度
    i += 1 # 接着文档中的内容往后看

  return instances


def create_instances_from_document_original( # THIS IS ORIGINAL BERT STYLE FOR CREATE DATA OF MLM AND NEXT SENTENCE PREDICTION TASK
    all_documents, document_index, max_seq_length, short_seq_prob,
    masked_lm_prob, max_predictions_per_seq, vocab_words, rng):
  """"""Creates `TrainingInstance`s for a single document.""""""
  document = all_documents[document_index] # 得到一个文档

  # Account for [CLS], [SEP], [SEP]
  max_num_tokens = max_seq_length - 3

  # We *usually* want to fill up the entire sequence since we are padding
  # to `max_seq_length` anyways, so short sequences are generally wasted
  # computation. However, we *sometimes*
  # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter
  # sequences to minimize the mismatch between pre-training and fine-tuning.
  # The `target_seq_length` is just a rough target however, whereas
  # `max_seq_length` is a hard limit.
  target_seq_length = max_num_tokens
  if rng.random() < short_seq_prob: # 有一定的比例，如10%的概率，我们使用比较短的序列长度，以缓解预训练的长序列和调优阶段（可能的）短序列的不一致情况
    target_seq_length = rng.randint(2, max_num_tokens)

  # We DON'T just concatenate all of the tokens from a document into a long
  # sequence and choose an arbitrary split point because this would make the
  # next sentence prediction task too easy. Instead, we split the input into
  # segments ""A"" and ""B"" based on the actual ""sentences"" provided by the user
  # input.
  # 设法使用实际的句子，而不是任意的截断句子，从而更好的构造句子连贯性预测的任务
  instances = []
  current_chunk = [] # 当前处理的文本段，包含多个句子
  current_length = 0
  i = 0
  # print(""###document:"",document) # 一个document可以是一整篇文章、新闻、一个词条等. document:[['是', '爷', '们', '，', '就', '得', '给', '媳', '妇', '幸', '福'], ['关', '注', '【', '晨', '曦', '教', '育', '】', '，', '获', '取', '育', '儿', '的', '智', '慧', '，', '与', '孩', '子', '一', '同', '成', '长', '！'], ['方', '法', ':', '打', '开', '微', '信', '→', '添', '加', '朋', '友', '→', '搜', '号', '→', '##he', '##bc', '##x', '##jy', '##→', '关', '注', '!', '我', '是', '一', '个', '爷', '们', '，', '孝', '顺', '是', '做', '人', '的', '第', '一', '准', '则', '。'], ['甭', '管', '小', '时', '候', '怎', '么', '跟', '家', '长', '犯', '混', '蛋', '，', '长', '大', '了', '，', '就', '底', '报', '答', '父', '母', '，', '以', '后', '我', '媳', '妇', '也', '必', '须', '孝', '顺', '。'], ['我', '是', '一', '个', '爷', '们', '，', '可', '以', '花', '心', '，', '可', '以', '好', '玩', '。'], ['但', '我', '一', '定', '会', '找', '一', '个', '管', '的', '住', '我', '的', '女', '人', '，', '和', '我', '一', '起', '生', '活', '。'], ['28', '岁', '以', '前', '在', '怎', '么', '玩', '都', '行', '，', '但', '我', '最', '后', '一', '定', '会', '找', '一', '个', '勤', '俭', '持', '家', '的', '女', '人', '。'], ['我', '是', '一', '爷', '们', '，', '我', '不', '会', '让', '自', '己', '的', '女', '人', '受', '一', '点', '委', '屈', '，', '每', '次', '把', '她', '抱', '在', '怀', '里', '，', '看', '她', '洋', '溢', '着', '幸', '福', '的', '脸', '，', '我', '都', '会', '引', '以', '为', '傲', '，', '这', '特', '么', '就', '是', '我', '的', '女', '人', '。'], ['我', '是', '一', '爷', '们', '，', '干', '什', '么', '也', '不', '能', '忘', '了', '自', '己', '媳', '妇', '，', '就', '算', '和', '哥', '们', '一', '起', '喝', '酒', '，', '喝', '到', '很', '晚', '，', '也', '要', '提', '前', '打', '电', '话', '告', '诉', '她', '，', '让', '她', '早', '点', '休', '息', '。'], ['我', '是', '一', '爷', '们', '，', '我', '媳', '妇', '绝', '对', '不', '能', '抽', '烟', '，', '喝', '酒', '还', '勉', '强', '过', '得', '去', '，', '不', '过', '该', '喝', '的', '时', '候', '喝', '，', '不', '该', '喝', '的', '时', '候', '，', '少', '扯', '纳', '极', '薄', '蛋', '。'], ['我', '是', '一', '爷', '们', '，', '我', '媳', '妇', '必', '须', '听', '我', '话', '，', '在', '人', '前', '一', '定', '要', '给', '我', '面', '子', '，', '回', '家', '了', '咱', '什', '么', '都', '好', '说', '。'], ['我', '是', '一', '爷', '们', '，', '就', '算', '难', '的', '吃', '不', '上', '饭', '了', '，', '都', '不', '张', '口', '跟', '媳', '妇', '要', '一', '分', '钱', '。'], ['我', '是', '一', '爷', '们', '，', '不', '管', '上', '学', '还', '是', '上', '班', '，', '我', '都', '会', '送', '媳', '妇', '回', '家', '。'], ['我', '是', '一', '爷', '们', '，', '交', '往', '不', '到', '1', '年', '，', '绝', '对', '不', '会', '和', '媳', '妇', '提', '过', '分', '的', '要', '求', '，', '我', '会', '尊', '重', '她', '。'], ['我', '是', '一', '爷', '们', '，', '游', '戏', '永', '远', '比', '不', '上', '我', '媳', '妇', '重', '要', '，', '只', '要', '媳', '妇', '发', '话', '，', '我', '绝', '对', '唯', '命', '是', '从', '。'], ['我', '是', '一', '爷', '们', '，', '上', 'q', '绝', '对', '是', '为', '了', '等', '媳', '妇', '，', '所', '有', '暧', '昧', '的', '心', '情', '只', '为', '她', '一', '个', '女', '人', '而', '写', '，', '我', '不', '一', '定', '会', '经', '常', '写', '日', '志', '，', '可', '是', '我', '会', '告', '诉', '全', '世', '界', '，', '我', '很', '爱', '她', '。'], ['我', '是', '一', '爷', '们', '，', '不', '一', '定', '要', '经', '常', '制', '造', '浪', '漫', '、', '偶', '尔', '过', '个', '节', '日', '也', '要', '送', '束', '玫', '瑰', '花', '给', '媳', '妇', '抱', '回', '家', '。'], ['我', '是', '一', '爷', '们', '，', '手', '机', '会', '24', '小', '时', '为', '她', '开', '机', '，', '让', '她', '半', '夜', '痛', '经', '的', '时', '候', '，', '做', '恶', '梦', '的', '时', '候', '，', '随', '时', '可', '以', '联', '系', '到', '我', '。'], ['我', '是', '一', '爷', '们', '，', '我', '会', '经', '常', '带', '媳', '妇', '出', '去', '玩', '，', '她', '不', '一', '定', '要', '和', '我', '所', '有', '的', '哥', '们', '都', '认', '识', '，', '但', '见', '面', '能', '说', '的', '上', '话', '就', '行', '。'], ['我', '是', '一', '爷', '们', '，', '我', '会', '和', '媳', '妇', '的', '姐', '妹', '哥', '们', '搞', '好', '关', '系', '，', '让', '她', '们', '相', '信', '我', '一', '定', '可', '以', '给', '我', '媳', '妇', '幸', '福', '。'], ['我', '是', '一', '爷', '们', '，', '吵', '架', '后', '、', '也', '要', '主', '动', '打', '电', '话', '关', '心', '她', '，', '咱', '是', '一', '爷', '们', '，', '给', '媳', '妇', '服', '个', '软', '，', '道', '个', '歉', '怎', '么', '了', '？'], ['我', '是', '一', '爷', '们', '，', '绝', '对', '不', '会', '嫌', '弃', '自', '己', '媳', '妇', '，', '拿', '她', '和', '别', '人', '比', '，', '说', '她', '这', '不', '如', '人', '家', '，', '纳', '不', '如', '人', '家', '的', '。'], ['我', '是', '一', '爷', '们', '，', '陪', '媳', '妇', '逛', '街', '时', '，', '碰', '见', '熟', '人', '，', '无', '论', '我', '媳', '妇', '长', '的', '好', '看', '与', '否', '，', '我', '都', '会', '大', '方', '的', '介', '绍', '。'], ['谁', '让', '咱', '爷', '们', '就', '好', '这', '口', '呢', '。'], ['我', '是', '一', '爷', '们', '，', '我', '想', '我', '会', '给', '我', '媳', '妇', '最', '好', '的', '幸', '福', '。'], ['【', '我', '们', '重', '在', '分', '享', '。'], ['所', '有', '文', '字', '和', '美', '图', '，', '来', '自', '网', '络', '，', '晨', '欣', '教', '育', '整', '理', '。'], ['对', '原', '文', '作', '者', '，', '表', '示', '敬', '意', '。'], ['】', '关', '注', '晨', '曦', '教', '育', '[UNK]', '[UNK]', '晨', '曦', '教', '育', '（', '微', '信', '号', '：', 'he', '##bc', '##x', '##jy', '）', '。'], ['打', '开', '微', '信', '，', '扫', '描', '二', '维', '码', '，', '关', '注', '[UNK]', '晨', '曦', '教', '育', '[UNK]', '，', '获', '取', '更', '多', '育', '儿', '资', '源', '。'], ['点', '击', '下', '面', '订', '阅', '按', '钮', '订', '阅', '，', '会', '有', '更', '多', '惊', '喜', '哦', '！']]
  while i < len(document): # 从文档的第一个位置开始，按个往下看
    segment = document[i] # segment是列表，代表的是按字分开的一个完整句子，如 segment=['我', '是', '一', '爷', '们', '，', '我', '想', '我', '会', '给', '我', '媳', '妇', '最', '好', '的', '幸', '福', '。']
    # print(""###i:"",i,"";segment:"",segment)
    current_chunk.append(segment) # 将一个独立的句子加入到当前的文本块中
    current_length += len(segment) # 累计到为止位置接触到句子的总长度
    if i == len(document) - 1 or current_length >= target_seq_length: # 如果累计的序列长度达到了目标的长度==>构造并添加到“A[SEP]B“中的A和B中。
      if current_chunk: # 如果当前块不为空
        # `a_end` is how many segments from `current_chunk` go into the `A`
        # (first) sentence.
        a_end = 1
        if len(current_chunk) >= 2: # 当前块，如果包含超过两个句子，怎取当前块的一部分作为“A[SEP]B“中的A部分
          a_end = rng.randint(1, len(current_chunk) - 1)
        # 将当前文本段中选取出来的前半部分，赋值给A即tokens_a
        tokens_a = []
        for j in range(a_end):
          tokens_a.extend(current_chunk[j])

        # 构造“A[SEP]B“中的B部分(原本的B有一部分是随机的从另一个文档中选取的，有一部分是正常的当前文档中的后半部）
        tokens_b = []
        # Random next
        is_random_next = False
        if len(current_chunk) == 1 or rng.random() < 0.5: # 有50%的概率，是从其他文档中随机的选取一个文档，并得到这个文档的后半版本作为B即tokens_b
          is_random_next = True
          target_b_length = target_seq_length - len(tokens_a)

          # This should rarely go for more than one iteration for large
          # corpora. However, just to be careful, we try to make sure that
          # the random document is not the same as the document
          # we're processing.
          random_document_index=0
          for _ in range(10): # 随机的选出一个与当前的文档不一样的文档的索引
            random_document_index = rng.randint(0, len(all_documents) - 1)
            if random_document_index != document_index:
              break

          random_document = all_documents[random_document_index] # 选出这个文档
          random_start = rng.randint(0, len(random_document) - 1) # 从这个文档选出一个段落的开始位置
          for j in range(random_start, len(random_document)): # 从这个文档的开始位置到结束，作为我们的“A[SEP]B“中的B即tokens_b
            tokens_b.extend(random_document[j])
            if len(tokens_b) >= target_b_length:
              break
          # We didn't actually use these segments so we ""put them back"" so
          # they don't go to waste. 这里是为了防止文本的浪费的一个小技巧
          num_unused_segments = len(current_chunk) - a_end # e.g. 550-200=350
          i -= num_unused_segments # i=i-num_unused_segments, e.g. i=400, num_unused_segments=350, 那么 i=i-num_unused_segments=400-350=50
        # Actual next
        else: # 有另外50%的几乎，从当前文本块（长度为max_sequence_length）中的后段中填充到tokens_b即“A[SEP]B“中的B。
          is_random_next = False
          for j in range(a_end, len(current_chunk)):
            tokens_b.extend(current_chunk[j])
        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)

        assert len(tokens_a) >= 1
        assert len(tokens_b) >= 1

        # 把tokens_a & tokens_b加入到按照bert的风格，即以[CLS]tokens_a[SEP]tokens_b[SEP]的形式，结合到一起，作为最终的tokens; 也带上segment_ids，前面部分segment_ids的值是0，后面部分的值是1.
        tokens = []
        segment_ids = []
        tokens.append(""[CLS]"")
        segment_ids.append(0)
        for token in tokens_a:
          tokens.append(token)
          segment_ids.append(0)

        tokens.append(""[SEP]"")
        segment_ids.append(0)

        for token in tokens_b:
          tokens.append(token)
          segment_ids.append(1)
        tokens.append(""[SEP]"")
        segment_ids.append(1)

        # 创建masked LM的任务的数据 Creates the predictions for the masked LM objective
        (tokens, masked_lm_positions,
         masked_lm_labels) = create_masked_lm_predictions(
             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)
        instance = TrainingInstance( # 创建训练实例的对象
            tokens=tokens,
            segment_ids=segment_ids,
            is_random_next=is_random_next,
            masked_lm_positions=masked_lm_positions,
            masked_lm_labels=masked_lm_labels)
        instances.append(instance)
      current_chunk = [] # 清空当前块
      current_length = 0 # 重置当前文本块的长度
    i += 1 # 接着文档中的内容往后看

  return instances


MaskedLmInstance = collections.namedtuple(""MaskedLmInstance"",
                                          [""index"", ""label""])


def create_masked_lm_predictions(tokens, masked_lm_prob,
                                 max_predictions_per_seq, vocab_words, rng):
  """"""Creates the predictions for the masked LM objective.""""""

  cand_indexes = []
  for (i, token) in enumerate(tokens):
    if token == ""[CLS]"" or token == ""[SEP]"":
      continue
    # Whole Word Masking means that if we mask all of the wordpieces
    # corresponding to an original word. When a word has been split into
    # WordPieces, the first token does not have any marker and any subsequence
    # tokens are prefixed with ##. So whenever we see the ## token, we
    # append it to the previous set of word indexes.
    #
    # Note that Whole Word Masking does *not* change the training code
    # at all -- we still predict each WordPiece independently, softmaxed
    # over the entire vocabulary.
    if (FLAGS.do_whole_word_mask and len(cand_indexes) >= 1 and
            token.startswith(""##"")):
      cand_indexes[-1].append(i)
    else:
      cand_indexes.append([i])

  rng.shuffle(cand_indexes)

  if FLAGS.non_chinese==False: # if non chinese is False, that means it is chinese, then try to remove ""##"" which is added previously
    output_tokens = [t[2:] if len(re.findall('##[\u4E00-\u9FA5]', t)) > 0 else t for t in tokens]  # 去掉""##""
  else: # english and other language, which is not chinese
    output_tokens = list(tokens)

  num_to_predict = min(max_predictions_per_seq,
                       max(1, int(round(len(tokens) * masked_lm_prob))))

  masked_lms = []
  covered_indexes = set()
  for index_set in cand_indexes:
    if len(masked_lms) >= num_to_predict:
      break
    # If adding a whole-word mask would exceed the maximum number of
    # predictions, then just skip this candidate.
    if len(masked_lms) + len(index_set) > num_to_predict:
      continue
    is_any_index_covered = False
    for index in index_set:
      if index in covered_indexes:
        is_any_index_covered = True
        break
    if is_any_index_covered:
      continue
    for index in index_set:
      covered_indexes.add(index)

      masked_token = None
      # 80% of the time, replace with [MASK]
      if rng.random() < 0.8:
        masked_token = ""[MASK]""
      else:
        # 10% of the time, keep original
        if rng.random() < 0.5:
          if FLAGS.non_chinese == False: # if non chinese is False, that means it is chinese, then try to remove ""##"" which is added previously
            masked_token = tokens[index][2:] if len(re.findall('##[\u4E00-\u9FA5]', tokens[index])) > 0 else tokens[index]  # 去掉""##""
          else:
            masked_token = tokens[index]
        # 10% of the time, replace with random word
        else:
          masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]

      output_tokens[index] = masked_token

      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))
  assert len(masked_lms) <= num_to_predict
  masked_lms = sorted(masked_lms, key=lambda x: x.index)

  masked_lm_positions = []
  masked_lm_labels = []
  for p in masked_lms:
    masked_lm_positions.append(p.index)
    masked_lm_labels.append(p.label)

  # tf.logging.info('%s' % (tokens))
  # tf.logging.info('%s' % (output_tokens))
  return (output_tokens, masked_lm_positions, masked_lm_labels)

def create_masked_lm_predictions_original(tokens, masked_lm_prob,
                                 max_predictions_per_seq, vocab_words, rng):
  """"""Creates the predictions for the masked LM objective.""""""

  cand_indexes = []
  for (i, token) in enumerate(tokens):
    if token == ""[CLS]"" or token == ""[SEP]"":
      continue
    # Whole Word Masking means that if we mask all of the wordpieces
    # corresponding to an original word. When a word has been split into
    # WordPieces, the first token does not have any marker and any subsequence
    # tokens are prefixed with ##. So whenever we see the ## token, we
    # append it to the previous set of word indexes.
    #
    # Note that Whole Word Masking does *not* change the training code
    # at all -- we still predict each WordPiece independently, softmaxed
    # over the entire vocabulary.
    if (FLAGS.do_whole_word_mask and len(cand_indexes) >= 1 and
        token.startswith(""##"")):
      cand_indexes[-1].append(i)
    else:
      cand_indexes.append([i])

  rng.shuffle(cand_indexes)

  output_tokens = list(tokens)

  num_to_predict = min(max_predictions_per_seq,
                       max(1, int(round(len(tokens) * masked_lm_prob))))

  masked_lms = []
  covered_indexes = set()
  for index_set in cand_indexes:
    if len(masked_lms) >= num_to_predict:
      break
    # If adding a whole-word mask would exceed the maximum number of
    # predictions, then just skip this candidate.
    if len(masked_lms) + len(index_set) > num_to_predict:
      continue
    is_any_index_covered = False
    for index in index_set:
      if index in covered_indexes:
        is_any_index_covered = True
        break
    if is_any_index_covered:
      continue
    for index in index_set:
      covered_indexes.add(index)

      masked_token = None
      # 80% of the time, replace with [MASK]
      if rng.random() < 0.8:
        masked_token = ""[MASK]""
      else:
        # 10% of the time, keep original
        if rng.random() < 0.5:
          masked_token = tokens[index]
        # 10% of the time, replace with random word
        else:
          masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]

      output_tokens[index] = masked_token

      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))
  assert len(masked_lms) <= num_to_predict
  masked_lms = sorted(masked_lms, key=lambda x: x.index)

  masked_lm_positions = []
  masked_lm_labels = []
  for p in masked_lms:
    masked_lm_positions.append(p.index)
    masked_lm_labels.append(p.label)

  return (output_tokens, masked_lm_positions, masked_lm_labels)


def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):
  """"""Truncates a pair of sequences to a maximum sequence length.""""""
  while True:
    total_length = len(tokens_a) + len(tokens_b)
    if total_length <= max_num_tokens:
      break

    trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b
    assert len(trunc_tokens) >= 1

    # We want to sometimes truncate from the front and sometimes from the
    # back to add more randomness and avoid biases.
    if rng.random() < 0.5:
      del trunc_tokens[0]
    else:
      trunc_tokens.pop()


def main(_):
  tf.logging.set_verbosity(tf.logging.INFO)

  tokenizer = tokenization.FullTokenizer(
      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)

  input_files = []
  for input_pattern in FLAGS.input_file.split("",""):
    input_files.extend(tf.gfile.Glob(input_pattern))

  tf.logging.info(""*** Reading from input files ***"")
  for input_file in input_files:
    tf.logging.info(""  %s"", input_file)

  rng = random.Random(FLAGS.random_seed)
  instances = create_training_instances(
      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,
      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,
      rng)

  output_files = FLAGS.output_file.split("","")
  tf.logging.info(""*** Writing to output files ***"")
  for output_file in output_files:
    tf.logging.info(""  %s"", output_file)

  write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,
                                  FLAGS.max_predictions_per_seq, output_files)


if __name__ == ""__main__"":
  flags.mark_flag_as_required(""input_file"")
  flags.mark_flag_as_required(""output_file"")
  flags.mark_flag_as_required(""vocab_file"")
  tf.app.run()"
Albert,optimization_finetuning.py,"# coding=utf-8
# Copyright 2018 The Google AI Language Team Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Functions and classes related to optimization (weight updates).""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import re
import tensorflow as tf


def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):
  """"""Creates an optimizer training op.""""""
  global_step = tf.train.get_or_create_global_step()

  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)

  # Implements linear decay of the learning rate.
  learning_rate = tf.train.polynomial_decay(
      learning_rate,
      global_step,
      num_train_steps,
      end_learning_rate=0.0,
      power=1.0,
      cycle=False)

  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the
  # learning rate will be `global_step/num_warmup_steps * init_lr`.
  if num_warmup_steps:
    global_steps_int = tf.cast(global_step, tf.int32)
    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)

    global_steps_float = tf.cast(global_steps_int, tf.float32)
    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)

    warmup_percent_done = global_steps_float / warmup_steps_float
    warmup_learning_rate = init_lr * warmup_percent_done

    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)
    learning_rate = (
        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)

  # It is recommended that you use this optimizer for fine tuning, since this
  # is how the model was trained (note that the Adam m/v variables are NOT
  # loaded from init_checkpoint.)
  optimizer = AdamWeightDecayOptimizer(
      learning_rate=learning_rate,
      weight_decay_rate=0.01,
      beta_1=0.9,
      beta_2=0.999, # 0.98 ONLY USED FOR PRETRAIN. MUST CHANGE AT FINE-TUNING 0.999,
      epsilon=1e-6,
      exclude_from_weight_decay=[""LayerNorm"", ""layer_norm"", ""bias""])

  if use_tpu:
    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)

  tvars = tf.trainable_variables()
  grads = tf.gradients(loss, tvars)

  # This is how the model was pre-trained.
  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)

  train_op = optimizer.apply_gradients(
      zip(grads, tvars), global_step=global_step)

  # Normally the global step update is done inside of `apply_gradients`.
  # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use
  # a different optimizer, you should probably take this line out.
  new_global_step = global_step + 1
  train_op = tf.group(train_op, [global_step.assign(new_global_step)])
  return train_op


class AdamWeightDecayOptimizer(tf.train.Optimizer):
  """"""A basic Adam optimizer that includes ""correct"" L2 weight decay.""""""

  def __init__(self,
               learning_rate,
               weight_decay_rate=0.0,
               beta_1=0.9,
               beta_2=0.999,
               epsilon=1e-6,
               exclude_from_weight_decay=None,
               name=""AdamWeightDecayOptimizer""):
    """"""Constructs a AdamWeightDecayOptimizer.""""""
    super(AdamWeightDecayOptimizer, self).__init__(False, name)

    self.learning_rate = learning_rate
    self.weight_decay_rate = weight_decay_rate
    self.beta_1 = beta_1
    self.beta_2 = beta_2
    self.epsilon = epsilon
    self.exclude_from_weight_decay = exclude_from_weight_decay

  def apply_gradients(self, grads_and_vars, global_step=None, name=None):
    """"""See base class.""""""
    assignments = []
    for (grad, param) in grads_and_vars:
      if grad is None or param is None:
        continue

      param_name = self._get_variable_name(param.name)

      m = tf.get_variable(
          name=param_name + ""/adam_m"",
          shape=param.shape.as_list(),
          dtype=tf.float32,
          trainable=False,
          initializer=tf.zeros_initializer())
      v = tf.get_variable(
          name=param_name + ""/adam_v"",
          shape=param.shape.as_list(),
          dtype=tf.float32,
          trainable=False,
          initializer=tf.zeros_initializer())

      # Standard Adam update.
      next_m = (
          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))
      next_v = (
          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,
                                                    tf.square(grad)))

      update = next_m / (tf.sqrt(next_v) + self.epsilon)

      # Just adding the square of the weights to the loss function is *not*
      # the correct way of using L2 regularization/weight decay with Adam,
      # since that will interact with the m and v parameters in strange ways.
      #
      # Instead we want ot decay the weights in a manner that doesn't interact
      # with the m/v parameters. This is equivalent to adding the square
      # of the weights to the loss with plain (non-momentum) SGD.
      if self._do_use_weight_decay(param_name):
        update += self.weight_decay_rate * param

      update_with_lr = self.learning_rate * update

      next_param = param - update_with_lr

      assignments.extend(
          [param.assign(next_param),
           m.assign(next_m),
           v.assign(next_v)])
    return tf.group(*assignments, name=name)

  def _do_use_weight_decay(self, param_name):
    """"""Whether to use L2 weight decay for `param_name`.""""""
    if not self.weight_decay_rate:
      return False
    if self.exclude_from_weight_decay:
      for r in self.exclude_from_weight_decay:
        if re.search(r, param_name) is not None:
          return False
    return True

  def _get_variable_name(self, param_name):
    """"""Get the variable name from the tensor name.""""""
    m = re.match(""^(.*):\\d+$"", param_name)
    if m is not None:
      param_name = m.group(1)
    return param_name
"
Albert,tokenization_google.py,"# coding=utf-8
# Copyright 2019 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Lint as: python2, python3
# coding=utf-8
""""""Tokenization classes.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import re
import unicodedata
import six
from six.moves import range
import tensorflow as tf
import sentencepiece as spm

SPIECE_UNDERLINE = u""▁"".encode(""utf-8"")


def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):
  """"""Checks whether the casing config is consistent with the checkpoint name.""""""

  # The casing has to be passed in by the user and there is no explicit check
  # as to whether it matches the checkpoint. The casing information probably
  # should have been stored in the bert_config.json file, but it's not, so
  # we have to heuristically detect it to validate.

  if not init_checkpoint:
    return

  m = re.match(""^.*?([A-Za-z0-9_-]+)/bert_model.ckpt"",
               six.ensure_str(init_checkpoint))
  if m is None:
    return

  model_name = m.group(1)

  lower_models = [
      ""uncased_L-24_H-1024_A-16"", ""uncased_L-12_H-768_A-12"",
      ""multilingual_L-12_H-768_A-12"", ""chinese_L-12_H-768_A-12""
  ]

  cased_models = [
      ""cased_L-12_H-768_A-12"", ""cased_L-24_H-1024_A-16"",
      ""multi_cased_L-12_H-768_A-12""
  ]

  is_bad_config = False
  if model_name in lower_models and not do_lower_case:
    is_bad_config = True
    actual_flag = ""False""
    case_name = ""lowercased""
    opposite_flag = ""True""

  if model_name in cased_models and do_lower_case:
    is_bad_config = True
    actual_flag = ""True""
    case_name = ""cased""
    opposite_flag = ""False""

  if is_bad_config:
    raise ValueError(
        ""You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. ""
        ""However, `%s` seems to be a %s model, so you ""
        ""should pass in `--do_lower_case=%s` so that the fine-tuning matches ""
        ""how the model was pre-training. If this error is wrong, please ""
        ""just comment out this check."" % (actual_flag, init_checkpoint,
                                          model_name, case_name, opposite_flag))


def preprocess_text(inputs, remove_space=True, lower=False):
  """"""preprocess data by removing extra space and normalize data.""""""
  outputs = inputs
  if remove_space:
    outputs = "" "".join(inputs.strip().split())

  if six.PY2 and isinstance(outputs, str):
    try:
      outputs = six.ensure_text(outputs, ""utf-8"")
    except UnicodeDecodeError:
      outputs = six.ensure_text(outputs, ""latin-1"")

  outputs = unicodedata.normalize(""NFKD"", outputs)
  outputs = """".join([c for c in outputs if not unicodedata.combining(c)])
  if lower:
    outputs = outputs.lower()

  return outputs


def encode_pieces(sp_model, text, return_unicode=True, sample=False):
  """"""turn sentences into word pieces.""""""

  if six.PY2 and isinstance(text, six.text_type):
    text = six.ensure_binary(text, ""utf-8"")

  if not sample:
    pieces = sp_model.EncodeAsPieces(text)
  else:
    pieces = sp_model.SampleEncodeAsPieces(text, 64, 0.1)
  new_pieces = []
  for piece in pieces:
    piece = printable_text(piece)
    if len(piece) > 1 and piece[-1] == "","" and piece[-2].isdigit():
      cur_pieces = sp_model.EncodeAsPieces(
          six.ensure_binary(piece[:-1]).replace(SPIECE_UNDERLINE, b""""))
      if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:
        if len(cur_pieces[0]) == 1:
          cur_pieces = cur_pieces[1:]
        else:
          cur_pieces[0] = cur_pieces[0][1:]
      cur_pieces.append(piece[-1])
      new_pieces.extend(cur_pieces)
    else:
      new_pieces.append(piece)

  # note(zhiliny): convert back to unicode for py2
  if six.PY2 and return_unicode:
    ret_pieces = []
    for piece in new_pieces:
      if isinstance(piece, str):
        piece = six.ensure_text(piece, ""utf-8"")
      ret_pieces.append(piece)
    new_pieces = ret_pieces

  return new_pieces


def encode_ids(sp_model, text, sample=False):
  pieces = encode_pieces(sp_model, text, return_unicode=False, sample=sample)
  ids = [sp_model.PieceToId(piece) for piece in pieces]
  return ids


def convert_to_unicode(text):
  """"""Converts `text` to Unicode (if it's not already), assuming utf-8 input.""""""
  if six.PY3:
    if isinstance(text, str):
      return text
    elif isinstance(text, bytes):
      return six.ensure_text(text, ""utf-8"", ""ignore"")
    else:
      raise ValueError(""Unsupported string type: %s"" % (type(text)))
  elif six.PY2:
    if isinstance(text, str):
      return six.ensure_text(text, ""utf-8"", ""ignore"")
    elif isinstance(text, six.text_type):
      return text
    else:
      raise ValueError(""Unsupported string type: %s"" % (type(text)))
  else:
    raise ValueError(""Not running on Python2 or Python 3?"")


def printable_text(text):
  """"""Returns text encoded in a way suitable for print or `tf.logging`.""""""

  # These functions want `str` for both Python2 and Python3, but in one case
  # it's a Unicode string and in the other it's a byte string.
  if six.PY3:
    if isinstance(text, str):
      return text
    elif isinstance(text, bytes):
      return six.ensure_text(text, ""utf-8"", ""ignore"")
    else:
      raise ValueError(""Unsupported string type: %s"" % (type(text)))
  elif six.PY2:
    if isinstance(text, str):
      return text
    elif isinstance(text, six.text_type):
      return six.ensure_binary(text, ""utf-8"")
    else:
      raise ValueError(""Unsupported string type: %s"" % (type(text)))
  else:
    raise ValueError(""Not running on Python2 or Python 3?"")


def load_vocab(vocab_file):
  """"""Loads a vocabulary file into a dictionary.""""""
  vocab = collections.OrderedDict()
  with tf.gfile.GFile(vocab_file, ""r"") as reader:
    while True:
      token = convert_to_unicode(reader.readline())
      if not token:
        break
      token = token.strip() # previous: token.strip().split()[0]
      if token not in vocab:
        vocab[token] = len(vocab)
  return vocab


def convert_by_vocab(vocab, items):
  """"""Converts a sequence of [tokens|ids] using the vocab.""""""
  output = []
  for item in items:
    output.append(vocab[item])
  return output


def convert_tokens_to_ids(vocab, tokens):
  return convert_by_vocab(vocab, tokens)


def convert_ids_to_tokens(inv_vocab, ids):
  return convert_by_vocab(inv_vocab, ids)


def whitespace_tokenize(text):
  """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""
  text = text.strip()
  if not text:
    return []
  tokens = text.split()
  return tokens


class FullTokenizer(object):
  """"""Runs end-to-end tokenziation.""""""

  def __init__(self, vocab_file, do_lower_case=True, spm_model_file=None):
    self.vocab = None
    self.sp_model = None
    print(""spm_model_file:"",spm_model_file,"";vocab_file:"",vocab_file)
    if spm_model_file:
      print(""#Use spm_model_file"")
      self.sp_model = spm.SentencePieceProcessor()
      tf.logging.info(""loading sentence piece model"")
      self.sp_model.Load(spm_model_file)
      # Note(mingdachen): For the purpose of consisent API, we are
      # generating a vocabulary for the sentence piece tokenizer.
      self.vocab = {self.sp_model.IdToPiece(i): i for i
                    in range(self.sp_model.GetPieceSize())}
    else:
      print(""#Use vocab_file"")
      self.vocab = load_vocab(vocab_file)
      self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)
      self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)
    self.inv_vocab = {v: k for k, v in self.vocab.items()}

  def tokenize(self, text):
    if self.sp_model:
      split_tokens = encode_pieces(self.sp_model, text, return_unicode=False)
    else:
      split_tokens = []
      for token in self.basic_tokenizer.tokenize(text):
        for sub_token in self.wordpiece_tokenizer.tokenize(token):
          split_tokens.append(sub_token)

    return split_tokens

  def convert_tokens_to_ids(self, tokens):
    if self.sp_model:
      tf.logging.info(""using sentence piece tokenzier."")
      return [self.sp_model.PieceToId(
          printable_text(token)) for token in tokens]
    else:
      return convert_by_vocab(self.vocab, tokens)

  def convert_ids_to_tokens(self, ids):
    if self.sp_model:
      tf.logging.info(""using sentence piece tokenzier."")
      return [self.sp_model.IdToPiece(id_) for id_ in ids]
    else:
      return convert_by_vocab(self.inv_vocab, ids)


class BasicTokenizer(object):
  """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""

  def __init__(self, do_lower_case=True):
    """"""Constructs a BasicTokenizer.

    Args:
      do_lower_case: Whether to lower case the input.
    """"""
    self.do_lower_case = do_lower_case

  def tokenize(self, text):
    """"""Tokenizes a piece of text.""""""
    text = convert_to_unicode(text)
    text = self._clean_text(text)

    # This was added on November 1st, 2018 for the multilingual and Chinese
    # models. This is also applied to the English models now, but it doesn't
    # matter since the English models were not trained on any Chinese data
    # and generally don't have any Chinese data in them (there are Chinese
    # characters in the vocabulary because Wikipedia does have some Chinese
    # words in the English Wikipedia.).
    text = self._tokenize_chinese_chars(text)

    orig_tokens = whitespace_tokenize(text)
    split_tokens = []
    for token in orig_tokens:
      if self.do_lower_case:
        token = token.lower()
        token = self._run_strip_accents(token)
      split_tokens.extend(self._run_split_on_punc(token))

    output_tokens = whitespace_tokenize("" "".join(split_tokens))
    return output_tokens

  def _run_strip_accents(self, text):
    """"""Strips accents from a piece of text.""""""
    text = unicodedata.normalize(""NFD"", text)
    output = []
    for char in text:
      cat = unicodedata.category(char)
      if cat == ""Mn"":
        continue
      output.append(char)
    return """".join(output)

  def _run_split_on_punc(self, text):
    """"""Splits punctuation on a piece of text.""""""
    chars = list(text)
    i = 0
    start_new_word = True
    output = []
    while i < len(chars):
      char = chars[i]
      if _is_punctuation(char):
        output.append([char])
        start_new_word = True
      else:
        if start_new_word:
          output.append([])
        start_new_word = False
        output[-1].append(char)
      i += 1

    return ["""".join(x) for x in output]

  def _tokenize_chinese_chars(self, text):
    """"""Adds whitespace around any CJK character.""""""
    output = []
    for char in text:
      cp = ord(char)
      if self._is_chinese_char(cp):
        output.append("" "")
        output.append(char)
        output.append("" "")
      else:
        output.append(char)
    return """".join(output)

  def _is_chinese_char(self, cp):
    """"""Checks whether CP is the codepoint of a CJK character.""""""
    # This defines a ""chinese character"" as anything in the CJK Unicode block:
    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
    #
    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,
    # despite its name. The modern Korean Hangul alphabet is a different block,
    # as is Japanese Hiragana and Katakana. Those alphabets are used to write
    # space-separated words, so they are not treated specially and handled
    # like the all of the other languages.
    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #
        (cp >= 0x3400 and cp <= 0x4DBF) or  #
        (cp >= 0x20000 and cp <= 0x2A6DF) or  #
        (cp >= 0x2A700 and cp <= 0x2B73F) or  #
        (cp >= 0x2B740 and cp <= 0x2B81F) or  #
        (cp >= 0x2B820 and cp <= 0x2CEAF) or
        (cp >= 0xF900 and cp <= 0xFAFF) or  #
        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #
      return True

    return False

  def _clean_text(self, text):
    """"""Performs invalid character removal and whitespace cleanup on text.""""""
    output = []
    for char in text:
      cp = ord(char)
      if cp == 0 or cp == 0xfffd or _is_control(char):
        continue
      if _is_whitespace(char):
        output.append("" "")
      else:
        output.append(char)
    return """".join(output)


class WordpieceTokenizer(object):
  """"""Runs WordPiece tokenziation.""""""

  def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=200):
    self.vocab = vocab
    self.unk_token = unk_token
    self.max_input_chars_per_word = max_input_chars_per_word

  def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.

    For example:
      input = ""unaffable""
      output = [""un"", ""##aff"", ""##able""]

    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.

    Returns:
      A list of wordpiece tokens.
    """"""

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = """".join(chars[start:end])
          if start > 0:
            substr = ""##"" + six.ensure_str(substr)
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens


def _is_whitespace(char):
  """"""Checks whether `chars` is a whitespace character.""""""
  # \t, \n, and \r are technically control characters but we treat them
  # as whitespace since they are generally considered as such.
  if char == "" "" or char == ""\t"" or char == ""\n"" or char == ""\r"":
    return True
  cat = unicodedata.category(char)
  if cat == ""Zs"":
    return True
  return False


def _is_control(char):
  """"""Checks whether `chars` is a control character.""""""
  # These are technically control characters but we count them as whitespace
  # characters.
  if char == ""\t"" or char == ""\n"" or char == ""\r"":
    return False
  cat = unicodedata.category(char)
  if cat in (""Cc"", ""Cf""):
    return True
  return False


def _is_punctuation(char):
  """"""Checks whether `chars` is a punctuation character.""""""
  cp = ord(char)
  # We treat all non-letter/number ASCII as punctuation.
  # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode
  # Punctuation class but we treat them as punctuation anyways, for
  # consistency.
  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or
      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):
    return True
  cat = unicodedata.category(char)
  if cat.startswith(""P""):
    return True
  return False
"
Albert,classifier_utils.py,"# -*- coding: utf-8 -*-
# @Author: bo.shi
# @Date:   2019-12-01 22:28:41
# @Last Modified by:   bo.shi
# @Last Modified time: 2019-12-02 18:36:50
# coding=utf-8
# Copyright 2019 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""Utility functions for GLUE classification tasks.""""""

from __future__ import absolute_import
from __future__ import division

from __future__ import print_function

import json
import csv
import os
import six

import tensorflow as tf


def convert_to_unicode(text):
  """"""Converts `text` to Unicode (if it's not already), assuming utf-8 input.""""""
  if six.PY3:
    if isinstance(text, str):
      return text
    elif isinstance(text, bytes):
      return text.decode(""utf-8"", ""ignore"")
    else:
      raise ValueError(""Unsupported string type: %s"" % (type(text)))
  elif six.PY2:
    if isinstance(text, str):
      return text.decode(""utf-8"", ""ignore"")
    elif isinstance(text, unicode):
      return text
    else:
      raise ValueError(""Unsupported string type: %s"" % (type(text)))
  else:
    raise ValueError(""Not running on Python2 or Python 3?"")


class InputExample(object):
  """"""A single training/test example for simple sequence classification.""""""

  def __init__(self, guid, text_a, text_b=None, label=None):
    """"""Constructs a InputExample.
    Args:
      guid: Unique id for the example.
      text_a: string. The untokenized text of the first sequence. For single
        sequence tasks, only this sequence must be specified.
      text_b: (Optional) string. The untokenized text of the second sequence.
        Only must be specified for sequence pair tasks.
      label: (Optional) string. The label of the example. This should be
        specified for train and dev examples, but not for test examples.
    """"""
    self.guid = guid
    self.text_a = text_a
    self.text_b = text_b
    self.label = label


class PaddingInputExample(object):
  """"""Fake example so the num input examples is a multiple of the batch size.
  When running eval/predict on the TPU, we need to pad the number of examples
  to be a multiple of the batch size, because the TPU requires a fixed batch
  size. The alternative is to drop the last batch, which is bad because it means
  the entire output data won't be generated.
  We use this class instead of `None` because treating `None` as padding
  battches could cause silent errors.
  """"""


class DataProcessor(object):
  """"""Base class for data converters for sequence classification data sets.""""""

  def get_train_examples(self, data_dir):
    """"""Gets a collection of `InputExample`s for the train set.""""""
    raise NotImplementedError()

  def get_dev_examples(self, data_dir):
    """"""Gets a collection of `InputExample`s for the dev set.""""""
    raise NotImplementedError()

  def get_test_examples(self, data_dir):
    """"""Gets a collection of `InputExample`s for prediction.""""""
    raise NotImplementedError()

  def get_labels(self):
    """"""Gets the list of labels for this data set.""""""
    raise NotImplementedError()

  @classmethod
  def _read_tsv(cls, input_file, delimiter=""\t"", quotechar=None):
    """"""Reads a tab separated value file.""""""
    with tf.gfile.Open(input_file, ""r"") as f:
      reader = csv.reader(f, delimiter=delimiter, quotechar=quotechar)
      lines = []
      for line in reader:
        lines.append(line)
      return lines

  @classmethod
  def _read_txt(cls, input_file):
    """"""Reads a tab separated value file.""""""
    with tf.gfile.Open(input_file, ""r"") as f:
      reader = f.readlines()
      lines = []
      for line in reader:
        lines.append(line.strip().split(""_!_""))
      return lines

  @classmethod
  def _read_json(cls, input_file):
    """"""Reads a tab separated value file.""""""
    with tf.gfile.Open(input_file, ""r"") as f:
      reader = f.readlines()
      lines = []
      for line in reader:
        lines.append(json.loads(line.strip()))
      return lines


class XnliProcessor(DataProcessor):
  """"""Processor for the XNLI data set.""""""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""train.json"")), ""train"")

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""dev.json"")), ""dev"")

  def get_test_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""test.json"")), ""test"")

  def _create_examples(self, lines, set_type):
    """"""See base class.""""""
    examples = []
    for (i, line) in enumerate(lines):
      guid = ""%s-%s"" % (set_type, i)
      text_a = convert_to_unicode(line['premise'])
      text_b = convert_to_unicode(line['hypo'])
      label = convert_to_unicode(line['label']) if set_type != 'test' else 'contradiction'
      examples.append(
          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
    return examples

  def get_labels(self):
    """"""See base class.""""""
    return [""contradiction"", ""entailment"", ""neutral""]


# class TnewsProcessor(DataProcessor):
#     """"""Processor for the MRPC data set (GLUE version).""""""
#
#     def get_train_examples(self, data_dir):
#         """"""See base class.""""""
#         return self._create_examples(
#             self._read_txt(os.path.join(data_dir, ""toutiao_category_train.txt"")), ""train"")
#
#     def get_dev_examples(self, data_dir):
#         """"""See base class.""""""
#         return self._create_examples(
#             self._read_txt(os.path.join(data_dir, ""toutiao_category_dev.txt"")), ""dev"")
#
#     def get_test_examples(self, data_dir):
#         """"""See base class.""""""
#         return self._create_examples(
#             self._read_txt(os.path.join(data_dir, ""toutiao_category_test.txt"")), ""test"")
#
#     def get_labels(self):
#         """"""See base class.""""""
#         labels = []
#         for i in range(17):
#             if i == 5 or i == 11:
#                 continue
#             labels.append(str(100 + i))
#         return labels
#
#     def _create_examples(self, lines, set_type):
#         """"""Creates examples for the training and dev sets.""""""
#         examples = []
#         for (i, line) in enumerate(lines):
#             if i == 0:
#                 continue
#             guid = ""%s-%s"" % (set_type, i)
#             text_a = convert_to_unicode(line[3])
#             text_b = None
#             label = convert_to_unicode(line[1])
#             examples.append(
#                 InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
#         return examples


class TnewsProcessor(DataProcessor):
  """"""Processor for the MRPC data set (GLUE version).""""""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""train.json"")), ""train"")

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""dev.json"")), ""dev"")

  def get_test_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""test.json"")), ""test"")

  def get_labels(self):
    """"""See base class.""""""
    labels = []
    for i in range(17):
      if i == 5 or i == 11:
        continue
      labels.append(str(100 + i))
    return labels

  def _create_examples(self, lines, set_type):
    """"""Creates examples for the training and dev sets.""""""
    examples = []
    for (i, line) in enumerate(lines):
      guid = ""%s-%s"" % (set_type, i)
      text_a = convert_to_unicode(line['sentence'])
      text_b = None
      label = convert_to_unicode(line['label']) if set_type != 'test' else ""100""
      examples.append(
          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
    return examples


# class iFLYTEKDataProcessor(DataProcessor):
#     """"""Processor for the iFLYTEKData data set (GLUE version).""""""
#
#     def get_train_examples(self, data_dir):
#         """"""See base class.""""""
#         return self._create_examples(
#             self._read_txt(os.path.join(data_dir, ""train.txt"")), ""train"")
#
#     def get_dev_examples(self, data_dir):
#         """"""See base class.""""""
#         return self._create_examples(
#             self._read_txt(os.path.join(data_dir, ""dev.txt"")), ""dev"")
#
#     def get_test_examples(self, data_dir):
#         """"""See base class.""""""
#         return self._create_examples(
#             self._read_txt(os.path.join(data_dir, ""test.txt"")), ""test"")
#
#     def get_labels(self):
#         """"""See base class.""""""
#         labels = []
#         for i in range(119):
#             labels.append(str(i))
#         return labels
#
#     def _create_examples(self, lines, set_type):
#         """"""Creates examples for the training and dev sets.""""""
#         examples = []
#         for (i, line) in enumerate(lines):
#             if i == 0:
#                 continue
#             guid = ""%s-%s"" % (set_type, i)
#             text_a = convert_to_unicode(line[1])
#             text_b = None
#             label = convert_to_unicode(line[0])
#             examples.append(
#                 InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
#         return examples


class iFLYTEKDataProcessor(DataProcessor):
  """"""Processor for the iFLYTEKData data set (GLUE version).""""""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""train.json"")), ""train"")

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""dev.json"")), ""dev"")

  def get_test_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""test.json"")), ""test"")

  def get_labels(self):
    """"""See base class.""""""
    labels = []
    for i in range(119):
      labels.append(str(i))
    return labels

  def _create_examples(self, lines, set_type):
    """"""Creates examples for the training and dev sets.""""""
    examples = []
    for (i, line) in enumerate(lines):
      guid = ""%s-%s"" % (set_type, i)
      text_a = convert_to_unicode(line['sentence'])
      text_b = None
      label = convert_to_unicode(line['label']) if set_type != 'test' else ""0""
      examples.append(
          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
    return examples


class AFQMCProcessor(DataProcessor):
  """"""Processor for the internal data set. sentence pair classification""""""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""train.json"")), ""train"")

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""dev.json"")), ""dev"")

  def get_test_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""test.json"")), ""test"")

  def get_labels(self):
    """"""See base class.""""""
    return [""0"", ""1""]

  def _create_examples(self, lines, set_type):
    """"""Creates examples for the training and dev sets.""""""
    examples = []
    for (i, line) in enumerate(lines):
      guid = ""%s-%s"" % (set_type, i)
      text_a = convert_to_unicode(line['sentence1'])
      text_b = convert_to_unicode(line['sentence2'])
      label = convert_to_unicode(line['label']) if set_type != 'test' else '0'
      examples.append(
          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
    return examples


class CMNLIProcessor(DataProcessor):
  """"""Processor for the CMNLI data set.""""""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples_json(os.path.join(data_dir, ""train.json""), ""train"")

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples_json(os.path.join(data_dir, ""dev.json""), ""dev"")

  def get_test_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples_json(os.path.join(data_dir, ""test.json""), ""test"")

  def get_labels(self):
    """"""See base class.""""""
    return [""contradiction"", ""entailment"", ""neutral""]

  def _create_examples_json(self, file_name, set_type):
    """"""Creates examples for the training and dev sets.""""""
    examples = []
    lines = tf.gfile.Open(file_name, ""r"")
    index = 0
    for line in lines:
      line_obj = json.loads(line)
      index = index + 1
      guid = ""%s-%s"" % (set_type, index)
      text_a = convert_to_unicode(line_obj[""sentence1""])
      text_b = convert_to_unicode(line_obj[""sentence2""])
      label = convert_to_unicode(line_obj[""label""]) if set_type != 'test' else 'neutral'

      if label != ""-"":
        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))

    return examples


class CslProcessor(DataProcessor):
  """"""Processor for the CSL data set.""""""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""train.json"")), ""train"")

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""dev.json"")), ""dev"")

  def get_test_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""test.json"")), ""test"")

  def get_labels(self):
    """"""See base class.""""""
    return [""0"", ""1""]

  def _create_examples(self, lines, set_type):
    """"""Creates examples for the training and dev sets.""""""
    examples = []
    for (i, line) in enumerate(lines):
      guid = ""%s-%s"" % (set_type, i)
      text_a = convert_to_unicode("" "".join(line['keyword']))
      text_b = convert_to_unicode(line['abst'])
      label = convert_to_unicode(line['label']) if set_type != 'test' else '0'
      examples.append(
          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
    return examples


# class InewsProcessor(DataProcessor):
#   """"""Processor for the MRPC data set (GLUE version).""""""
#
#   def get_train_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_txt(os.path.join(data_dir, ""train.txt"")), ""train"")
#
#   def get_dev_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_txt(os.path.join(data_dir, ""dev.txt"")), ""dev"")
#
#   def get_test_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_txt(os.path.join(data_dir, ""test.txt"")), ""test"")
#
#   def get_labels(self):
#     """"""See base class.""""""
#     labels = [""0"", ""1"", ""2""]
#     return labels
#
#   def _create_examples(self, lines, set_type):
#     """"""Creates examples for the training and dev sets.""""""
#     examples = []
#     for (i, line) in enumerate(lines):
#       if i == 0:
#         continue
#       guid = ""%s-%s"" % (set_type, i)
#       text_a = convert_to_unicode(line[2])
#       text_b = convert_to_unicode(line[3])
#       label = convert_to_unicode(line[0]) if set_type != ""test"" else '0'
#       examples.append(
#           InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
#     return examples
#
#
# class THUCNewsProcessor(DataProcessor):
#   """"""Processor for the THUCNews data set (GLUE version).""""""
#
#   def get_train_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_txt(os.path.join(data_dir, ""train.txt"")), ""train"")
#
#   def get_dev_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_txt(os.path.join(data_dir, ""dev.txt"")), ""dev"")
#
#   def get_test_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_txt(os.path.join(data_dir, ""test.txt"")), ""test"")
#
#   def get_labels(self):
#     """"""See base class.""""""
#     labels = []
#     for i in range(14):
#       labels.append(str(i))
#     return labels
#
#   def _create_examples(self, lines, set_type):
#     """"""Creates examples for the training and dev sets.""""""
#     examples = []
#     for (i, line) in enumerate(lines):
#       if i == 0 or len(line) < 3:
#         continue
#       guid = ""%s-%s"" % (set_type, i)
#       text_a = convert_to_unicode(line[3])
#       text_b = None
#       label = convert_to_unicode(line[0])
#       examples.append(
#           InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
#     return examples
#
# class LCQMCProcessor(DataProcessor):
#   """"""Processor for the internal data set. sentence pair classification""""""
#
#   def __init__(self):
#     self.language = ""zh""
#
#   def get_train_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""train.txt"")), ""train"")
#     # dev_0827.tsv
#
#   def get_dev_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""dev.txt"")), ""dev"")
#
#   def get_test_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""test.txt"")), ""test"")
#
#   def get_labels(self):
#     """"""See base class.""""""
#     return [""0"", ""1""]
#     # return [""-1"",""0"", ""1""]
#
#   def _create_examples(self, lines, set_type):
#     """"""Creates examples for the training and dev sets.""""""
#     examples = []
#     print(""length of lines:"", len(lines))
#     for (i, line) in enumerate(lines):
#       # print('#i:',i,line)
#       if i == 0:
#         continue
#       guid = ""%s-%s"" % (set_type, i)
#       try:
#         label = convert_to_unicode(line[2])
#         text_a = convert_to_unicode(line[0])
#         text_b = convert_to_unicode(line[1])
#         examples.append(
#             InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
#       except Exception:
#         print('###error.i:', i, line)
#     return examples
#
#
# class JDCOMMENTProcessor(DataProcessor):
#   """"""Processor for the internal data set. sentence pair classification""""""
#
#   def __init__(self):
#     self.language = ""zh""
#
#   def get_train_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""jd_train.csv""), "","", ""\""""), ""train"")
#     # dev_0827.tsv
#
#   def get_dev_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""jd_dev.csv""), "","", ""\""""), ""dev"")
#
#   def get_test_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""jd_test.csv""), "","", ""\""""), ""test"")
#
#   def get_labels(self):
#     """"""See base class.""""""
#     return [""1"", ""2"", ""3"", ""4"", ""5""]
#     # return [""-1"",""0"", ""1""]
#
#   def _create_examples(self, lines, set_type):
#     """"""Creates examples for the training and dev sets.""""""
#     examples = []
#     print(""length of lines:"", len(lines))
#     for (i, line) in enumerate(lines):
#       # print('#i:',i,line)
#       if i == 0:
#         continue
#       guid = ""%s-%s"" % (set_type, i)
#       try:
#         label = convert_to_unicode(line[0])
#         text_a = convert_to_unicode(line[1])
#         text_b = convert_to_unicode(line[2])
#         examples.append(
#             InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
#       except Exception:
#         print('###error.i:', i, line)
#     return examples
#
#
# class BQProcessor(DataProcessor):
#   """"""Processor for the internal data set. sentence pair classification""""""
#
#   def __init__(self):
#     self.language = ""zh""
#
#   def get_train_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""train.txt"")), ""train"")
#     # dev_0827.tsv
#
#   def get_dev_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""dev.txt"")), ""dev"")
#
#   def get_test_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""test.txt"")), ""test"")
#
#   def get_labels(self):
#     """"""See base class.""""""
#     return [""0"", ""1""]
#     # return [""-1"",""0"", ""1""]
#
#   def _create_examples(self, lines, set_type):
#     """"""Creates examples for the training and dev sets.""""""
#     examples = []
#     print(""length of lines:"", len(lines))
#     for (i, line) in enumerate(lines):
#       # print('#i:',i,line)
#       if i == 0:
#         continue
#       guid = ""%s-%s"" % (set_type, i)
#       try:
#         label = convert_to_unicode(line[2])
#         text_a = convert_to_unicode(line[0])
#         text_b = convert_to_unicode(line[1])
#         examples.append(
#             InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
#       except Exception:
#         print('###error.i:', i, line)
#     return examples
#
#
# class MnliProcessor(DataProcessor):
#   """"""Processor for the MultiNLI data set (GLUE version).""""""
#
#   def get_train_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")
#
#   def get_dev_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""dev_matched.tsv"")),
#         ""dev_matched"")
#
#   def get_test_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""test_matched.tsv"")), ""test"")
#
#   def get_labels(self):
#     """"""See base class.""""""
#     return [""contradiction"", ""entailment"", ""neutral""]
#
#   def _create_examples(self, lines, set_type):
#     """"""Creates examples for the training and dev sets.""""""
#     examples = []
#     for (i, line) in enumerate(lines):
#       if i == 0:
#         continue
#       guid = ""%s-%s"" % (set_type, convert_to_unicode(line[0]))
#       text_a = convert_to_unicode(line[8])
#       text_b = convert_to_unicode(line[9])
#       if set_type == ""test"":
#         label = ""contradiction""
#       else:
#         label = convert_to_unicode(line[-1])
#       examples.append(
#           InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
#     return examples
#
#
# class MrpcProcessor(DataProcessor):
#   """"""Processor for the MRPC data set (GLUE version).""""""
#
#   def get_train_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")
#
#   def get_dev_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")
#
#   def get_test_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")
#
#   def get_labels(self):
#     """"""See base class.""""""
#     return [""0"", ""1""]
#
#   def _create_examples(self, lines, set_type):
#     """"""Creates examples for the training and dev sets.""""""
#     examples = []
#     for (i, line) in enumerate(lines):
#       if i == 0:
#         continue
#       guid = ""%s-%s"" % (set_type, i)
#       text_a = convert_to_unicode(line[3])
#       text_b = convert_to_unicode(line[4])
#       if set_type == ""test"":
#         label = ""0""
#       else:
#         label = convert_to_unicode(line[0])
#       examples.append(
#           InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
#     return examples
#
#
# class ColaProcessor(DataProcessor):
#   """"""Processor for the CoLA data set (GLUE version).""""""
#
#   def get_train_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")
#
#   def get_dev_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")
#
#   def get_test_examples(self, data_dir):
#     """"""See base class.""""""
#     return self._create_examples(
#         self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")
#
#   def get_labels(self):
#     """"""See base class.""""""
#     return [""0"", ""1""]
#
#   def _create_examples(self, lines, set_type):
#     """"""Creates examples for the training and dev sets.""""""
#     examples = []
#     for (i, line) in enumerate(lines):
#       # Only the test set has a header
#       if set_type == ""test"" and i == 0:
#         continue
#       guid = ""%s-%s"" % (set_type, i)
#       if set_type == ""test"":
#         text_a = convert_to_unicode(line[1])
#         label = ""0""
#       else:
#         text_a = convert_to_unicode(line[3])
#         label = convert_to_unicode(line[1])
#       examples.append(
#           InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
#     return examples

class WSCProcessor(DataProcessor):
  """"""Processor for the internal data set. sentence pair classification""""""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""train.json"")), ""train"")

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""dev.json"")), ""dev"")

  def get_test_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""test.json"")), ""test"")

  def get_labels(self):
    """"""See base class.""""""
    return [""true"", ""false""]

  def _create_examples(self, lines, set_type):
    """"""Creates examples for the training and dev sets.""""""
    examples = []
    for (i, line) in enumerate(lines):
      guid = ""%s-%s"" % (set_type, i)
      text_a = convert_to_unicode(line['text'])
      text_a_list = list(text_a)
      target = line['target']
      query = target['span1_text']
      query_idx = target['span1_index']
      pronoun = target['span2_text']
      pronoun_idx = target['span2_index']

      assert text_a[pronoun_idx: (pronoun_idx + len(pronoun))
                    ] == pronoun, ""pronoun: {}"".format(pronoun)
      assert text_a[query_idx: (query_idx + len(query))] == query, ""query: {}"".format(query)

      if pronoun_idx > query_idx:
        text_a_list.insert(query_idx, ""_"")
        text_a_list.insert(query_idx + len(query) + 1, ""_"")
        text_a_list.insert(pronoun_idx + 2, ""["")
        text_a_list.insert(pronoun_idx + len(pronoun) + 2 + 1, ""]"")
      else:
        text_a_list.insert(pronoun_idx, ""["")
        text_a_list.insert(pronoun_idx + len(pronoun) + 1, ""]"")
        text_a_list.insert(query_idx + 2, ""_"")
        text_a_list.insert(query_idx + len(query) + 2 + 1, ""_"")

      text_a = """".join(text_a_list)

      if set_type == ""test"":
        label = ""true""
      else:
        label = line['label']

      examples.append(
          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
    return examples


class COPAProcessor(DataProcessor):
  """"""Processor for the internal data set. sentence pair classification""""""

  def __init__(self):
    self.language = ""zh""

  def get_train_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""train.json"")), ""train"")
    # dev_0827.tsv

  def get_dev_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""dev.json"")), ""dev"")

  def get_test_examples(self, data_dir):
    """"""See base class.""""""
    return self._create_examples(
        self._read_json(os.path.join(data_dir, ""test.json"")), ""test"")

  def get_labels(self):
    """"""See base class.""""""
    return [""0"", ""1""]

  @classmethod
  def _create_examples_one(self, lines, set_type):
    examples = []
    for (i, line) in enumerate(lines):
      guid1 = ""%s-%s"" % (set_type, i)
#         try:
      if line['question'] == 'cause':
        text_a = convert_to_unicode(line['premise'] + '原因是什么呢？' + line['choice0'])
        text_b = convert_to_unicode(line['premise'] + '原因是什么呢？' + line['choice1'])
      else:
        text_a = convert_to_unicode(line['premise'] + '造成了什么影响呢？' + line['choice0'])
        text_b = convert_to_unicode(line['premise'] + '造成了什么影响呢？' + line['choice1'])
      label = convert_to_unicode(str(1 if line['label'] == 0 else 0)) if set_type != 'test' else '0'
      examples.append(
          InputExample(guid=guid1, text_a=text_a, text_b=text_b, label=label))
#         except Exception as e:
#             print('###error.i:',e, i, line)
    return examples

  @classmethod
  def _create_examples(self, lines, set_type):
    examples = []
    for (i, line) in enumerate(lines):
      i = 2 * i
      guid1 = ""%s-%s"" % (set_type, i)
      guid2 = ""%s-%s"" % (set_type, i + 1)
#         try:
      premise = convert_to_unicode(line['premise'])
      choice0 = convert_to_unicode(line['choice0'])
      label = convert_to_unicode(str(1 if line['label'] == 0 else 0)) if set_type != 'test' else '0'
      #text_a2 = convert_to_unicode(line['premise'])
      choice1 = convert_to_unicode(line['choice1'])
      label2 = convert_to_unicode(
          str(0 if line['label'] == 0 else 1)) if set_type != 'test' else '0'
      if line['question'] == 'effect':
        text_a = premise
        text_b = choice0
        text_a2 = premise
        text_b2 = choice1
      elif line['question'] == 'cause':
        text_a = choice0
        text_b = premise
        text_a2 = choice1
        text_b2 = premise
      else:
        print('wrong format!!')
        return None
      examples.append(
          InputExample(guid=guid1, text_a=text_a, text_b=text_b, label=label))
      examples.append(
          InputExample(guid=guid2, text_a=text_a2, text_b=text_b2, label=label2))
#         except Exception as e:
#             print('###error.i:',e, i, line)
    return examples"
Albert,args.py,"import os
import tensorflow as tf

tf.logging.set_verbosity(tf.logging.INFO)

file_path = os.path.dirname(__file__)


#模型目录
model_dir = os.path.join(file_path, 'albert_lcqmc_checkpoints/')

#config文件
config_name = os.path.join(file_path, 'albert_config/albert_config_tiny.json')
#ckpt文件名称
ckpt_name = os.path.join(model_dir, 'model.ckpt')
#输出文件目录
output_dir = os.path.join(file_path, 'albert_lcqmc_checkpoints/')
#vocab文件目录
vocab_file = os.path.join(file_path, 'albert_config/vocab.txt')
#数据目录
data_dir = os.path.join(file_path, 'data/')

num_train_epochs = 10
batch_size = 128
learning_rate = 0.00005

# gpu使用率
gpu_memory_fraction = 0.8

# 默认取倒数第二层的输出值作为句向量
layer_indexes = [-2]

# 序列的最大程度，单文本建议把该值调小
max_seq_len = 128

# graph名字
graph_file = os.path.join(file_path, 'albert_lcqmc_checkpoints/graph')"
Albert,modeling.py,"# coding=utf-8
# Copyright 2018 The Google AI Language Team Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""The main BERT model and related functions.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import copy
import json
import math
import re
import numpy as np
import six
import tensorflow as tf
import bert_utils

class BertConfig(object):
  """"""Configuration for `BertModel`.""""""

  def __init__(self,
               vocab_size,
               hidden_size=768,
               num_hidden_layers=12,
               num_attention_heads=12,
               intermediate_size=3072,
               hidden_act=""gelu"",
               hidden_dropout_prob=0.1,
               attention_probs_dropout_prob=0.1,
               max_position_embeddings=512,
               type_vocab_size=16,
               initializer_range=0.02):
    """"""Constructs BertConfig.

    Args:
      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.
      hidden_size: Size of the encoder layers and the pooler layer.
      num_hidden_layers: Number of hidden layers in the Transformer encoder.
      num_attention_heads: Number of attention heads for each attention layer in
        the Transformer encoder.
      intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)
        layer in the Transformer encoder.
      hidden_act: The non-linear activation function (function or string) in the
        encoder and pooler.
      hidden_dropout_prob: The dropout probability for all fully connected
        layers in the embeddings, encoder, and pooler.
      attention_probs_dropout_prob: The dropout ratio for the attention
        probabilities.
      max_position_embeddings: The maximum sequence length that this model might
        ever be used with. Typically set this to something large just in case
        (e.g., 512 or 1024 or 2048).
      type_vocab_size: The vocabulary size of the `token_type_ids` passed into
        `BertModel`.
      initializer_range: The stdev of the truncated_normal_initializer for
        initializing all weight matrices.
    """"""
    self.vocab_size = vocab_size
    self.hidden_size = hidden_size
    self.num_hidden_layers = num_hidden_layers
    self.num_attention_heads = num_attention_heads
    self.hidden_act = hidden_act
    self.intermediate_size = intermediate_size
    self.hidden_dropout_prob = hidden_dropout_prob
    self.attention_probs_dropout_prob = attention_probs_dropout_prob
    self.max_position_embeddings = max_position_embeddings
    self.type_vocab_size = type_vocab_size
    self.initializer_range = initializer_range

  @classmethod
  def from_dict(cls, json_object):
    """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""
    config = BertConfig(vocab_size=None)
    for (key, value) in six.iteritems(json_object):
      config.__dict__[key] = value
    return config

  @classmethod
  def from_json_file(cls, json_file):
    """"""Constructs a `BertConfig` from a json file of parameters.""""""
    with tf.gfile.GFile(json_file, ""r"") as reader:
      text = reader.read()
    return cls.from_dict(json.loads(text))

  def to_dict(self):
    """"""Serializes this instance to a Python dictionary.""""""
    output = copy.deepcopy(self.__dict__)
    return output

  def to_json_string(self):
    """"""Serializes this instance to a JSON string.""""""
    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\n""


class BertModel(object):
  """"""BERT model (""Bidirectional Encoder Representations from Transformers"").

  Example usage:

  ```python
  # Already been converted into WordPiece token ids
  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])
  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])
  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])

  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,
    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)

  model = modeling.BertModel(config=config, is_training=True,
    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)

  label_embeddings = tf.get_variable(...)
  pooled_output = model.get_pooled_output()
  logits = tf.matmul(pooled_output, label_embeddings)
  ...
  ```
  """"""

  def __init__(self,
               config,
               is_training,
               input_ids,
               input_mask=None,
               token_type_ids=None,
               use_one_hot_embeddings=False,
               scope=None):
    """"""Constructor for BertModel.

    Args:
      config: `BertConfig` instance.
      is_training: bool. true for training model, false for eval model. Controls
        whether dropout will be applied.
      input_ids: int32 Tensor of shape [batch_size, seq_length].
      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].
      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].
      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word
        embeddings or tf.embedding_lookup() for the word embeddings.
      scope: (optional) variable scope. Defaults to ""bert"".

    Raises:
      ValueError: The config is invalid or one of the input tensor shapes
        is invalid.
    """"""
    config = copy.deepcopy(config)
    if not is_training:
      config.hidden_dropout_prob = 0.0
      config.attention_probs_dropout_prob = 0.0

    input_shape = get_shape_list(input_ids, expected_rank=2)
    batch_size = input_shape[0]
    seq_length = input_shape[1]

    if input_mask is None:
      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)

    if token_type_ids is None:
      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)

    with tf.variable_scope(scope, default_name=""bert""):
      with tf.variable_scope(""embeddings""):
        # Perform embedding lookup on the word ids, but use stype of factorized embedding parameterization from albert. add by brightmart, 2019-09-28
        (self.embedding_output, self.embedding_table,self.embedding_table_2) = embedding_lookup_factorized(
            input_ids=input_ids,
            vocab_size=config.vocab_size,
            hidden_size=config.hidden_size,
            embedding_size=config.embedding_size,
            initializer_range=config.initializer_range,
            word_embedding_name=""word_embeddings"",
            use_one_hot_embeddings=use_one_hot_embeddings)

        # Add positional embeddings and token type embeddings, then layer
        # normalize and perform dropout.
        self.embedding_output = embedding_postprocessor(
            input_tensor=self.embedding_output,
            use_token_type=True,
            token_type_ids=token_type_ids,
            token_type_vocab_size=config.type_vocab_size,
            token_type_embedding_name=""token_type_embeddings"",
            use_position_embeddings=True,
            position_embedding_name=""position_embeddings"",
            initializer_range=config.initializer_range,
            max_position_embeddings=config.max_position_embeddings,
            dropout_prob=config.hidden_dropout_prob)

      with tf.variable_scope(""encoder""):
        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D
        # mask of shape [batch_size, seq_length, seq_length] which is used
        # for the attention scores.
        attention_mask = create_attention_mask_from_input_mask(
            input_ids, input_mask)

        # Run the stacked transformer.
        # `sequence_output` shape = [batch_size, seq_length, hidden_size].
        ln_type=config.ln_type
        print(""ln_type:"",ln_type)
        if ln_type=='postln' or ln_type is None: # currently, base or large of albert used post-LN structure
            print(""old structure of transformer.use: transformer_model,which use post-LN"")
            self.all_encoder_layers = transformer_model(
                input_tensor=self.embedding_output,
                attention_mask=attention_mask,
                hidden_size=config.hidden_size,
                num_hidden_layers=config.num_hidden_layers,
                num_attention_heads=config.num_attention_heads,
                intermediate_size=config.intermediate_size,
                intermediate_act_fn=get_activation(config.hidden_act),
                hidden_dropout_prob=config.hidden_dropout_prob,
                attention_probs_dropout_prob=config.attention_probs_dropout_prob,
                initializer_range=config.initializer_range,
                do_return_all_layers=True)
        else: # xlarge or xxlarge of albert, used pre-LN structure
            print(""new structure of transformer.use: prelln_transformer_model,which use pre-LN"")
            self.all_encoder_layers = prelln_transformer_model( # change by brightmart, 4th, oct, 2019. pre-Layer Normalization can converge fast and better. check paper: ON LAYER NORMALIZATION IN THE TRANSFORMER ARCHITECTURE
                input_tensor=self.embedding_output,
                attention_mask=attention_mask,
                hidden_size=config.hidden_size,
                num_hidden_layers=config.num_hidden_layers,
                num_attention_heads=config.num_attention_heads,
                intermediate_size=config.intermediate_size,
                intermediate_act_fn=get_activation(config.hidden_act),
                hidden_dropout_prob=config.hidden_dropout_prob,
                attention_probs_dropout_prob=config.attention_probs_dropout_prob,
                initializer_range=config.initializer_range,
                do_return_all_layers=True,
                shared_type='all') #  do_return_all_layers=True

      self.sequence_output = self.all_encoder_layers[-1] # [batch_size, seq_length, hidden_size]
      # The ""pooler"" converts the encoded sequence tensor of shape
      # [batch_size, seq_length, hidden_size] to a tensor of shape
      # [batch_size, hidden_size]. This is necessary for segment-level
      # (or segment-pair-level) classification tasks where we need a fixed
      # dimensional representation of the segment.
      with tf.variable_scope(""pooler""):
        # We ""pool"" the model by simply taking the hidden state corresponding
        # to the first token. We assume that this has been pre-trained
        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)
        self.pooled_output = tf.layers.dense(
            first_token_tensor,
            config.hidden_size,
            activation=tf.tanh,
            kernel_initializer=create_initializer(config.initializer_range))

  def get_pooled_output(self):
    return self.pooled_output

  def get_sequence_output(self):
    """"""Gets final hidden layer of encoder.

    Returns:
      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
      to the final hidden of the transformer encoder.
    """"""
    return self.sequence_output

  def get_all_encoder_layers(self):
    return self.all_encoder_layers

  def get_embedding_output(self):
    """"""Gets output of the embedding lookup (i.e., input to the transformer).

    Returns:
      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding
      to the output of the embedding layer, after summing the word
      embeddings with the positional embeddings and the token type embeddings,
      then performing layer normalization. This is the input to the transformer.
    """"""
    return self.embedding_output

  def get_embedding_table(self):
    return self.embedding_table

  def get_embedding_table_2(self):
    return self.embedding_table_2

def gelu(x):
  """"""Gaussian Error Linear Unit.

  This is a smoother version of the RELU.
  Original paper: https://arxiv.org/abs/1606.08415
  Args:
    x: float Tensor to perform activation.

  Returns:
    `x` with the GELU activation applied.
  """"""
  cdf = 0.5 * (1.0 + tf.tanh(
      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))
  return x * cdf


def get_activation(activation_string):
  """"""Maps a string to a Python function, e.g., ""relu"" => `tf.nn.relu`.

  Args:
    activation_string: String name of the activation function.

  Returns:
    A Python function corresponding to the activation function. If
    `activation_string` is None, empty, or ""linear"", this will return None.
    If `activation_string` is not a string, it will return `activation_string`.

  Raises:
    ValueError: The `activation_string` does not correspond to a known
      activation.
  """"""

  # We assume that anything that""s not a string is already an activation
  # function, so we just return it.
  if not isinstance(activation_string, six.string_types):
    return activation_string

  if not activation_string:
    return None

  act = activation_string.lower()
  if act == ""linear"":
    return None
  elif act == ""relu"":
    return tf.nn.relu
  elif act == ""gelu"":
    return gelu
  elif act == ""tanh"":
    return tf.tanh
  else:
    raise ValueError(""Unsupported activation: %s"" % act)


def get_assignment_map_from_checkpoint(tvars, init_checkpoint):
  """"""Compute the union of the current variables and checkpoint variables.""""""
  assignment_map = {}
  initialized_variable_names = {}

  name_to_variable = collections.OrderedDict()
  for var in tvars:
    name = var.name
    m = re.match(""^(.*):\\d+$"", name)
    if m is not None:
      name = m.group(1)
    name_to_variable[name] = var

  init_vars = tf.train.list_variables(init_checkpoint)

  assignment_map = collections.OrderedDict()
  for x in init_vars:
    (name, var) = (x[0], x[1])
    if name not in name_to_variable:
      continue
    assignment_map[name] = name
    initialized_variable_names[name] = 1
    initialized_variable_names[name + "":0""] = 1

  return (assignment_map, initialized_variable_names)


def dropout(input_tensor, dropout_prob):
  """"""Perform dropout.

  Args:
    input_tensor: float Tensor.
    dropout_prob: Python float. The probability of dropping out a value (NOT of
      *keeping* a dimension as in `tf.nn.dropout`).

  Returns:
    A version of `input_tensor` with dropout applied.
  """"""
  if dropout_prob is None or dropout_prob == 0.0:
    return input_tensor

  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)
  return output


def layer_norm(input_tensor, name=None):
  """"""Run layer normalization on the last dimension of the tensor.""""""
  return tf.contrib.layers.layer_norm(
      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)


def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):
  """"""Runs layer normalization followed by dropout.""""""
  output_tensor = layer_norm(input_tensor, name)
  output_tensor = dropout(output_tensor, dropout_prob)
  return output_tensor


def create_initializer(initializer_range=0.02):
  """"""Creates a `truncated_normal_initializer` with the given range.""""""
  return tf.truncated_normal_initializer(stddev=initializer_range)


def embedding_lookup(input_ids,
                     vocab_size,
                     embedding_size=128,
                     initializer_range=0.02,
                     word_embedding_name=""word_embeddings"",
                     use_one_hot_embeddings=False):
  """"""Looks up words embeddings for id tensor.

  Args:
    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word
      ids.
    vocab_size: int. Size of the embedding vocabulary.
    embedding_size: int. Width of the word embeddings.
    initializer_range: float. Embedding initialization range.
    word_embedding_name: string. Name of the embedding table.
    use_one_hot_embeddings: bool. If True, use one-hot method for word
      embeddings. If False, use `tf.gather()`.

  Returns:
    float Tensor of shape [batch_size, seq_length, embedding_size].
  """"""
  # This function assumes that the input is of shape [batch_size, seq_length,
  # num_inputs].
  #
  # If the input is a 2D tensor of shape [batch_size, seq_length], we
  # reshape to [batch_size, seq_length, 1].
  if input_ids.shape.ndims == 2:
    input_ids = tf.expand_dims(input_ids, axis=[-1]) # shape of input_ids is:[ batch_size, seq_length, 1]

  embedding_table = tf.get_variable( # [vocab_size, embedding_size]
      name=word_embedding_name,
      shape=[vocab_size, embedding_size],
      initializer=create_initializer(initializer_range))

  flat_input_ids = tf.reshape(input_ids, [-1]) # one rank. shape as (batch_size * sequence_length,)
  if use_one_hot_embeddings:
    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size) # one_hot_input_ids=[batch_size * sequence_length,vocab_size]
    output = tf.matmul(one_hot_input_ids, embedding_table) # output=[batch_size * sequence_length,embedding_size]
  else:
    output = tf.gather(embedding_table, flat_input_ids) # [vocab_size, embedding_size]*[batch_size * sequence_length,]--->[batch_size * sequence_length,embedding_size]

  input_shape = get_shape_list(input_ids) # input_shape=[ batch_size, seq_length, 1]

  output = tf.reshape(output,input_shape[0:-1] + [input_shape[-1] * embedding_size]) # output=[batch_size,sequence_length,embedding_size]
  return (output, embedding_table)

def embedding_lookup_factorized(input_ids, # Factorized embedding parameterization provide by albert
                     vocab_size,
                     hidden_size,
                     embedding_size=128,
                     initializer_range=0.02,
                     word_embedding_name=""word_embeddings"",
                     use_one_hot_embeddings=False):
    """"""Looks up words embeddings for id tensor, but in a factorized style followed by albert. it is used to reduce much percentage of parameters previous exists.
       Check ""Factorized embedding parameterization"" session in the paper.

     Args:
       input_ids: int32 Tensor of shape [batch_size, seq_length] containing word
         ids.
       vocab_size: int. Size of the embedding vocabulary.
       embedding_size: int. Width of the word embeddings.
       initializer_range: float. Embedding initialization range.
       word_embedding_name: string. Name of the embedding table.
       use_one_hot_embeddings: bool. If True, use one-hot method for word
         embeddings. If False, use `tf.gather()`.

     Returns:
       float Tensor of shape [batch_size, seq_length, embedding_size].
     """"""
    # This function assumes that the input is of shape [batch_size, seq_length,
    # num_inputs].
    #
    # If the input is a 2D tensor of shape [batch_size, seq_length], we
    # reshape to [batch_size, seq_length, 1].

    # 1.first project one-hot vectors into a lower dimensional embedding space of size E
    print(""embedding_lookup_factorized. factorized embedding parameterization is used."")
    if input_ids.shape.ndims == 2:
        input_ids = tf.expand_dims(input_ids, axis=[-1])  # shape of input_ids is:[ batch_size, seq_length, 1]

    embedding_table = tf.get_variable(  # [vocab_size, embedding_size]
        name=word_embedding_name,
        shape=[vocab_size, embedding_size],
        initializer=create_initializer(initializer_range))

    flat_input_ids = tf.reshape(input_ids, [-1])  # one rank. shape as (batch_size * sequence_length,)
    if use_one_hot_embeddings:
        one_hot_input_ids = tf.one_hot(flat_input_ids,depth=vocab_size)  # one_hot_input_ids=[batch_size * sequence_length,vocab_size]
        output_middle = tf.matmul(one_hot_input_ids, embedding_table)  # output=[batch_size * sequence_length,embedding_size]
    else:
        output_middle = tf.gather(embedding_table,flat_input_ids)  # [vocab_size, embedding_size]*[batch_size * sequence_length,]--->[batch_size * sequence_length,embedding_size]

    # 2. project vector(output_middle) to the hidden space
    project_variable = tf.get_variable(  # [embedding_size, hidden_size]
        name=word_embedding_name+""_2"",
        shape=[embedding_size, hidden_size],
        initializer=create_initializer(initializer_range))
    output = tf.matmul(output_middle, project_variable) # ([batch_size * sequence_length, embedding_size] * [embedding_size, hidden_size])--->[batch_size * sequence_length, hidden_size]
    # reshape back to 3 rank
    input_shape = get_shape_list(input_ids)  # input_shape=[ batch_size, seq_length, 1]
    batch_size, sequene_length, _=input_shape
    output = tf.reshape(output, (batch_size,sequene_length,hidden_size))  # output=[batch_size, sequence_length, hidden_size]
    return (output, embedding_table, project_variable)


def embedding_postprocessor(input_tensor,
                            use_token_type=False,
                            token_type_ids=None,
                            token_type_vocab_size=16,
                            token_type_embedding_name=""token_type_embeddings"",
                            use_position_embeddings=True,
                            position_embedding_name=""position_embeddings"",
                            initializer_range=0.02,
                            max_position_embeddings=512,
                            dropout_prob=0.1):
  """"""Performs various post-processing on a word embedding tensor.

  Args:
    input_tensor: float Tensor of shape [batch_size, seq_length,
      embedding_size].
    use_token_type: bool. Whether to add embeddings for `token_type_ids`.
    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].
      Must be specified if `use_token_type` is True.
    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.
    token_type_embedding_name: string. The name of the embedding table variable
      for token type ids.
    use_position_embeddings: bool. Whether to add position embeddings for the
      position of each token in the sequence.
    position_embedding_name: string. The name of the embedding table variable
      for positional embeddings.
    initializer_range: float. Range of the weight initialization.
    max_position_embeddings: int. Maximum sequence length that might ever be
      used with this model. This can be longer than the sequence length of
      input_tensor, but cannot be shorter.
    dropout_prob: float. Dropout probability applied to the final output tensor.

  Returns:
    float tensor with same shape as `input_tensor`.

  Raises:
    ValueError: One of the tensor shapes or input values is invalid.
  """"""
  input_shape = get_shape_list(input_tensor, expected_rank=3)
  batch_size = input_shape[0]
  seq_length = input_shape[1]
  width = input_shape[2]

  output = input_tensor

  if use_token_type:
    if token_type_ids is None:
      raise ValueError(""`token_type_ids` must be specified if""
                       ""`use_token_type` is True."")
    token_type_table = tf.get_variable(
        name=token_type_embedding_name,
        shape=[token_type_vocab_size, width],
        initializer=create_initializer(initializer_range))
    # This vocab will be small so we always do one-hot here, since it is always
    # faster for a small vocabulary.
    flat_token_type_ids = tf.reshape(token_type_ids, [-1])
    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)
    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)
    token_type_embeddings = tf.reshape(token_type_embeddings,
                                       [batch_size, seq_length, width])
    output += token_type_embeddings

  if use_position_embeddings:
    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)
    with tf.control_dependencies([assert_op]):
      full_position_embeddings = tf.get_variable(
          name=position_embedding_name,
          shape=[max_position_embeddings, width],
          initializer=create_initializer(initializer_range))
      # Since the position embedding table is a learned variable, we create it
      # using a (long) sequence length `max_position_embeddings`. The actual
      # sequence length might be shorter than this, for faster training of
      # tasks that do not have long sequences.
      #
      # So `full_position_embeddings` is effectively an embedding table
      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current
      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just
      # perform a slice.
      position_embeddings = tf.slice(full_position_embeddings, [0, 0],
                                     [seq_length, -1])
      num_dims = len(output.shape.as_list())

      # Only the last two dimensions are relevant (`seq_length` and `width`), so
      # we broadcast among the first dimensions, which is typically just
      # the batch size.
      position_broadcast_shape = []
      for _ in range(num_dims - 2):
        position_broadcast_shape.append(1)
      position_broadcast_shape.extend([seq_length, width])
      position_embeddings = tf.reshape(position_embeddings,
                                       position_broadcast_shape)
      output += position_embeddings

  output = layer_norm_and_dropout(output, dropout_prob)
  return output


def create_attention_mask_from_input_mask(from_tensor, to_mask):
  """"""Create 3D attention mask from a 2D tensor mask.

  Args:
    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].
    to_mask: int32 Tensor of shape [batch_size, to_seq_length].

  Returns:
    float Tensor of shape [batch_size, from_seq_length, to_seq_length].
  """"""
  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])
  batch_size = from_shape[0]
  from_seq_length = from_shape[1]

  to_shape = get_shape_list(to_mask, expected_rank=2)
  to_seq_length = to_shape[1]

  to_mask = tf.cast(
      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)

  # We don't assume that `from_tensor` is a mask (although it could be). We
  # don't actually care if we attend *from* padding tokens (only *to* padding)
  # tokens so we create a tensor of all ones.
  #
  # `broadcast_ones` = [batch_size, from_seq_length, 1]
  broadcast_ones = tf.ones(
      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)

  # Here we broadcast along two dimensions to create the mask.
  mask = broadcast_ones * to_mask

  return mask


def attention_layer(from_tensor,
                    to_tensor,
                    attention_mask=None,
                    num_attention_heads=1,
                    size_per_head=512,
                    query_act=None,
                    key_act=None,
                    value_act=None,
                    attention_probs_dropout_prob=0.0,
                    initializer_range=0.02,
                    do_return_2d_tensor=False,
                    batch_size=None,
                    from_seq_length=None,
                    to_seq_length=None):
  """"""Performs multi-headed attention from `from_tensor` to `to_tensor`.

  This is an implementation of multi-headed attention based on ""Attention
  is all you Need"". If `from_tensor` and `to_tensor` are the same, then
  this is self-attention. Each timestep in `from_tensor` attends to the
  corresponding sequence in `to_tensor`, and returns a fixed-with vector.

  This function first projects `from_tensor` into a ""query"" tensor and
  `to_tensor` into ""key"" and ""value"" tensors. These are (effectively) a list
  of tensors of length `num_attention_heads`, where each tensor is of shape
  [batch_size, seq_length, size_per_head].

  Then, the query and key tensors are dot-producted and scaled. These are
  softmaxed to obtain attention probabilities. The value tensors are then
  interpolated by these probabilities, then concatenated back to a single
  tensor and returned.

  In practice, the multi-headed attention are done with transposes and
  reshapes rather than actual separate tensors.

  Args:
    from_tensor: float Tensor of shape [batch_size, from_seq_length,
      from_width].
    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].
    attention_mask: (optional) int32 Tensor of shape [batch_size,
      from_seq_length, to_seq_length]. The values should be 1 or 0. The
      attention scores will effectively be set to -infinity for any positions in
      the mask that are 0, and will be unchanged for positions that are 1.
    num_attention_heads: int. Number of attention heads.
    size_per_head: int. Size of each attention head.
    query_act: (optional) Activation function for the query transform.
    key_act: (optional) Activation function for the key transform.
    value_act: (optional) Activation function for the value transform.
    attention_probs_dropout_prob: (optional) float. Dropout probability of the
      attention probabilities.
    initializer_range: float. Range of the weight initializer.
    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size
      * from_seq_length, num_attention_heads * size_per_head]. If False, the
      output will be of shape [batch_size, from_seq_length, num_attention_heads
      * size_per_head].
    batch_size: (Optional) int. If the input is 2D, this might be the batch size
      of the 3D version of the `from_tensor` and `to_tensor`.
    from_seq_length: (Optional) If the input is 2D, this might be the seq length
      of the 3D version of the `from_tensor`.
    to_seq_length: (Optional) If the input is 2D, this might be the seq length
      of the 3D version of the `to_tensor`.

  Returns:
    float Tensor of shape [batch_size, from_seq_length,
      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is
      true, this will be of shape [batch_size * from_seq_length,
      num_attention_heads * size_per_head]).

  Raises:
    ValueError: Any of the arguments or tensor shapes are invalid.
  """"""

  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,
                           seq_length, width):
    output_tensor = tf.reshape(
        input_tensor, [batch_size, seq_length, num_attention_heads, width])

    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])
    return output_tensor

  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])
  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])

  if len(from_shape) != len(to_shape):
    raise ValueError(
        ""The rank of `from_tensor` must match the rank of `to_tensor`."")

  if len(from_shape) == 3:
    batch_size = from_shape[0]
    from_seq_length = from_shape[1]
    to_seq_length = to_shape[1]
  elif len(from_shape) == 2:
    if (batch_size is None or from_seq_length is None or to_seq_length is None):
      raise ValueError(
          ""When passing in rank 2 tensors to attention_layer, the values ""
          ""for `batch_size`, `from_seq_length`, and `to_seq_length` ""
          ""must all be specified."")

  # Scalar dimensions referenced here:
  #   B = batch size (number of sequences)
  #   F = `from_tensor` sequence length
  #   T = `to_tensor` sequence length
  #   N = `num_attention_heads`
  #   H = `size_per_head`

  from_tensor_2d = reshape_to_matrix(from_tensor)
  to_tensor_2d = reshape_to_matrix(to_tensor)

  # `query_layer` = [B*F, N*H]
  query_layer = tf.layers.dense(
      from_tensor_2d,
      num_attention_heads * size_per_head,
      activation=query_act,
      name=""query"",
      kernel_initializer=create_initializer(initializer_range))

  # `key_layer` = [B*T, N*H]
  key_layer = tf.layers.dense(
      to_tensor_2d,
      num_attention_heads * size_per_head,
      activation=key_act,
      name=""key"",
      kernel_initializer=create_initializer(initializer_range))

  # `value_layer` = [B*T, N*H]
  value_layer = tf.layers.dense(
      to_tensor_2d,
      num_attention_heads * size_per_head,
      activation=value_act,
      name=""value"",
      kernel_initializer=create_initializer(initializer_range))

  # `query_layer` = [B, N, F, H]
  query_layer = transpose_for_scores(query_layer, batch_size,
                                     num_attention_heads, from_seq_length,
                                     size_per_head)

  # `key_layer` = [B, N, T, H]
  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,
                                   to_seq_length, size_per_head)

  # Take the dot product between ""query"" and ""key"" to get the raw
  # attention scores.
  # `attention_scores` = [B, N, F, T]
  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)
  attention_scores = tf.multiply(attention_scores,
                                 1.0 / math.sqrt(float(size_per_head)))

  if attention_mask is not None:
    # `attention_mask` = [B, 1, F, T]
    attention_mask = tf.expand_dims(attention_mask, axis=[1])

    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
    # masked positions, this operation will create a tensor which is 0.0 for
    # positions we want to attend and -10000.0 for masked positions.
    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0

    # Since we are adding it to the raw scores before the softmax, this is
    # effectively the same as removing these entirely.
    attention_scores += adder

  # Normalize the attention scores to probabilities.
  # `attention_probs` = [B, N, F, T]
  attention_probs = tf.nn.softmax(attention_scores)

  # This is actually dropping out entire tokens to attend to, which might
  # seem a bit unusual, but is taken from the original Transformer paper.
  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)

  # `value_layer` = [B, T, N, H]
  value_layer = tf.reshape(
      value_layer,
      [batch_size, to_seq_length, num_attention_heads, size_per_head])

  # `value_layer` = [B, N, T, H]
  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])

  # `context_layer` = [B, N, F, H]
  context_layer = tf.matmul(attention_probs, value_layer)

  # `context_layer` = [B, F, N, H]
  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])

  if do_return_2d_tensor:
    # `context_layer` = [B*F, N*H]
    context_layer = tf.reshape(
        context_layer,
        [batch_size * from_seq_length, num_attention_heads * size_per_head])
  else:
    # `context_layer` = [B, F, N*H]
    context_layer = tf.reshape(
        context_layer,
        [batch_size, from_seq_length, num_attention_heads * size_per_head])

  return context_layer


def transformer_model(input_tensor,
                      attention_mask=None,
                      hidden_size=768,
                      num_hidden_layers=12,
                      num_attention_heads=12,
                      intermediate_size=3072,
                      intermediate_act_fn=gelu,
                      hidden_dropout_prob=0.1,
                      attention_probs_dropout_prob=0.1,
                      initializer_range=0.02,
                      do_return_all_layers=False,
                      share_parameter_across_layers=True):
  """"""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".

  This is almost an exact implementation of the original Transformer encoder.

  See the original paper:
  https://arxiv.org/abs/1706.03762

  Also see:
  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py

  Args:
    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].
    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,
      seq_length], with 1 for positions that can be attended to and 0 in
      positions that should not be.
    hidden_size: int. Hidden size of the Transformer.
    num_hidden_layers: int. Number of layers (blocks) in the Transformer.
    num_attention_heads: int. Number of attention heads in the Transformer.
    intermediate_size: int. The size of the ""intermediate"" (a.k.a., feed
      forward) layer.
    intermediate_act_fn: function. The non-linear activation function to apply
      to the output of the intermediate/feed-forward layer.
    hidden_dropout_prob: float. Dropout probability for the hidden layers.
    attention_probs_dropout_prob: float. Dropout probability of the attention
      probabilities.
    initializer_range: float. Range of the initializer (stddev of truncated
      normal).
    do_return_all_layers: Whether to also return all layers or just the final
      layer.

  Returns:
    float Tensor of shape [batch_size, seq_length, hidden_size], the final
    hidden layer of the Transformer.

  Raises:
    ValueError: A Tensor shape or parameter is invalid.
  """"""
  if hidden_size % num_attention_heads != 0:
    raise ValueError(
        ""The hidden size (%d) is not a multiple of the number of attention ""
        ""heads (%d)"" % (hidden_size, num_attention_heads))

  attention_head_size = int(hidden_size / num_attention_heads)
  input_shape = get_shape_list(input_tensor, expected_rank=3)
  batch_size = input_shape[0]
  seq_length = input_shape[1]
  input_width = input_shape[2]

  # The Transformer performs sum residuals on all layers so the input needs
  # to be the same as the hidden size.
  if input_width != hidden_size:
    raise ValueError(""The width of the input tensor (%d) != hidden size (%d)"" %
                     (input_width, hidden_size))

  # We keep the representation as a 2D tensor to avoid re-shaping it back and
  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on
  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to
  # help the optimizer.
  prev_output = reshape_to_matrix(input_tensor)

  all_layer_outputs = []
  for layer_idx in range(num_hidden_layers):
    if share_parameter_across_layers:
        name_variable_scope=""layer_shared""
    else:
        name_variable_scope=""layer_%d"" % layer_idx
    # share all parameters across layers. add by brightmart, 2019-09-28. previous it is like this: ""layer_%d"" % layer_idx
    with tf.variable_scope(name_variable_scope, reuse=True if (share_parameter_across_layers and layer_idx>0) else False):

      layer_input = prev_output

      with tf.variable_scope(""attention""):
        attention_heads = []
        with tf.variable_scope(""self""):
          attention_head = attention_layer(
              from_tensor=layer_input,
              to_tensor=layer_input,
              attention_mask=attention_mask,
              num_attention_heads=num_attention_heads,
              size_per_head=attention_head_size,
              attention_probs_dropout_prob=attention_probs_dropout_prob,
              initializer_range=initializer_range,
              do_return_2d_tensor=True,
              batch_size=batch_size,
              from_seq_length=seq_length,
              to_seq_length=seq_length)
          attention_heads.append(attention_head)

        attention_output = None
        if len(attention_heads) == 1:
          attention_output = attention_heads[0]
        else:
          # In the case where we have other sequences, we just concatenate
          # them to the self-attention head before the projection.
          attention_output = tf.concat(attention_heads, axis=-1)

        # Run a linear projection of `hidden_size` then add a residual
        # with `layer_input`.
        with tf.variable_scope(""output""):
          attention_output = tf.layers.dense(
              attention_output,
              hidden_size,
              kernel_initializer=create_initializer(initializer_range))
          attention_output = dropout(attention_output, hidden_dropout_prob)
          attention_output = layer_norm(attention_output + layer_input)

      # The activation is only applied to the ""intermediate"" hidden layer.
      with tf.variable_scope(""intermediate""):
        intermediate_output = tf.layers.dense(
            attention_output,
            intermediate_size,
            activation=intermediate_act_fn,
            kernel_initializer=create_initializer(initializer_range))

      # Down-project back to `hidden_size` then add the residual.
      with tf.variable_scope(""output""):
        layer_output = tf.layers.dense(
            intermediate_output,
            hidden_size,
            kernel_initializer=create_initializer(initializer_range))
        layer_output = dropout(layer_output, hidden_dropout_prob)
        layer_output = layer_norm(layer_output + attention_output)
        prev_output = layer_output
        all_layer_outputs.append(layer_output)

  if do_return_all_layers:
    final_outputs = []
    for layer_output in all_layer_outputs:
      final_output = reshape_from_matrix(layer_output, input_shape)
      final_outputs.append(final_output)
    return final_outputs
  else:
    final_output = reshape_from_matrix(prev_output, input_shape)
    return final_output


def get_shape_list(tensor, expected_rank=None, name=None):
  """"""Returns a list of the shape of tensor, preferring static dimensions.

  Args:
    tensor: A tf.Tensor object to find the shape of.
    expected_rank: (optional) int. The expected rank of `tensor`. If this is
      specified and the `tensor` has a different rank, and exception will be
      thrown.
    name: Optional name of the tensor for the error message.

  Returns:
    A list of dimensions of the shape of tensor. All static dimensions will
    be returned as python integers, and dynamic dimensions will be returned
    as tf.Tensor scalars.
  """"""
  if name is None:
    name = tensor.name

  if expected_rank is not None:
    assert_rank(tensor, expected_rank, name)

  shape = tensor.shape.as_list()

  non_static_indexes = []
  for (index, dim) in enumerate(shape):
    if dim is None:
      non_static_indexes.append(index)

  if not non_static_indexes:
    return shape

  dyn_shape = tf.shape(tensor)
  for index in non_static_indexes:
    shape[index] = dyn_shape[index]
  return shape


def reshape_to_matrix(input_tensor):
  """"""Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).""""""
  ndims = input_tensor.shape.ndims
  if ndims < 2:
    raise ValueError(""Input tensor must have at least rank 2. Shape = %s"" %
                     (input_tensor.shape))
  if ndims == 2:
    return input_tensor

  width = input_tensor.shape[-1]
  output_tensor = tf.reshape(input_tensor, [-1, width])
  return output_tensor


def reshape_from_matrix(output_tensor, orig_shape_list):
  """"""Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""""""
  if len(orig_shape_list) == 2:
    return output_tensor

  output_shape = get_shape_list(output_tensor)

  orig_dims = orig_shape_list[0:-1]
  width = output_shape[-1]

  return tf.reshape(output_tensor, orig_dims + [width])


def assert_rank(tensor, expected_rank, name=None):
  """"""Raises an exception if the tensor rank is not of the expected rank.

  Args:
    tensor: A tf.Tensor to check the rank of.
    expected_rank: Python integer or list of integers, expected rank.
    name: Optional name of the tensor for the error message.

  Raises:
    ValueError: If the expected shape doesn't match the actual shape.
  """"""
  if name is None:
    name = tensor.name

  expected_rank_dict = {}
  if isinstance(expected_rank, six.integer_types):
    expected_rank_dict[expected_rank] = True
  else:
    for x in expected_rank:
      expected_rank_dict[x] = True

  actual_rank = tensor.shape.ndims
  if actual_rank not in expected_rank_dict:
    scope_name = tf.get_variable_scope().name
    raise ValueError(
        ""For the tensor `%s` in scope `%s`, the actual rank ""
        ""`%d` (shape = %s) is not equal to the expected rank `%s`"" %
        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))

def prelln_transformer_model(input_tensor,
						attention_mask=None,
						hidden_size=768,
						num_hidden_layers=12,
						num_attention_heads=12,
						intermediate_size=3072,
						intermediate_act_fn=gelu,
						hidden_dropout_prob=0.1,
						attention_probs_dropout_prob=0.1,
						initializer_range=0.02,
						do_return_all_layers=False,
						shared_type='all', # None,
						adapter_fn=None):
	""""""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".

	This is almost an exact implementation of the original Transformer encoder.

	See the original paper:
	https://arxiv.org/abs/1706.03762

	Also see:
	https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py

	Args:
		input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].
		attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,
			seq_length], with 1 for positions that can be attended to and 0 in
			positions that should not be.
		hidden_size: int. Hidden size of the Transformer.
		num_hidden_layers: int. Number of layers (blocks) in the Transformer.
		num_attention_heads: int. Number of attention heads in the Transformer.
		intermediate_size: int. The size of the ""intermediate"" (a.k.a., feed
			forward) layer.
		intermediate_act_fn: function. The non-linear activation function to apply
			to the output of the intermediate/feed-forward layer.
		hidden_dropout_prob: float. Dropout probability for the hidden layers.
		attention_probs_dropout_prob: float. Dropout probability of the attention
			probabilities.
		initializer_range: float. Range of the initializer (stddev of truncated
			normal).
		do_return_all_layers: Whether to also return all layers or just the final
			layer.

	Returns:
		float Tensor of shape [batch_size, seq_length, hidden_size], the final
		hidden layer of the Transformer.

	Raises:
		ValueError: A Tensor shape or parameter is invalid.
	""""""
	if hidden_size % num_attention_heads != 0:
		raise ValueError(
				""The hidden size (%d) is not a multiple of the number of attention ""
				""heads (%d)"" % (hidden_size, num_attention_heads))

	attention_head_size = int(hidden_size / num_attention_heads)

	input_shape = bert_utils.get_shape_list(input_tensor, expected_rank=3)
	batch_size = input_shape[0]
	seq_length = input_shape[1]
	input_width = input_shape[2]

	# The Transformer performs sum residuals on all layers so the input needs
	# to be the same as the hidden size.
	if input_width != hidden_size:
		raise ValueError(""The width of the input tensor (%d) != hidden size (%d)"" %
										 (input_width, hidden_size))

	# We keep the representation as a 2D tensor to avoid re-shaping it back and
	# forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on
	# the GPU/CPU but may not be free on the TPU, so we want to minimize them to
	# help the optimizer.
	prev_output = bert_utils.reshape_to_matrix(input_tensor)

	all_layer_outputs = []

	def layer_scope(idx, shared_type):
		if shared_type == 'all':
			tmp = {
				""layer"":""layer_shared"",
				'attention':'attention',
				'intermediate':'intermediate',
				'output':'output'
			}
		elif shared_type == 'attention':
			tmp = {
				""layer"":""layer_shared"",
				'attention':'attention',
				'intermediate':'intermediate_{}'.format(idx),
				'output':'output_{}'.format(idx)
			}
		elif shared_type == 'ffn':
			tmp = {
				""layer"":""layer_shared"",
				'attention':'attention_{}'.format(idx),
				'intermediate':'intermediate',
				'output':'output'
			}
		else:
			tmp = {
				""layer"":""layer_{}"".format(idx),
				'attention':'attention',
				'intermediate':'intermediate',
				'output':'output'
			}

		return tmp

	all_layer_outputs = []

	for layer_idx in range(num_hidden_layers):

		idx_scope = layer_scope(layer_idx, shared_type)

		with tf.variable_scope(idx_scope['layer'], reuse=tf.AUTO_REUSE):
			layer_input = prev_output

			with tf.variable_scope(idx_scope['attention'], reuse=tf.AUTO_REUSE):
				attention_heads = []

				with tf.variable_scope(""output"", reuse=tf.AUTO_REUSE):
					layer_input_pre = layer_norm(layer_input)

				with tf.variable_scope(""self""):
					attention_head = attention_layer(
							from_tensor=layer_input_pre,
							to_tensor=layer_input_pre,
							attention_mask=attention_mask,
							num_attention_heads=num_attention_heads,
							size_per_head=attention_head_size,
							attention_probs_dropout_prob=attention_probs_dropout_prob,
							initializer_range=initializer_range,
							do_return_2d_tensor=True,
							batch_size=batch_size,
							from_seq_length=seq_length,
							to_seq_length=seq_length)
					attention_heads.append(attention_head)

				attention_output = None
				if len(attention_heads) == 1:
					attention_output = attention_heads[0]
				else:
					# In the case where we have other sequences, we just concatenate
					# them to the self-attention head before the projection.
					attention_output = tf.concat(attention_heads, axis=-1)

				# Run a linear projection of `hidden_size` then add a residual
				# with `layer_input`.
				with tf.variable_scope(""output"", reuse=tf.AUTO_REUSE):
					attention_output = tf.layers.dense(
							attention_output,
							hidden_size,
							kernel_initializer=create_initializer(initializer_range))
					attention_output = dropout(attention_output, hidden_dropout_prob)

					# attention_output = layer_norm(attention_output + layer_input)
					attention_output = attention_output + layer_input

			with tf.variable_scope(idx_scope['output'], reuse=tf.AUTO_REUSE):
				attention_output_pre = layer_norm(attention_output)

			# The activation is only applied to the ""intermediate"" hidden layer.
			with tf.variable_scope(idx_scope['intermediate'], reuse=tf.AUTO_REUSE):
				intermediate_output = tf.layers.dense(
						attention_output_pre,
						intermediate_size,
						activation=intermediate_act_fn,
						kernel_initializer=create_initializer(initializer_range))

			# Down-project back to `hidden_size` then add the residual.
			with tf.variable_scope(idx_scope['output'], reuse=tf.AUTO_REUSE):
				layer_output = tf.layers.dense(
						intermediate_output,
						hidden_size,
						kernel_initializer=create_initializer(initializer_range))
				layer_output = dropout(layer_output, hidden_dropout_prob)

				# layer_output = layer_norm(layer_output + attention_output)
				layer_output = layer_output + attention_output
				prev_output = layer_output
				all_layer_outputs.append(layer_output)

	if do_return_all_layers:
		final_outputs = []
		for layer_output in all_layer_outputs:
			final_output = bert_utils.reshape_from_matrix(layer_output, input_shape)
			final_outputs.append(final_output)
		return final_outputs
	else:
		final_output = bert_utils.reshape_from_matrix(prev_output, input_shape)
		return final_output
"
Albert,create_pretraining_data_roberta.py,"# coding=utf-8
# Copyright 2018 The Google AI Language Team Authors.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
""""""Create masked LM/next sentence masked_lm TF examples for BERT.""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import random
import re
import tokenization
import tensorflow as tf
import jieba

flags = tf.flags

FLAGS = flags.FLAGS

flags.DEFINE_string(""input_file"", None,
                    ""Input raw text file (or comma-separated list of files)."")

flags.DEFINE_string(
    ""output_file"", None,
    ""Output TF example file (or comma-separated list of files)."")

flags.DEFINE_string(""vocab_file"", None,
                    ""The vocabulary file that the BERT model was trained on."")

flags.DEFINE_bool(
    ""do_lower_case"", True,
    ""Whether to lower case the input text. Should be True for uncased ""
    ""models and False for cased models."")

flags.DEFINE_bool(
    ""do_whole_word_mask"", False,
    ""Whether to use whole word masking rather than per-WordPiece masking."")

flags.DEFINE_integer(""max_seq_length"", 128, ""Maximum sequence length."")

flags.DEFINE_integer(""max_predictions_per_seq"", 20,
                     ""Maximum number of masked LM predictions per sequence."")

flags.DEFINE_integer(""random_seed"", 12345, ""Random seed for data generation."")

flags.DEFINE_integer(
    ""dupe_factor"", 10,
    ""Number of times to duplicate the input data (with different masks)."")

flags.DEFINE_float(""masked_lm_prob"", 0.15, ""Masked LM probability."")

flags.DEFINE_float(
    ""short_seq_prob"", 0.1,
    ""Probability of creating sequences which are shorter than the ""
    ""maximum length."")


class TrainingInstance(object):
    """"""A single training instance (sentence pair).""""""

    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,
                 is_random_next):
        self.tokens = tokens
        self.segment_ids = segment_ids
        self.is_random_next = is_random_next
        self.masked_lm_positions = masked_lm_positions
        self.masked_lm_labels = masked_lm_labels

    def __str__(self):
        s = """"
        s += ""tokens: %s\n"" % ("" "".join(
            [tokenization.printable_text(x) for x in self.tokens]))
        s += ""segment_ids: %s\n"" % ("" "".join([str(x) for x in self.segment_ids]))
        s += ""is_random_next: %s\n"" % self.is_random_next
        s += ""masked_lm_positions: %s\n"" % ("" "".join(
            [str(x) for x in self.masked_lm_positions]))
        s += ""masked_lm_labels: %s\n"" % ("" "".join(
            [tokenization.printable_text(x) for x in self.masked_lm_labels]))
        s += ""\n""
        return s

    def __repr__(self):
        return self.__str__()


def write_instance_to_example_files(instances, tokenizer, max_seq_length,
                                    max_predictions_per_seq, output_files):
    """"""Create TF example files from `TrainingInstance`s.""""""
    writers = []
    for output_file in output_files:
        writers.append(tf.python_io.TFRecordWriter(output_file))

    writer_index = 0

    total_written = 0
    for (inst_index, instance) in enumerate(instances):
        input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)
        input_mask = [1] * len(input_ids)
        segment_ids = list(instance.segment_ids)
        assert len(input_ids) <= max_seq_length

        while len(input_ids) < max_seq_length:
            input_ids.append(0)
            input_mask.append(0)
            segment_ids.append(0)

        assert len(input_ids) == max_seq_length
        assert len(input_mask) == max_seq_length
        # print(""length of segment_ids:"",len(segment_ids),""max_seq_length:"", max_seq_length)
        assert len(segment_ids) == max_seq_length

        masked_lm_positions = list(instance.masked_lm_positions)
        masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)
        masked_lm_weights = [1.0] * len(masked_lm_ids)

        while len(masked_lm_positions) < max_predictions_per_seq:
            masked_lm_positions.append(0)
            masked_lm_ids.append(0)
            masked_lm_weights.append(0.0)

        next_sentence_label = 1 if instance.is_random_next else 0

        features = collections.OrderedDict()
        features[""input_ids""] = create_int_feature(input_ids)
        features[""input_mask""] = create_int_feature(input_mask)
        features[""segment_ids""] = create_int_feature(segment_ids)
        features[""masked_lm_positions""] = create_int_feature(masked_lm_positions)
        features[""masked_lm_ids""] = create_int_feature(masked_lm_ids)
        features[""masked_lm_weights""] = create_float_feature(masked_lm_weights)
        features[""next_sentence_labels""] = create_int_feature([next_sentence_label])

        tf_example = tf.train.Example(features=tf.train.Features(feature=features))

        writers[writer_index].write(tf_example.SerializeToString())
        writer_index = (writer_index + 1) % len(writers)

        total_written += 1

        if inst_index < 20:
            tf.logging.info(""*** Example ***"")
            tf.logging.info(""tokens: %s"" % "" "".join(
                [tokenization.printable_text(x) for x in instance.tokens]))

            for feature_name in features.keys():
                feature = features[feature_name]
                values = []
                if feature.int64_list.value:
                    values = feature.int64_list.value
                elif feature.float_list.value:
                    values = feature.float_list.value
                tf.logging.info(
                    ""%s: %s"" % (feature_name, "" "".join([str(x) for x in values])))

    for writer in writers:
        writer.close()

    tf.logging.info(""Wrote %d total instances"", total_written)


def create_int_feature(values):
    feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))
    return feature


def create_float_feature(values):
    feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))
    return feature


def create_training_instances(input_files, tokenizer, max_seq_length,
                              dupe_factor, short_seq_prob, masked_lm_prob,
                              max_predictions_per_seq, rng):
    """"""Create `TrainingInstance`s from raw text.""""""
    all_documents = [[]]

    # Input file format:
    # (1) One sentence per line. These should ideally be actual sentences, not
    # entire paragraphs or arbitrary spans of text. (Because we use the
    # sentence boundaries for the ""next sentence prediction"" task).
    # (2) Blank lines between documents. Document boundaries are needed so
    # that the ""next sentence prediction"" task doesn't span between documents.
    print(""create_training_instances.started..."")
    for input_file in input_files:
        with tf.gfile.GFile(input_file, ""r"") as reader:
            while True:
                line = tokenization.convert_to_unicode(reader.readline().replace(""<eop>"",""""))# .replace(""”"","""")) # 将<eop>、”替换掉。
                if not line:
                    break
                line = line.strip()

                # Empty lines are used as document delimiters
                if not line:
                    all_documents.append([])
                tokens = tokenizer.tokenize(line)
                if tokens:
                    all_documents[-1].append(tokens)

    # Remove empty documents
    all_documents = [x for x in all_documents if x]
    rng.shuffle(all_documents)

    vocab_words = list(tokenizer.vocab.keys())
    instances = []
    for _ in range(dupe_factor):
        for document_index in range(len(all_documents)):
            instances.extend(
                create_instances_from_document(
                    all_documents, document_index, max_seq_length, short_seq_prob,
                    masked_lm_prob, max_predictions_per_seq, vocab_words, rng))

    rng.shuffle(instances)
    print(""create_training_instances.ended..."")

    return instances


def _is_chinese_char(cp):
    """"""Checks whether CP is the codepoint of a CJK character.""""""
    # This defines a ""chinese character"" as anything in the CJK Unicode block:
    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
    #
    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,
    # despite its name. The modern Korean Hangul alphabet is a different block,
    # as is Japanese Hiragana and Katakana. Those alphabets are used to write
    # space-separated words, so they are not treated specially and handled
    # like the all of the other languages.
    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #
        (cp >= 0x3400 and cp <= 0x4DBF) or  #
        (cp >= 0x20000 and cp <= 0x2A6DF) or  #
        (cp >= 0x2A700 and cp <= 0x2B73F) or  #
        (cp >= 0x2B740 and cp <= 0x2B81F) or  #
        (cp >= 0x2B820 and cp <= 0x2CEAF) or
        (cp >= 0xF900 and cp <= 0xFAFF) or  #
            (cp >= 0x2F800 and cp <= 0x2FA1F)):  #
        return True


def get_new_segment(segment): #  新增的方法 ####
    """"""
    输入一句话，返回一句经过处理的话: 为了支持中文全称mask，将被分开的词，将上特殊标记(""#"")，使得后续处理模块，能够知道哪些字是属于同一个词的。
    :param segment: 一句话
    :return: 一句处理过的话
    """"""
    seq_cws = jieba.lcut("""".join(segment))
    seq_cws_dict = {x: 1 for x in seq_cws}
    new_segment = []
    i = 0
    while i < len(segment):
        if len(re.findall('[\u4E00-\u9FA5]', segment[i]))==0: # 不是中文的，原文加进去。
            new_segment.append(segment[i])
            i += 1
            continue

        has_add = False
        for length in range(3,0,-1):
            if i+length>len(segment):
                continue
            if ''.join(segment[i:i+length]) in seq_cws_dict:
                new_segment.append(segment[i])
                for l in range(1, length):
                    new_segment.append('##' + segment[i+l])
                i += length
                has_add = True
                break
        if not has_add:
            new_segment.append(segment[i])
            i += 1
    return new_segment

def get_raw_instance(document,max_sequence_length): # 新增的方法 TODO need check again to ensure full use of data
    """"""
    获取初步的训练实例，将整段按照max_sequence_length切分成多个部分,并以多个处理好的实例的形式返回。
    :param document: 一整段
    :param max_sequence_length:
    :return: a list. each element is a sequence of text
    """"""
    max_sequence_length_allowed=max_sequence_length-2
    document = [seq for seq in document if len(seq)<max_sequence_length_allowed]
    sizes = [len(seq) for seq in document]

    result_list = []
    curr_seq = [] # 当前处理的序列
    sz_idx = 0
    while sz_idx < len(sizes):
        # 当前句子加上新的句子，如果长度小于最大限制，则合并当前句子和新句子；否则即超过了最大限制，那么做为一个新的序列加到目标列表中
        if len(curr_seq) + sizes[sz_idx] <= max_sequence_length_allowed: # or len(curr_seq)==0:
            curr_seq += document[sz_idx]
            sz_idx += 1
        else:
            result_list.append(curr_seq)
            curr_seq = []
    # 对最后一个序列进行处理，如果太短的话，丢弃掉。
    if len(curr_seq)>max_sequence_length_allowed/2: # /2
        result_list.append(curr_seq)

    # # 计算总共可以得到多少份
    # num_instance=int(len(big_list)/max_sequence_length_allowed)+1
    # print(""num_instance:"",num_instance)
    # # 切分成多份，添加到列表中
    # result_list=[]
    # for j in range(num_instance):
    #     index=j*max_sequence_length_allowed
    #     end_index=index+max_sequence_length_allowed if j!=num_instance-1 else -1
    #     result_list.append(big_list[index:end_index])
    return result_list

def create_instances_from_document( # 新增的方法
    # 目标按照RoBERTa的思路，使用DOC-SENTENCES，并会去掉NSP任务: 从一个文档中连续的获得文本，直到达到最大长度。如果是从下一个文档中获得，那么加上一个分隔符
    #  document即一整段话，包含多个句子。每个句子叫做segment.
    # 给定一个document即一整段话，生成一些instance.
        all_documents, document_index, max_seq_length, short_seq_prob,
        masked_lm_prob, max_predictions_per_seq, vocab_words, rng):
    """"""Creates `TrainingInstance`s for a single document.""""""
    document = all_documents[document_index]

    # Account for [CLS], [SEP], [SEP]
    max_num_tokens = max_seq_length - 3

    # We *usually* want to fill up the entire sequence since we are padding
    # to `max_seq_length` anyways, so short sequences are generally wasted
    # computation. However, we *sometimes*
    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter
    # sequences to minimize the mismatch between pre-training and fine-tuning.
    # The `target_seq_length` is just a rough target however, whereas
    # `max_seq_length` is a hard limit.

    #target_seq_length = max_num_tokens
    #if rng.random() < short_seq_prob:
    #    target_seq_length = rng.randint(2, max_num_tokens)

    instances = []
    raw_text_list_list=get_raw_instance(document, max_seq_length) # document即一整段话，包含多个句子。每个句子叫做segment.
    for j, raw_text_list in enumerate(raw_text_list_list):
        ####################################################################################################################
        raw_text_list = get_new_segment(raw_text_list) # 结合分词的中文的whole mask设置即在需要的地方加上“##”
        # 1、设置token, segment_ids
        is_random_next=True # this will not be used, so it's value doesn't matter
        tokens = []
        segment_ids = []
        tokens.append(""[CLS]"")
        segment_ids.append(0)
        for token in raw_text_list:
            tokens.append(token)
            segment_ids.append(0)
        tokens.append(""[SEP]"")
        segment_ids.append(0)
        ################################################################################################################
        # 2、调用原有的方法
        (tokens, masked_lm_positions,
         masked_lm_labels) = create_masked_lm_predictions(
            tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)
        instance = TrainingInstance(
            tokens=tokens,
            segment_ids=segment_ids,
            is_random_next=is_random_next,
            masked_lm_positions=masked_lm_positions,
            masked_lm_labels=masked_lm_labels)
        instances.append(instance)

    return instances



def create_instances_from_document_original(
        all_documents, document_index, max_seq_length, short_seq_prob,
        masked_lm_prob, max_predictions_per_seq, vocab_words, rng):
    """"""Creates `TrainingInstance`s for a single document.""""""
    document = all_documents[document_index]

    # Account for [CLS], [SEP], [SEP]
    max_num_tokens = max_seq_length - 3

    # We *usually* want to fill up the entire sequence since we are padding
    # to `max_seq_length` anyways, so short sequences are generally wasted
    # computation. However, we *sometimes*
    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter
    # sequences to minimize the mismatch between pre-training and fine-tuning.
    # The `target_seq_length` is just a rough target however, whereas
    # `max_seq_length` is a hard limit.
    target_seq_length = max_num_tokens
    if rng.random() < short_seq_prob:
        target_seq_length = rng.randint(2, max_num_tokens)

    # We DON'T just concatenate all of the tokens from a document into a long
    # sequence and choose an arbitrary split point because this would make the
    # next sentence prediction task too easy. Instead, we split the input into
    # segments ""A"" and ""B"" based on the actual ""sentences"" provided by the user
    # input.
    instances = []
    current_chunk = []
    current_length = 0
    i = 0
    print(""document_index:"",document_index,""document:"",type(document),"" ;document:"",document) # document即一整段话，包含多个句子。每个句子叫做segment.
    while i < len(document):
        segment = document[i] # 取到一个部分（可能是一段话）
        print(""i:"",i,"" ;segment:"",segment)
        ####################################################################################################################
        segment = get_new_segment(segment) # 结合分词的中文的whole mask设置即在需要的地方加上“##”
        ###################################################################################################################
        current_chunk.append(segment)
        current_length += len(segment)
        print(""#####condition:"",i == len(document) - 1 or current_length >= target_seq_length)
        if i == len(document) - 1 or current_length >= target_seq_length:
            if current_chunk:
                # `a_end` is how many segments from `current_chunk` go into the `A`
                # (first) sentence.
                a_end = 1
                if len(current_chunk) >= 2:
                    a_end = rng.randint(1, len(current_chunk) - 1)

                tokens_a = []
                for j in range(a_end):
                    tokens_a.extend(current_chunk[j])

                tokens_b = []
                # Random next
                is_random_next = False
                if len(current_chunk) == 1 or rng.random() < 0.5:
                    is_random_next = True
                    target_b_length = target_seq_length - len(tokens_a)

                    # This should rarely go for more than one iteration for large
                    # corpora. However, just to be careful, we try to make sure that
                    # the random document is not the same as the document
                    # we're processing.
                    for _ in range(10):
                        random_document_index = rng.randint(0, len(all_documents) - 1)
                        if random_document_index != document_index:
                            break

                    random_document = all_documents[random_document_index]
                    random_start = rng.randint(0, len(random_document) - 1)
                    for j in range(random_start, len(random_document)):
                        tokens_b.extend(random_document[j])
                        if len(tokens_b) >= target_b_length:
                            break
                    # We didn't actually use these segments so we ""put them back"" so
                    # they don't go to waste.
                    num_unused_segments = len(current_chunk) - a_end
                    i -= num_unused_segments
                # Actual next
                else:
                    is_random_next = False
                    for j in range(a_end, len(current_chunk)):
                        tokens_b.extend(current_chunk[j])
                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)

                assert len(tokens_a) >= 1
                assert len(tokens_b) >= 1

                tokens = []
                segment_ids = []
                tokens.append(""[CLS]"")
                segment_ids.append(0)
                for token in tokens_a:
                    tokens.append(token)
                    segment_ids.append(0)

                tokens.append(""[SEP]"")
                segment_ids.append(0)

                for token in tokens_b:
                    tokens.append(token)
                    segment_ids.append(1)
                tokens.append(""[SEP]"")
                segment_ids.append(1)

                (tokens, masked_lm_positions,
                 masked_lm_labels) = create_masked_lm_predictions(
                     tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)
                instance = TrainingInstance(
                    tokens=tokens,
                    segment_ids=segment_ids,
                    is_random_next=is_random_next,
                    masked_lm_positions=masked_lm_positions,
                    masked_lm_labels=masked_lm_labels)
                instances.append(instance)
            current_chunk = []
            current_length = 0
        i += 1

    return instances


MaskedLmInstance = collections.namedtuple(""MaskedLmInstance"",
                                          [""index"", ""label""])


def create_masked_lm_predictions(tokens, masked_lm_prob,
                                 max_predictions_per_seq, vocab_words, rng):
    """"""Creates the predictions for the masked LM objective.""""""

    cand_indexes = []
    for (i, token) in enumerate(tokens):
        if token == ""[CLS]"" or token == ""[SEP]"":
            continue
        # Whole Word Masking means that if we mask all of the wordpieces
        # corresponding to an original word. When a word has been split into
        # WordPieces, the first token does not have any marker and any subsequence
        # tokens are prefixed with ##. So whenever we see the ## token, we
        # append it to the previous set of word indexes.
        #
        # Note that Whole Word Masking does *not* change the training code
        # at all -- we still predict each WordPiece independently, softmaxed
        # over the entire vocabulary.
        if (FLAGS.do_whole_word_mask and len(cand_indexes) >= 1 and
                token.startswith(""##"")):
            cand_indexes[-1].append(i)
        else:
            cand_indexes.append([i])

    rng.shuffle(cand_indexes)

    output_tokens = [t[2:] if len(re.findall('##[\u4E00-\u9FA5]', t))>0 else t for t in tokens] # 去掉""##""

    num_to_predict = min(max_predictions_per_seq,
                         max(1, int(round(len(tokens) * masked_lm_prob))))

    masked_lms = []
    covered_indexes = set()
    for index_set in cand_indexes:
        if len(masked_lms) >= num_to_predict:
            break
        # If adding a whole-word mask would exceed the maximum number of
        # predictions, then just skip this candidate.
        if len(masked_lms) + len(index_set) > num_to_predict:
            continue
        is_any_index_covered = False
        for index in index_set:
            if index in covered_indexes:
                is_any_index_covered = True
                break
        if is_any_index_covered:
            continue
        for index in index_set:
            covered_indexes.add(index)

            masked_token = None
            # 80% of the time, replace with [MASK]
            if rng.random() < 0.8:
                masked_token = ""[MASK]""
            else:
                # 10% of the time, keep original
                if rng.random() < 0.5:
                    masked_token = tokens[index][2:] if len(re.findall('##[\u4E00-\u9FA5]', tokens[index]))>0 else tokens[index] # 去掉""##""
                # 10% of the time, replace with random word
                else:
                    masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]

            output_tokens[index] = masked_token

            masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))
    assert len(masked_lms) <= num_to_predict
    masked_lms = sorted(masked_lms, key=lambda x: x.index)

    masked_lm_positions = []
    masked_lm_labels = []
    for p in masked_lms:
        masked_lm_positions.append(p.index)
        masked_lm_labels.append(p.label)

    # tf.logging.info('%s' % (tokens))
    # tf.logging.info('%s' % (output_tokens))
    return (output_tokens, masked_lm_positions, masked_lm_labels)


def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):
    """"""Truncates a pair of sequences to a maximum sequence length.""""""
    while True:
        total_length = len(tokens_a) + len(tokens_b)
        if total_length <= max_num_tokens:
            break

        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b
        assert len(trunc_tokens) >= 1

        # We want to sometimes truncate from the front and sometimes from the
        # back to add more randomness and avoid biases.
        if rng.random() < 0.5:
            del trunc_tokens[0]
        else:
            trunc_tokens.pop()


def main(_):
    tf.logging.set_verbosity(tf.logging.INFO)

    tokenizer = tokenization.FullTokenizer(
        vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)

    input_files = []
    for input_pattern in FLAGS.input_file.split("",""):
        input_files.extend(tf.gfile.Glob(input_pattern))

    tf.logging.info(""*** Reading from input files ***"")
    for input_file in input_files:
        tf.logging.info(""  %s"", input_file)

    rng = random.Random(FLAGS.random_seed)
    instances = create_training_instances(
        input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,
        FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,
        rng)

    output_files = FLAGS.output_file.split("","")
    tf.logging.info(""*** Writing to output files ***"")
    for output_file in output_files:
        tf.logging.info(""  %s"", output_file)

    write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,
                                    FLAGS.max_predictions_per_seq, output_files)


if __name__ == ""__main__"":
    flags.mark_flag_as_required(""input_file"")
    flags.mark_flag_as_required(""output_file"")
    flags.mark_flag_as_required(""vocab_file"")
    tf.app.run()"
Tiler,conf.py,"# GEN TILES CONFS

# number of divisions per channel (R, G and B)
# DEPTH = 4 -> 4 * 4 * 4 = 64 colors
DEPTH = 4
# list of rotations, in degrees, to apply over the original image
ROTATIONS = [0]


#############################


# TILER CONFS

# number of divisions per channel
# (COLOR_DEPTH = 32 -> 32 * 32 * 32 = 32768 colors)
COLOR_DEPTH = 32
# tiles scales (1 = default resolution)
RESIZING_SCALES = [0.5, 0.4, 0.3, 0.2, 0.1]
# number of pixels shifted to create each box (tuple with (x,y))
# if value is None, shift will be done accordingly to tiles dimensions
PIXEL_SHIFT = (5, 5)
# if tiles can overlap
OVERLAP_TILES = False
# render image as its being built
RENDER = False
# multiprocessing pool size
POOL_SIZE = 8

# out file name
OUT = 'out.png'
# image to tile (ignored if passed as the 1st arg)
IMAGE_TO_TILE = None
# folder with tiles (ignored if passed as the 2nd arg)
TILES_FOLDER = None
"
Tiler,tiler.py,"import cv2
import numpy as np
import os
import sys
from collections import defaultdict
from tqdm import tqdm
from multiprocessing import Pool
import math
import pickle
import conf
from time import sleep


# number of colors per image
COLOR_DEPTH = conf.COLOR_DEPTH
# tiles scales
RESIZING_SCALES = conf.RESIZING_SCALES
# number of pixels shifted to create each box (x,y)
PIXEL_SHIFT = conf.PIXEL_SHIFT
# multiprocessing pool size
POOL_SIZE = conf.POOL_SIZE
# if tiles can overlap
OVERLAP_TILES = conf.OVERLAP_TILES


# reduces the number of colors in an image
def color_quantization(img, n_colors):
    return np.round(img / 255 * n_colors) / n_colors * 255


# returns an image given its path
def read_image(path):
    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)
    if img.shape[2] == 3:
        img = cv2.cvtColor(img, cv2.COLOR_BGR2BGRA)
    img = color_quantization(img.astype('float'), COLOR_DEPTH)
    return img.astype('uint8')


# scales an image
def resize_image(img, ratio):
    img = cv2.resize(img, (int(img.shape[1] * ratio), int(img.shape[0] * ratio)))
    return img


# the most frequent color in an image and its relative frequency
def mode_color(img, ignore_alpha=False):
    counter = defaultdict(int)
    total = 0
    for y in img:
        for x in y:
            if len(x) < 4 or ignore_alpha or x[3] != 0:
                counter[tuple(x[:3])] += 1
            else:
                counter[(-1,-1,-1)] += 1
            total += 1

    if total > 0:
        mode_color = max(counter, key=counter.get)
        if mode_color == (-1,-1,-1):
            return None, None
        else:
            return mode_color, counter[mode_color] / total
    else:
        return None, None


# displays an image
def show_image(img, wait=True):
    cv2.imshow('img', img)
    if wait:
        cv2.waitKey(0)
    else:
        cv2.waitKey(1)


# load and process the tiles
def load_tiles(paths):
    print('Loading tiles')
    tiles = defaultdict(list)

    for path in paths:
        if os.path.isdir(path):
            for tile_name in tqdm(os.listdir(path)):
                tile = read_image(os.path.join(path, tile_name))
                mode, rel_freq = mode_color(tile, ignore_alpha=True)
                if mode is not None:
                    for scale in RESIZING_SCALES:
                        t = resize_image(tile, scale)
                        res = tuple(t.shape[:2])
                        tiles[res].append({
                            'tile': t,
                            'mode': mode,
                            'rel_freq': rel_freq
                        })

            with open('tiles.pickle', 'wb') as f:
                pickle.dump(tiles, f)

        # load pickle with tiles (one file only)
        else:
            with open(path, 'rb') as f:
                tiles = pickle.load(f)

    return tiles


# returns the boxes (image and start pos) from an image, with 'res' resolution
def image_boxes(img, res):
    if not PIXEL_SHIFT:
        shift = np.flip(res)
    else:
        shift = PIXEL_SHIFT

    boxes = []
    for y in range(0, img.shape[0], shift[1]):
        for x in range(0, img.shape[1], shift[0]):
            boxes.append({
                'img': img[y:y+res[0], x:x+res[1]],
                'pos': (x,y)
            })

    return boxes


# euclidean distance between two colors
def color_distance(c1, c2):
    c1_int = [int(x) for x in c1]
    c2_int = [int(x) for x in c2]
    return math.sqrt((c1_int[0] - c2_int[0])**2 + (c1_int[1] - c2_int[1])**2 + (c1_int[2] - c2_int[2])**2)


# returns the most similar tile to a box (in terms of color)
def most_similar_tile(box_mode_freq, tiles):
    if not box_mode_freq[0]:
        return (0, np.zeros(shape=tiles[0]['tile'].shape))
    else:
        min_distance = None
        min_tile_img = None
        for t in tiles:
            dist = (1 + color_distance(box_mode_freq[0], t['mode'])) / box_mode_freq[1]
            if min_distance is None or dist < min_distance:
                min_distance = dist
                min_tile_img = t['tile']
        return (min_distance, min_tile_img)


# builds the boxes and finds the best tile for each one
def get_processed_image_boxes(image_path, tiles):
    print('Getting and processing boxes')
    img = read_image(image_path)
    pool = Pool(POOL_SIZE)
    all_boxes = []

    for res, ts in tqdm(sorted(tiles.items(), reverse=True)):
        boxes = image_boxes(img, res)
        modes = pool.map(mode_color, [x['img'] for x in boxes])
        most_similar_tiles = pool.starmap(most_similar_tile, zip(modes, [ts for x in range(len(modes))]))

        i = 0
        for min_dist, tile in most_similar_tiles:
            boxes[i]['min_dist'] = min_dist
            boxes[i]['tile'] = tile
            i += 1

        all_boxes += boxes

    return all_boxes, img.shape


# places a tile in the image
def place_tile(img, box):
    p1 = np.flip(box['pos'])
    p2 = p1 + box['img'].shape[:2]
    img_box = img[p1[0]:p2[0], p1[1]:p2[1]]
    mask = box['tile'][:, :, 3] != 0
    mask = mask[:img_box.shape[0], :img_box.shape[1]]
    if OVERLAP_TILES or not np.any(img_box[mask]):
        img_box[mask] = box['tile'][:img_box.shape[0], :img_box.shape[1], :][mask]


# tiles the image
def create_tiled_image(boxes, res, render=False):
    print('Creating tiled image')
    img = np.zeros(shape=(res[0], res[1], 4), dtype=np.uint8)

    for box in tqdm(sorted(boxes, key=lambda x: x['min_dist'], reverse=OVERLAP_TILES)):
        place_tile(img, box)
        if render:
            show_image(img, wait=False)
            sleep(0.025)

    return img


# main
def main():
    if len(sys.argv) > 1:
        image_path = sys.argv[1]
    else:
        image_path = conf.IMAGE_TO_TILE

    if len(sys.argv) > 2:
        tiles_paths = sys.argv[2:]
    else:
        tiles_paths = conf.TILES_FOLDER.split(' ')

    if not os.path.exists(image_path):
        print('Image not found')
        exit(-1)
    for path in tiles_paths:
        if not os.path.exists(path):
            print('Tiles folder not found')
            exit(-1)

    tiles = load_tiles(tiles_paths)
    boxes, original_res = get_processed_image_boxes(image_path, tiles)
    img = create_tiled_image(boxes, original_res, render=conf.RENDER)
    cv2.imwrite(conf.OUT, img)


if __name__ == ""__main__"":
    main()
"
Tiler,gen_tiles.py,"import cv2 
import numpy as np
import os
import sys
from tqdm import tqdm
import math
import conf

# DEPTH = 4 -> 4 * 4 * 4 = 64 colors
DEPTH = conf.DEPTH
# list of rotations, in degrees, to apply over the original image
ROTATIONS = conf.ROTATIONS

img_path = sys.argv[1]
img_dir = os.path.dirname(img_path)
img_name, ext = os.path.basename(img_path).rsplit('.', 1)
out_folder = img_dir + '/gen_' + img_name

if not os.path.exists(out_folder):
    os.mkdir(out_folder)

img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)
img = img.astype('float')

height, width, channels = img.shape
center = (width/2, height/2)

for b in tqdm(np.arange(0, 1.01, 1 / DEPTH)):
    for g in np.arange(0, 1.01, 1 / DEPTH):
        for r in np.arange(0, 1.01, 1 / DEPTH):
            mult_vector = [b, g, r]
            if channels == 4:
                mult_vector.append(1)
            new_img = img * mult_vector
            new_img = new_img.astype('uint8')
            for rotation in ROTATIONS:
                rotation_matrix = cv2.getRotationMatrix2D(center, rotation, 1)
                abs_cos = abs(rotation_matrix[0,0])
                abs_sin = abs(rotation_matrix[0,1])
                new_w = int(height * abs_sin + width * abs_cos)
                new_h = int(height * abs_cos + width * abs_sin)
                rotation_matrix[0, 2] += new_w/2 - center[0]
                rotation_matrix[1, 2] += new_h/2 - center[1]
                cv2.imwrite(
                    f'{out_folder}/{img_name}_{round(r,1)}_{round(g,1)}_{round(b,1)}_r{rotation}.{ext}',
                    cv2.warpAffine(new_img, rotation_matrix, (new_w, new_h)),
                    # compress image
                    [cv2.IMWRITE_PNG_COMPRESSION, 9])
"
InLong(TubeMQ),get-project-version.py,"#!/usr/bin/env python
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

import xml.etree.ElementTree as ET
from os.path import dirname, realpath, join

# Derive the POM path from the current script location
TOP_LEVEL_PATH = dirname(dirname(realpath(__file__)))
POM_PATH = join(TOP_LEVEL_PATH, 'pom.xml')

root = ET.XML(open(POM_PATH).read())
print(root.find('{http://maven.apache.org/POM/4.0.0}version').text)
"
InLong(TubeMQ),__init__.py,"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
"
InLong(TubeMQ),setup.py,"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

from pybind11.setup_helpers import Pybind11Extension, build_ext
from glob import glob
from setuptools import setup

import sys

__version__ = ""0.0.1""

# The main interface is through Pybind11Extension.
# the include dir of TubeMQ C++ installed in /usr/local/include/
# include_dirs=[""/usr/local/include/""], runtime_library_dirs=[""/usr/local/lib""],
# yum install python-devel

extra_link_args = [""-ltubemq"", ""-ltubemq_proto"", ""-Wl,-Bstatic"",
    ""-lprotobuf"", ""-Wl,-Bdynamic"", ""-llog4cplus"", ""-lssl"",
    ""-lcrypto"", ""-lpthread"", ""-lrt""]

ext_modules = [
    Pybind11Extension(""tubemq_client"",
        sorted(glob(""src/cpp/tubemq_client.cc"")),
        cxx_std=11,
        extra_link_args=extra_link_args,
        define_macros=[('VERSION_INFO', __version__)],
        ),
    Pybind11Extension(""tubemq_config"",
                      sorted(glob(""src/cpp/tubemq_config.cc"")),
                      cxx_std=11,
                      extra_link_args=extra_link_args,
                      define_macros=[('VERSION_INFO', __version__)],
                      ),
    Pybind11Extension(""tubemq_errcode"",
                      sorted(glob(""src/cpp/tubemq_errcode.cc"")),
                      cxx_std=11,
                      extra_link_args=extra_link_args,
                      define_macros=[('VERSION_INFO', __version__)],
                      ),
    Pybind11Extension(""tubemq_message"",
                      sorted(glob(""src/cpp/tubemq_message.cc"")),
                      cxx_std=11,
                      extra_link_args=extra_link_args,
                      define_macros=[('VERSION_INFO', __version__)],
                      ),
    Pybind11Extension(""tubemq_return"",
                      sorted(glob(""src/cpp/tubemq_return.cc"")),
                      cxx_std=11,
                      extra_link_args=extra_link_args,
                      define_macros=[('VERSION_INFO', __version__)],
                      ),
    Pybind11Extension(""tubemq_tdmsg"",
                      sorted(glob(""src/cpp/tubemq_tdmsg.cc"")),
                      cxx_std=11,
                      extra_link_args=extra_link_args + [""-lsnappy""],
                      define_macros=[('VERSION_INFO', __version__)],
                      )
]

setup(
    name=""tubemq"",
    version=__version__,
    author=""dockerzhang"",
    author_email=""dockerzhang@apache.org"",
    url=""https://github.com/apache/inlong/tree/master/inlong-tubemq/tubemq-client-twins/tubemq-client-python"",
    description=""TubeMq Python SDK Client project built with pybind11"",
    long_description="""",
    ext_modules=ext_modules,
    extras_require={""test"": ""pytest""},
    cmdclass={""build_ext"": build_ext},
    zip_safe=False,
    packages=['tubemq'],
    package_dir={'tubemq': 'src/python/tubemq'},
    package_data={'tubemq': ['client.conf']}
)
"
InLong(TubeMQ),client.py,"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

from __future__ import print_function
import os
import tubemq_client
import tubemq_config
import tubemq_errcode
import tubemq_return
import tubemq_tdmsg  # pylint: disable=unused-import
import tubemq_message  # pylint: disable=unused-import


class Producer(tubemq_client.TubeMQProducer):
    def __init__(self, 
                 master_addr,
                 rpc_read_timeout_ms=20000,
                 conf_file=os.path.join(os.path.dirname(__file__), ""client.conf"")):
        super(Producer, self).__init__()

        producer_config = tubemq_config.ProducerConfig()
        producer_config.setRpcReadTimeoutMs(rpc_read_timeout_ms)
        
        err_info = """"
        result = producer_config.setMasterAddrInfo(err_info, master_addr)
        if not result:
            print(""Set Master AddrInfo failure:"", err_info)
            exit(1)
        
        result = tubemq_client.startTubeMQService(err_info, conf_file)
        if not result:
            print(""StartTubeMQService failure:"", err_info)
            exit(1)
        
        result = self.start(err_info, producer_config)
        if not result:
            print(""Initial producer failure, error is:"", err_info)
            exit(1)

    def publish(self, topic_list):
        if not isinstance(topic_list, (tuple, list, set, str)):
            raise TypeError(""Accepted types: `list`, `tuple`, `set` or `str`, get {}"".format(type(topic_list)))
        if isinstance(topic_list, (tuple, list)):
            topic_list = set(topic_list)
        elif isinstance(topic_list, str):
            topic_list = {topic_list}
        
        err_info = """"
        result = self.publishTopics(err_info, topic_list)
        if not result:
            print(""Python Producer push topics failed, error is:"", err_info)
            exit(1)

    def send(self, msg, is_sync=False, callback=None):
        if is_sync:
            err_info = """"
            result = self.sendMessage(err_info, msg)
            if not result:
                print(""Send Message failure, error is:"", err_info)
            return result
        else:
            if callback is None:
                raise ValueError(""The callback function should be provided when sending message async."")
            self.sendMessage(msg, callback)

    def stop(self):
        err_info = ''
        result = self.shutDown()
        result = tubemq_client.stopTubeMQService(err_info)
        if not result:
            print(""StopTubeMQService failure, reason is:"" + err_info)
            exit(1)


class Consumer(tubemq_client.TubeMQConsumer):
    def __init__(self,
                 master_addr,
                 group_name,
                 topic_list,
                 rpc_read_timeout_ms=20000,
                 consume_osition=tubemq_config.ConsumePosition.kConsumeFromLatestOffset,
                 conf_file=os.path.join(os.path.dirname(__file__), 'client.conf')):

        super(Consumer, self).__init__()

        consumer_config = tubemq_config.ConsumerConfig()
        consumer_config.setRpcReadTimeoutMs(rpc_read_timeout_ms)
        consumer_config.setConsumePosition(consume_osition)

        err_info = ''
        result = consumer_config.setMasterAddrInfo(err_info, master_addr)
        if not result:
            print(""Set Master AddrInfo failure:"", err_info)
            exit(1)

        result = consumer_config.setGroupConsumeTarget(err_info, group_name, topic_list)
        if not result:
            print(""Set GroupConsume Target failure:"", err_info)
            exit(1)

        result = tubemq_client.startTubeMQService(err_info, conf_file)
        if not result:
            print(""StartTubeMQService failure:"", err_info)
            exit(1)

        result = self.start(err_info, consumer_config)
        if not result:
            print(""Initial consumer failure, error is:"", err_info)
            exit(1)

        self.getRet = tubemq_return.ConsumerResult()
        self.confirm_result = tubemq_return.ConsumerResult()

    def receive(self):
        result = self.getMessage(self.getRet)
        if result:
            return self.getRet.getMessageList()
        else:
            # 2.2.1 if failure, check error code
            print('GetMessage failure, err_code=%d, err_msg is: %s' 
                % (self.getRet.getErrCode(), self.getRet.getErrMessage()))

    def acknowledge(self):
        self.confirm(self.getRet.getConfirmContext(), True, self.confirm_result)

    def stop(self):
        err_info = ''
        result = self.shutDown()
        result = tubemq_client.stopTubeMQService(err_info)
        if not result:
            print(""StopTubeMQService failure, reason is:"" + err_info)
            exit(1)
"
InLong(TubeMQ),tube_msg.py,"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

from __future__ import print_function
import tubemq_tdmsg


class TubeMsg:
    def __init__(self, header, data):
        self.header = header
        self.data = data

    def __str__(self):
        return 'TubeMsg (%d, %d)' % (self.header, self.data)

    @staticmethod
    def parse_tdmsg_type_msg(data):
        err_info = ''
        tube_tdmsg = tubemq_tdmsg.TubeMQTDMsg()
        parsedMsgs = []
        if tube_tdmsg.parseTDMsg(data, err_info):
            tube_datamap = tube_tdmsg.getAttr2DataMap()
            for parsed_data_val in tube_datamap.values():
                for data_it in parsed_data_val:
                    parsedMsgs.append(data_it.getData())
        else:
            print(err_info)
        return parsedMsgs
"
InLong(TubeMQ),__init__.py,"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

from .client import Consumer
from .client import Producer
from .tube_msg import TubeMsg
"
InLong(TubeMQ),test_producer.py,"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

import tubemq
import tubemq_message
import tubemq_errcode
import argparse
import time
import datetime

from threading import Lock 

kTotalCounter = 0
kSuccessCounter = 0
kFailCounter = 0
counter_lock = Lock()


# Reference: java producer: MixedUtils.buildTestData, only for demo
def build_test_data(msg_data_size):
    transmit_data = ""This is a test data!""
    data = """"
    while (len(data) + len(transmit_data)) <= msg_data_size:
        data += transmit_data
    if len(data) < msg_data_size:
        data += transmit_data[:msg_data_size - len(data)]
    return data


def send_callback(error_code):
    global counter_lock
    global kTotalCounter
    global kSuccessCounter
    global kFailCounter
    with counter_lock:
        kTotalCounter += 1
        if error_code == tubemq_errcode.kErrSuccess:
            kSuccessCounter += 1
        else:
            kFailCounter += 1

parser = argparse.ArgumentParser()
parser.add_argument(""--master_servers"", type=str, required=True,
                    help=""The master address(es) to connect to, the format is master1_ip:port[,master2_ip:port]"")
parser.add_argument(""--topics"", type=str, required=True, help=""The topic names."")
parser.add_argument(""--conf_file"", type=str, default=""/tubemq-python/src/python/tubemq/client.conf"", 
                    help=""The path of configuration file."")
parser.add_argument(""--sync_produce"", type=int, default=0, help=""Whether synchronous production."")
parser.add_argument(""--msg_count"", type=int, default=10, help=""The number of messages to send."")
parser.add_argument(""--msg_data_size"", type=int, default=1000, help=""The message size, (0, 1024 * 1024) bytes."")

params = parser.parse_args()

# start producer
producer = tubemq.Producer(params.master_servers)
producer.publish(params.topics)

# wait for the first heartbeath to master ready
time.sleep(10)

send_data = build_test_data(params.msg_data_size)
curr_time = datetime.datetime.now().strftime(""%Y%m%d%H%M"")
# for test, only take the first topic
first_topic = params.topics if isinstance(params.topics, str) else params.topics.split("","")[0]

t0 = time.time()
for i in range(params.msg_count):
    msg = tubemq_message.Message(first_topic, send_data, len(send_data))
    msg.putSystemHeader(str(i), curr_time)
    if params.sync_produce:
        res = producer.send(msg, is_sync=True)
        kTotalCounter += 1
        if res:
            kSuccessCounter += 1
        else:
            kFailCounter += 1
    else:
        producer.send(msg, callback=send_callback)

while kTotalCounter < params.msg_count:
    time.sleep(1e-6)

t1 = time.time()
print(""Python producer send costs {} seconds."".format(t1 - t0))

# Stop producer
producer.stop()
 "
InLong(TubeMQ),test_consumer.py,"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

import time
import tubemq

topic_list = {'test_topic'}  # consum all of topic
# topic_list = {'test_topic': {'test_tid1', 'test_tid2'}}  # filter by tids
MASTER_ADDR = '127.0.0.1:8000'
GROUP_NAME = 'test_group'

# Start consumer
consumer = tubemq.Consumer(MASTER_ADDR, GROUP_NAME, topic_list)

# Test consumer
start_time = time.time()
while True:
    msgs = consumer.receive()
    if msgs:
        print(""GetMessage success, msssage count ="", len(msgs))
    consumer.acknowledge()
    # used for test, consume 10 minutes only
    stop_time = time.time()
    if stop_time - start_time > 10 * 60:
        break

# Stop consumer
consumer.stop()
"
InLong(TubeMQ),test_tdmsg.py,"#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

from __future__ import unicode_literals
import time
import tubemq_config
import tubemq

topic_list = {'test_topic'}  # consum all of topic
# topic_list = {'test_topic': {'test_tid1', 'test_tid2'}}  # filter by tids
MASTER_ADDR = '127.0.0.1:8000'
GROUP_NAME = 'test_group'

# Start consumer
consumer = tubemq.Consumer(MASTER_ADDR, GROUP_NAME, topic_list,
                           consume_osition=tubemq_config.ConsumePosition.kConsumeFromFirstOffset)

# Test consumer
start_time = time.time()
tubeMsgsArr = []
while True:
    messageList = consumer.receive()
    if messageList:
        print(""GetMessage success, msssage count ="", len(messageList))
        for message in messageList:
            attributeMap = message.getProperties()
            attribute = ''
            for (key, value) in attributeMap.items():
                attribute = attribute + key + '=' + value + ','
            attribute = attribute[:-1]
            print(""tube msg attribute "", attribute)
            data = message.getMsgData()
            decodeMsgs = tubemq.TubeMsg.parse_tdmsg_type_msg(message.getMsgData())
            for msgIter in decodeMsgs:
                tube_msg = tubemq.TubeMsg(attribute, msgIter)
                tubeMsgsArr.append(tube_msg)
                print(""tube msg msgIter "", msgIter)

    consumer.acknowledge()

    # used for test, consume 10 minutes only
    stop_time = time.time()
    if stop_time - start_time > 10 * 60:
        break

# Stop consumer
consumer.stop()
"
DeepPrivacy,setup.py,"import torch
import torchvision
from setuptools import setup, find_packages
torch_ver = [int(x) for x in torch.__version__.split(""."")[:2]]
assert torch_ver >= [1, 7], ""Requires PyTorch >= 1.7""
torchvision_ver = [int(x) for x in torchvision.__version__.split(""."")[:2]]
assert torchvision_ver >= [0, 6], ""Requires torchvision >= 0.6""

setup(
    name='DeepPrivacy',
    version='0.1.0',
    packages=find_packages(),
    install_requires=[
        ""numpy"",
        ""cython"",
        ""scikit-learn>=0.2"",
        ""matplotlib"",
        ""tqdm"",
        ""tflib"",
        ""autopep8"",
        ""moviepy"",
        ""tensorboard"",
        ""opencv-python"",
        ""requests"",
        ""pyyaml"",
        ""scikit-image"",
        ""addict"",
        ""albumentations"",
        ""face_detection>=0.2.0""
    ]
)"
DeepPrivacy,webcam.py,"
import cv2
import time
import numpy as np
import torch
from deep_privacy import cli
from deep_privacy.visualization import utils as vis_utils
from deep_privacy.utils import BufferlessVideoCapture
from deep_privacy.build import build_anonymizer
import os
# Configs
torch.backends.cudnn.benchmark = False
parser = cli.get_parser()
parser.add_argument(""--debug"", default=False, action=""store_true"")
parser.add_argument(""-f"", ""--file"", default=None)
args = parser.parse_args()
anonymizer, cfg = build_anonymizer(
    args.model, opts=args.opts, config_path=args.config_path,
    return_cfg=True)
if args.debug:
    anonymizer.save_debug = True

width = 1280
height = 720

if args.file is not None:
    assert os.path.isfile(args.file)
    cap = cv2.VideoCapture(args.file)
else:
    cap = BufferlessVideoCapture(0)
frames = 0
WARMUP = True
t = time.time()
while True:
    # Capture frame-by-frame
    ret, frame = cap.read()
    frame = cv2.resize(frame, (width, height))
    frame = frame[:, :, ::-1]
    frame = anonymizer.detect_and_anonymize_images([frame])[0]
    frame = frame[:, :, ::-1]

    # Display the resulting frame
    if WARMUP and frames > 30:
        WARMUP = False
        t = time.time()
        frames = 0

    frames += 1
    delta = time.time() - t
    fps = ""?""
    if delta > 1e-6:
        fps = frames / delta
    print(f""FPS: {delta:.3f}"", end=""\r"")
    if args.debug:
        debug_im = cv2.imread("".debug/inference/im0_face0.png"")
        debug_im = vis_utils.pad_im_as(debug_im, frame)
        frame = np.concatenate((frame, debug_im))
    cv2.imshow('frame',frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# When everything done, release the capture
cap.release()
cv2.destroyAllWindows()
"
DeepPrivacy,train.py,"import torch
from deep_privacy import config
from deep_privacy.engine import Trainer, ProgressiveTrainer
# Debug
parser = config.default_parser()
parser.add_argument(
    ""--debug"", default=False, action=""store_true"")
args = parser.parse_args()

if args.debug:
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    torch.set_printoptions(precision=10)

cfg = config.Config.fromfile(args.config_path)
cfg.dump()
if cfg.trainer.progressive.enabled:
    trainer = ProgressiveTrainer(cfg)
else:
    trainer = Trainer(cfg)
trainer.train()
"
DeepPrivacy,anonymize.py,"from deep_privacy import cli


if __name__ == ""__main__"":
    cli.main()"
DeepPrivacy,mask_infer.py,"import pathlib
import numpy as np
import typing
import cv2
import time
from deep_privacy import utils, file_util
from deep_privacy.config import Config,  default_parser
from deep_privacy.inference import infer, inpaint_inference
import torch

torch.manual_seed(0)
np.random.seed(0)
torch.backends.cudnn.benchmark = True


def get_paths(args):
    image_path = pathlib.Path(args.image_path)
    mask_path = args.mask_path
    single_file = not image_path.is_dir()

    if single_file:
        image_paths = [image_path]
        if mask_path is None:
            filename = image_path.stem + ""_mask"" + image_path.suffix
            mask_path = image_path.parent.joinpath(filename)
            assert mask_path.is_file(),\
                f""Did not find mask at location: {mask_path}""
        else:
            mask_path = pathlib.Path(mask_path)
        mask_paths = [mask_path]
    else:
        image_paths = file_util.find_all_files(image_path)
        if mask_path is None:
            mask_path = image_path.parent.joinpath(""masks"")
        mask_paths = file_util.find_matching_files(
            mask_path,
            image_paths)

    target_path = args.target_path
    if target_path is None:
        if single_file:
            filename = image_path.stem + ""_result"" + image_path.suffix
            target_path = image_path.parent.joinpath(filename)
            target_paths = [target_path]
        else:
            target_path = image_path.parent.joinpath(""result"")
            target_path.mkdir(exist_ok=True, parents=True)
            target_paths = []
            for impath in image_paths:
                target_paths.append(
                    target_path.joinpath(impath.name)
                )
    else:
        target_paths = [pathlib.Path(target_path)]
    return image_paths, mask_paths, target_paths


def is_same_shape(images: typing.List[np.ndarray]):
    shape1 = images[0].shape
    for im in images:
        if im.shape != shape1:
            return False
    return True



if __name__ == ""__main__"":
    parser = default_parser()
    parser.add_argument(
        ""-i"", ""--image_path"", default=""data/validation_datasets/celebA-HQ"",
    )
    parser.add_argument(
        ""-m"", ""--mask_path"", default=None,
        help=""Path to mask dir/file. Sets the default to _mask file or image_path/../mask""
    )
    parser.add_argument(
        ""-t"", ""--target_path"", default=None
    )
    parser.add_argument(
        ""--step"", default=None,
        type=int,
        help=""Load a specific step from the validation checkpoint dir""
    )
    parser.add_argument(
        ""--batch_size"", default=None,
        type=int,
        help=""Batch size for generator""
    )
    args = parser.parse_args()
    cfg = Config.fromfile(args.config_path)
    generator = infer.load_model_from_checkpoint(
        cfg,
        args.step
    )
    image_paths, mask_paths, target_paths = get_paths(args)
    assert len(image_paths) > 0, f""found no images in {args.image_path}""

    images, masks = file_util.read_mask_images(
        image_paths, mask_paths, generator.current_imsize)
    start = time.time()
    inpainted_images, inputs = inpaint_inference.inpaint_images(
        images, masks, generator)
    tot_time = time.time() - start
    avg_time = tot_time / inpainted_images.shape[0]
    fps = inpainted_images.shape[0] / tot_time
    print(f""Inpainted {inpainted_images.shape[0]} in {tot_time:.2f} seconds. FPS: {fps}, Average time: {avg_time}"")
    for (image, masked_out), target_path in zip(zip(inpainted_images, inputs), target_paths):
        image = image
        cv2.imwrite(
            str(target_path),
            image[:, :, ::-1])
#        print(""Saving to:"", target_path)
        input_im_path = target_path.parent.parent.joinpath(""masked_out"")
        input_im_path.mkdir(exist_ok=True)
        input_im_path = input_im_path.joinpath(target_path.name)
        cv2.imwrite(str(input_im_path), (masked_out*255)[:, :, ::-1].astype(np.uint8))"
DeepPrivacy,places2.py,"import os

_base_config_ = ""defaults.py""

models = dict(
    pose_size=0,
    max_imsize=256
)

dataset_type = ""Places2Dataset""
data_root = os.path.join(""data"", ""places2"")
data_train = dict(
    dataset=dict(
        type=dataset_type,
        dirpath=os.path.join(data_root, ""train""),
        percentage=1.0,
        is_train=True
    ),
    transforms=[
        dict(type=""RandomFlip"", flip_ratio=0.5),
        dict(type=""RandomCrop"")
    ],
)
data_val = dict(
    dataset=dict(
        type=dataset_type,
        dirpath=os.path.join(data_root, ""val""),
        percentage=.137, #5000 images out of 36500
        is_train=False
    ),
    transforms=[
        dict(type=""CenterCrop"")
    ],
)
"
DeepPrivacy,defaults.py,"import os
# Default values
_output_dir = ""outputs""
_cache_dir = "".deep_privacy_cache""

model_size = 128
model_url = None
models = dict(
    max_imsize=128,
    min_imsize=8,
    pose_size=14,
    image_channels=3,
    conv_size={
        4: model_size,
        8: model_size,
        16: model_size,
        32: model_size,
        64: model_size//2,
        128: model_size//4,
        256: model_size//8,
        512: model_size//16
    },
    generator=dict(
        scalar_pose_input=False,
        type=""Generator"",
        unet=dict(
            enabled=True,
            residual=False
        ),
        min_fmap_resolution=4,
        use_skip=True,
        z_shape=(32, 4, 4),
        conv2d_config=dict(
            pixel_normalization=True,
            leaky_relu_nslope=.2,
            normalization=""pixel_wise"",
            conv=dict(
                type=""conv"",
                wsconv=True,
                gain=1,
            )
        ),
        residual=False,
    ),
    discriminator=dict(
        type=""Discriminator"",
        residual=False,
        scalar_pose_input=False,
        scalar_pose_input_imsize=32,
        min_fmap_resolution=4,
        conv_multiplier=1,
        conv2d_config=dict(
            leaky_relu_nslope=.2,
            normalization=None,
            conv=dict(
                type=""conv"",
                wsconv=True,
                gain=1,
            )
        ),
    )
)

trainer = dict(
    hooks=[
        dict(type=""RunningAverageHook""),
        dict(
            type=""CheckpointHook"",
            ims_per_checkpoint=2e5
        ),
        dict(type=""SigTermHook""),
        dict(
            type=""ImageSaveHook"",
            ims_per_save=1e5,
            n_diverse_samples=5
        ),
        dict(
            type=""MetricHook"",
            ims_per_log=2e5,
            lpips_batch_size=128,
            fid_batch_size=8,
            min_imsize_to_calculate=128
        ),
        dict(
            type=""StatsLogger"",
            num_ims_per_log=500
        )
    ],
    progressive=dict(
        transition_iters=12e5,
        minibatch_repeats=4,
        enabled=True
    ),
    batch_size_schedule={
        4: 256,
        4: 256,
        8: 256,
        16: 256,
        32: 128,
        64: 96,
        128: 32,
        256: 32
    },
    optimizer=dict(
        learning_rate=0.001,
        amp_opt_level=""O1"",
        lazy_regularization=True
    )
)

adversarial_loss = ""WGANCriterion""
discriminator_criterions = {
    0: dict(
        type=adversarial_loss,
        fake_index=-1
    ),
    1: dict(
        type=""GradientPenalty"",
        lambd=10,
        mask_region_only=True,
        norm=""L2"",
        distance=""clamp"",
        lazy_reg_interval=16,
        mask_decoder_gradient=False,
        fake_index=-1
    ),
    2: dict(
        type=""EpsilonPenalty"",
        weight=0.001,
        fake_index=-1
    )
}
generator_criterions = {
    0: dict(
        type=adversarial_loss,
        fake_index=-1
    )
}

## DATASETS
dataset_type = ""FDFDataset""
data_root = os.path.join(""data"", ""fdf"")
data_train = dict(
    dataset=dict(
        type=dataset_type,
        dirpath=os.path.join(data_root, ""train""),
        percentage=1.0
    ),
    transforms=[
        dict(type=""RandomFlip"", flip_ratio=0.5),
        dict(type=""FlattenLandmark"")
    ],
    loader=dict(
        shuffle=True,
        num_workers=16,
        drop_last=True
    )
)
data_val = dict(
    dataset=dict(
        type=dataset_type,
        dirpath=os.path.join(data_root, ""val""),
        percentage=0.2
    ),
    transforms=[
        dict(type=""FlattenLandmark"")
    ],
    loader=dict(
        shuffle=False,
        num_workers=16,
        drop_last=True
    )
)

anonymizer = dict(
    truncation_level=0,
    save_debug=False,
    batch_size=1,
    fp16_inference=False,
    jit_trace=False,
    detector_cfg=dict(
        type=""BaseDetector"",
        keypoint_threshold=.2,
        densepose_threshold=.3,
        simple_expand=True,
        align_faces=False,
        resize_background=True,
        face_detector_cfg=dict(
            name=""RetinaNetResNet50"",
            confidence_threshold=.3,
            nms_iou_threshold=.3,
            max_resolution=1080,
            fp16_inference=True,
            clip_boxes=True
        )
    )
)
"
DeepPrivacy,celebA-HQ.py,"import os

_base_config_ = ""defaults.py""

models = dict(
    pose_size=0
)

dataset_type = ""CelebAHQDataset""
data_root = os.path.join(""data"", ""celebA-HQ"")
data_train = dict(
    dataset=dict(
        type=dataset_type,
        dirpath=os.path.join(data_root, ""train""),
        percentage=1.0,
        is_train=True
    ),
    transforms=[
        dict(type=""RandomFlip"", flip_ratio=0.5),
    ],
)
data_val = dict(
    dataset=dict(
        type=dataset_type,
        dirpath=os.path.join(data_root, ""val""),
        percentage=.2,
        is_train=False
    ),
    transforms=[
    ],
)
"
DeepPrivacy,deep_privacy_v1.py,"
_base_config_ = ""base.py""

model_size = 512

model_url = ""http://folk.ntnu.no/haakohu/checkpoints/step_42000000.ckpt""

models = dict(
    scalar_pose_input=False,
    max_imsize=128,
    conv_size={
        4: model_size,
        8: model_size,
        16: model_size,
        32: model_size,
        64: model_size//2,
        128: model_size//4,
        256: model_size//8,
        512: model_size//16
    },

    generator=dict(
        conv2d_config=dict(
            conv=dict(
                gain=2**0.5
            )
        ),
        type=""DeepPrivacyV1""),
)
trainer = dict(
    progressive=dict(
        enabled=False,
        lazy_regularization=True
    ),
    batch_size_schedule={
        128: 32,
        256: 32
    },
    optimizer=dict(
        learning_rate=0.0015
    )
)
"
DeepPrivacy,512.py,"
_base_config_ = ""base.py""
model_size = 512
model_url = ""http://folk.ntnu.no/haakohu/checkpoints/fdf128_model512.ckpt""
models = dict(
    scalar_pose_input=True,
    conv_size={
        4: model_size,
        8: model_size,
        16: model_size,
        32: model_size,
        64: model_size//2,
        128: model_size//4,
        256: model_size//8,
        512: model_size//16
    },
    generator=dict(
        type=""MSGGenerator""),
    discriminator=dict(
        residual=True,
        scalar_pose_input=False
    )
)
trainer = dict(
    progressive=dict(
        enabled=False
    ),
    batch_size_schedule={
        256: 32
    },
    optimizer=dict(
        learning_rate=0.0015
    )
)
"
DeepPrivacy,base.py,"import os

_base_config_ = ""../defaults.py""

dataset_type = ""FDFDataset""
data_root = os.path.join(""data"", ""fdf"")
data_train = dict(
    dataset=dict(
        type=dataset_type,
        dirpath=os.path.join(data_root, ""train""),
        percentage=1.0
    ),
    transforms=[
        dict(type=""RandomFlip"", flip_ratio=0.5),
        dict(type=""FlattenLandmark"")
    ],
)
data_val = dict(
    dataset=dict(
        type=dataset_type,
        dirpath=os.path.join(data_root, ""val""),
        percentage=.2
    ),
    transforms=[
        dict(type=""FlattenLandmark"")
    ],
)

landmarks = [
    ""Nose"",
    ""Left Eye"",
    ""Right Eye"",
    ""Left Ear"",
    ""Right Ear"",
    ""Left Shoulder"",
    ""Right Shoulder""
]
anonymizer = dict(
    detector_cfg=dict(
        type=""RCNNDetector"",
        simple_expand=False,
        rcnn_batch_size=8,
        face_detector_cfg=dict(
            name=""RetinaNetResNet50"",
        )
    )
)"
DeepPrivacy,128.py,"import os

_base_config_ = ""base.py""
model_url = ""http://folk.ntnu.no/haakohu/checkpoints/fdf/retinanet128.ckpt""

models = dict(
    generator=dict(
        type=""MSGGenerator"",
        scalar_pose_input=True
    ),
    discriminator=dict(
        residual=True
    )
)

trainer = dict(
    progressive=dict(
        enabled=False,
    )
)"
DeepPrivacy,256.py,"import os

_base_config_ = ""base.py""
model_url = ""http://folk.ntnu.no/haakohu/checkpoints/fdf/retinanet256.ckpt""

model_size=256
models = dict(
    conv_size={
        4: model_size,
        8: model_size,
        16: model_size,
        32: model_size,
        64: model_size//2,
        128: model_size//4,
        256: model_size//8,
        512: model_size//16
    },
    generator=dict(
        residual=True,
        scalar_pose_input=True
    ),
    discriminator=dict(
        residual=True
    )
)

trainer = dict(
    progressive=dict(
        enabled=False,
    ),
)"
DeepPrivacy,512.py,"import os

_base_config_ = ""base.py""
model_url = ""http://folk.ntnu.no/haakohu/checkpoints/fdf/retinanet512.ckpt""

model_size=512
models = dict(
    conv_size={
        4: model_size,
        8: model_size,
        16: model_size,
        32: model_size,
        64: model_size//2,
        128: model_size//4,
        256: model_size//8,
        512: model_size//16
    },
    generator=dict(
        residual=True,
        scalar_pose_input=True
    ),
    discriminator=dict(
        residual=True
    )
)

trainer = dict(
    max_images_to_train=20e6,
    progressive=dict(
        enabled=False,
    ),
)"
DeepPrivacy,celebA256.py,"
_base_config_ = ""../celebA-HQ.py""
model_size=256
models = dict(
    max_imsize=256,
    conv_size={
        4: model_size,
        8: model_size,
        16: model_size,
        32: model_size,
        64: model_size//2,
        128: model_size//4,
        256: model_size//8,
        512: model_size//16
    },

    pose_size=0,
    generator=dict(
        type=""MSGGenerator"",
        conv2d_config=dict(
            conv=dict(type=""iconv"")
            )
    ),
    discriminator=dict(
        residual=True,
        scalar_pose_input=False
    )
)
trainer = dict(
    progressive=dict(
        enabled=False
    ),
    batch_size_schedule={
        256: 32
    },
    optimizer=dict(
        learning_rate=0.0015
    )
)
"
DeepPrivacy,places_B.py,"
_base_config_ = ""../../places2.py""
"
DeepPrivacy,places_E_conv.py,"
_base_config_ = ""../../places2.py""
models = dict(
    generator=dict(
        type=""MSGGenerator"",
        conv2d_config=dict(
        )
    ),
    discriminator=dict(
        residual=True,
        scalar_pose_input=False
    )
)
trainer = dict(
    progressive=dict(
        enabled=False
    ),
)"
DeepPrivacy,places_D.py,"
_base_config_ = ""../../places2.py""
models = dict(
    scalar_pose_input=False,
    max_imsize=256,
    pose_size=0,
    generator=dict(
        conv2d_config=dict(
            conv=dict(
                type=""iconv""
            )
        )
    ),
)"
DeepPrivacy,places256.py,"
_base_config_ = ""../../places2.py""
model_size = 256
models = dict(
    scalar_pose_input=False,
    max_imsize=256,
    conv_size={
        4: model_size,
        8: model_size,
        16: model_size,
        32: model_size,
        64: model_size//2,
        128: model_size//4,
        256: model_size//8,
        512: model_size//16
    },
    pose_size=0,
    generator=dict(
        type=""MSGGenerator"",
        conv2d_config=dict(
            conv=dict(
                type=""iconv""
            )
        )
    ),
    discriminator=dict(
        residual=True,
        scalar_pose_input=False
    )
)
trainer = dict(
    max_images_to_train=20e6,
    progressive=dict(
        enabled=False
    ),
    batch_size_schedule={
        256: 32
    },
    optimizer=dict(
        learning_rate=0.0015
    )
)
"
DeepPrivacy,places_A.py,"
_base_config_ = ""../../places2.py""

adversarial_loss = ""WGANCriterion""
discriminator_criterions = [
    dict(
        type=adversarial_loss,
        fake_index=-1
    ),
    dict(
        type=""GradientPenalty"",
        lambd=10,
        mask_region_only=False,
        norm=""L2"",
        distance=""L2"",
        lazy_reg_interval=16,
        mask_decoder_gradient=False,
        fake_index=-1
    ),
    dict(
        type=""EpsilonPenalty"",
        weight=0.001,
        fake_index=-1
    )
]
"
DeepPrivacy,gcpr_D.py,"
_base_config_ = ""../../fdf/base.py""

models = dict(
    generator=dict(
        scalar_pose_input=True,
        conv2d_config=dict(
            conv=dict(
                type=""iconv""
            )
        )
    ),
)
trainer = dict(
    progressive=dict(
        enabled=True
    )
)
"
DeepPrivacy,gcpr_A.py,"_base_config_ = ""../../fdf/base.py""

adversarial_loss = ""WGANCriterion""
discriminator_criterions = [
    dict(
        type=adversarial_loss,
        fake_index=-1
    ),
    dict(
        type=""GradientPenalty"",
        lambd=10,
        mask_region_only=False,
        norm=""L2"",
        distance=""L2"",
        lazy_reg_interval=16,
        mask_decoder_gradient=False,
        fake_index=-1
    ),
    dict(
        type=""EpsilonPenalty"",
        weight=0.001,
        fake_index=-1
    )
]
"
DeepPrivacy,gcpr_E.py,"
_base_config_ = ""../../fdf/base.py""

models = dict(
    generator=dict(
        type=""MSGGenerator"",
        scalar_pose_input=True,
        conv2d_config=dict(
            conv=dict(
                type=""iconv""
            )
        )
    ),
    discriminator=dict(
        residual=True,
        scalar_pose_input=True,
        scalar_pose_imsize=64
    )
)
trainer = dict(
    progressive=dict(
        enabled=False
    )
)
"
DeepPrivacy,gcpr_B.py,"
_base_config_ = ""../../fdf/base.py""
"
DeepPrivacy,gcpr_C.py,"
_base_config_ = ""../../fdf/base.py""

models = dict(
    generator=dict(
        scalar_pose_input=True,
    )
)"
DeepPrivacy,build.py,"import pathlib
import torch
import os
from urllib.parse import urlparse
from deep_privacy import logger, torch_utils
from deep_privacy.config import Config
from deep_privacy.inference.infer import load_model_from_checkpoint
from deep_privacy.inference.deep_privacy_anonymizer import DeepPrivacyAnonymizer

available_models = [
    ""fdf128_rcnn512"",
    ""fdf128_retinanet512"",
    ""fdf128_retinanet256"",
    ""fdf128_retinanet128"",
    ""deep_privacy_V1"",

]

config_urls = {
    ""fdf128_retinanet512"": ""https://folk.ntnu.no/haakohu/configs/fdf/retinanet512.json"",
    ""fdf128_retinanet256"": ""https://folk.ntnu.no/haakohu/configs/fdf/retinanet256.json"",
    ""fdf128_retinanet128"": ""https://folk.ntnu.no/haakohu/configs/fdf/retinanet128.json"",
    ""fdf128_rcnn512"": ""https://folk.ntnu.no/haakohu/configs/fdf_512.json"",
    ""deep_privacy_V1"": ""https://folk.ntnu.no/haakohu/configs/deep_privacy_v1.json"",
}


def get_config(config_url):
    parts = urlparse(config_url)
    cfg_name = os.path.basename(parts.path)
    assert cfg_name is not None
    cfg_path = pathlib.Path(
        torch.hub._get_torch_home(), ""deep_privacy_cache"", cfg_name)
    cfg_path.parent.mkdir(exist_ok=True, parents=True)
    if not cfg_path.is_file():
        torch.hub.download_url_to_file(config_url, cfg_path)
    assert cfg_path.is_file()
    return Config.fromfile(cfg_path)


def build_anonymizer(
        model_name=available_models[0],
        batch_size: int = 1,
        fp16_inference: bool = True,
        truncation_level: float = 0,
        detection_threshold: float = .1,
        opts: str = None,
        config_path: str = None,
        return_cfg=False) -> DeepPrivacyAnonymizer:
    """"""
        Builds anonymizer with detector and generator from checkpoints.

        Args:
            config_path: If not None, will override model_name
            opts: if not None, can override default settings. For example:
                opts=""anonymizer.truncation_level=5, anonymizer.batch_size=32""
    """"""
    if config_path is None:
        print(config_path)
        assert model_name in available_models,\
            f""{model_name} not in available models: {available_models}""
        cfg = get_config(config_urls[model_name])
    else:
        cfg = Config.fromfile(config_path)
    logger.info(""Loaded model:"" + cfg.model_name)
    generator = load_model_from_checkpoint(cfg)
    logger.info(f""Generator initialized with {torch_utils.number_of_parameters(generator)/1e6:.2f}M parameters"")
    cfg.anonymizer.truncation_level = truncation_level
    cfg.anonymizer.batch_size = batch_size
    cfg.anonymizer.fp16_inference = fp16_inference
    cfg.anonymizer.detector_cfg.face_detector_cfg.confidence_threshold = detection_threshold
    cfg.merge_from_str(opts)
    anonymizer = DeepPrivacyAnonymizer(generator, cfg=cfg, **cfg.anonymizer)
    if return_cfg:
        return anonymizer, cfg
    return anonymizer
"
DeepPrivacy,__init__.py,"from .build import build_anonymizer
"
DeepPrivacy,file_util.py,"import pathlib
import typing
import numpy as np
from . import utils


def find_all_files(directory: pathlib.Path,
                   suffixes=[""png"", ""jpg"", ""jpeg""]
                   ) -> typing.List[pathlib.Path]:
    image_paths = []
    for suffix in suffixes:
        image_paths.extend(
            directory.glob(f""*.{suffix}"")
        )
    image_paths.sort()
    return image_paths


def find_matching_files(new_directory: pathlib.Path,
                        filepaths: typing.List[pathlib.Path]
                        ) -> typing.List[pathlib.Path]:
    new_files = []
    for impath in filepaths:
        mpath = new_directory.joinpath(impath.name)
        assert mpath.is_file(), f""Did not find path: {mpath}""
        new_files.append(mpath)
    assert len(new_files) == len(filepaths)
    return new_files


def read_images(filepaths: typing.List[pathlib.Path]) -> np.ndarray:
    im0 = utils.read_im(filepaths[0])
    images = np.zeros((len(filepaths), *im0.shape), dtype=im0.dtype)
    for idx, impath in enumerate(filepaths):
        images[idx] = utils.read_im(impath)
    return images


def _is_same_shape(images: typing.List[np.ndarray]):
    shape1 = images[0].shape
    for im in images:
        if im.shape != shape1:
            return False
    return True


def read_mask_images(image_paths: typing.List[pathlib.Path],
                     mask_paths: typing.List[pathlib.Path],
                     imsize: int):
    images = [utils.read_im(impath, imsize) for impath in image_paths]
    masks = [utils.read_im(impath, imsize) for impath in mask_paths]
    if _is_same_shape(images):
        images = np.concatenate([im[None] for im in images], axis=0)
        masks = np.concatenate([im[None] for im in masks], axis=0)
    return images, masks
"
DeepPrivacy,logger.py,"import torchvision
import pathlib
import math
import logging
from . import torch_utils
from torch.utils.tensorboard import SummaryWriter


writer = None
global_step = 0
image_dir = None
logFormatter = logging.Formatter(
    ""%(asctime)s [%(levelname)-5.5s]  %(message)s"")
rootLogger = logging.getLogger()
rootLogger.setLevel(logging.INFO)

consoleHandler = logging.StreamHandler()
consoleHandler.setFormatter(logFormatter)
rootLogger.addHandler(consoleHandler)


def init(output_dir):
    global writer, image_dir, rootLogger
    logdir = pathlib.Path(
        output_dir, ""summaries"")
    writer = SummaryWriter(logdir.joinpath(""train""))

    image_dir = pathlib.Path(output_dir, ""generated_data"")
    image_dir.joinpath(""validation"").mkdir(exist_ok=True, parents=True)
    image_dir.joinpath(""transition"").mkdir(exist_ok=True, parents=True)
    filepath = pathlib.Path(output_dir, ""train.log"")
    fileHandler = logging.FileHandler(filepath)
    fileHandler.setFormatter(logFormatter)
    rootLogger.addHandler(fileHandler)


def update_global_step(val):
    global global_step
    global_step = val


def log_variable(tag, value, log_to_validation=False, log_level=logging.DEBUG):
    if math.isnan(value):
        rootLogger.debug(f""Tried to log nan/inf for tag={tag}"")
        return
    value = float(value)
    rootLogger.log(log_level, f""{tag}: {value}"")
    assert not log_to_validation
    writer.add_scalar(tag, value, global_step=global_step)


def log_dictionary(dictionary: dict, log_to_validation=False):
    for key, item in dictionary.items():
        log_variable(key, item, log_to_validation=log_to_validation)


def save_images(tag, images,
                log_to_validation=False,
                log_to_writer=True,
                nrow=10,
                denormalize=False):
    if denormalize:
        images = torch_utils.denormalize_img(images)
    imsize = images.shape[2]
    imdir = image_dir
    if log_to_validation:
        imdir = image_dir.joinpath(""validation"")
    filename = ""{0}{1}_{2}x{2}.jpg"".format(tag, global_step, imsize)

    filepath = imdir.joinpath(filename)
    torchvision.utils.save_image(images, filepath, nrow=nrow)
    image_grid = torchvision.utils.make_grid(images, nrow=nrow)
    if log_to_writer:
        if log_to_validation:
            tag = f""validation/{tag}""
        else:
            tag = f""train/{tag}""
        writer.add_image(tag, image_grid, global_step)


def info(text):
    rootLogger.info(text)


def warn(text):
    rootLogger.warn(text)
"
DeepPrivacy,box_utils.py,"import numpy as np


def quadratic_bounding_box(x0, y0, width, height, imshape):
    # We assume that we can create a image that is quadratic without
    # minimizing any of the sides
    assert width <= min(imshape[:2])
    assert height <= min(imshape[:2])
    min_side = min(height, width)
    if height != width:
        side_diff = abs(height - width)
        # Want to extend the shortest side
        if min_side == height:
            # Vertical side
            height += side_diff
            if height > imshape[0]:
                # Take full frame, and shrink width
                y0 = 0
                height = imshape[0]

                side_diff = abs(height - width)
                width -= side_diff
                x0 += side_diff // 2
            else:
                y0 -= side_diff // 2
                y0 = max(0, y0)
        else:
            # Horizontal side
            width += side_diff
            if width > imshape[1]:
                # Take full frame width, and shrink height
                x0 = 0
                width = imshape[1]

                side_diff = abs(height - width)
                height -= side_diff
                y0 += side_diff // 2
            else:
                x0 -= side_diff // 2
                x0 = max(0, x0)
        # Check that bbox goes outside image
        x1 = x0 + width
        y1 = y0 + height
        if imshape[1] < x1:
            diff = x1 - imshape[1]
            x0 -= diff
        if imshape[0] < y1:
            diff = y1 - imshape[0]
            y0 -= diff
    assert x0 >= 0, ""Bounding box outside image.""
    assert y0 >= 0, ""Bounding box outside image.""
    assert x0 + width <= imshape[1], ""Bounding box outside image.""
    assert y0 + height <= imshape[0], ""Bounding box outside image.""
    return x0, y0, width, height


def expand_bounding_box(bbox, percentage, imshape):
    orig_bbox = bbox.copy()
    x0, y0, x1, y1 = bbox
    width = x1 - x0
    height = y1 - y0
    x0, y0, width, height = quadratic_bounding_box(
        x0, y0, width, height, imshape)
    expanding_factor = int(max(height, width) * percentage)

    possible_max_expansion = [(imshape[0] - width) // 2,
                              (imshape[1] - height) // 2,
                              expanding_factor]

    expanding_factor = min(possible_max_expansion)
    # Expand height

    if expanding_factor > 0:

        y0 = y0 - expanding_factor
        y0 = max(0, y0)

        height += expanding_factor * 2
        if height > imshape[0]:
            y0 -= (imshape[0] - height)
            height = imshape[0]

        if height + y0 > imshape[0]:
            y0 -= (height + y0 - imshape[0])

        # Expand width
        x0 = x0 - expanding_factor
        x0 = max(0, x0)

        width += expanding_factor * 2
        if width > imshape[1]:
            x0 -= (imshape[1] - width)
            width = imshape[1]

        if width + x0 > imshape[1]:
            x0 -= (width + x0 - imshape[1])
    y1 = y0 + height
    x1 = x0 + width
    assert y0 >= 0, ""Y0 is minus""
    assert height <= imshape[0], ""Height is larger than image.""
    assert x0 + width <= imshape[1]
    assert y0 + height <= imshape[0]
    assert width == height, ""HEIGHT IS NOT EQUAL WIDTH!!""
    assert x0 >= 0, ""Y0 is minus""
    assert width <= imshape[1], ""Height is larger than image.""
    # Check that original bbox is within new
    x0_o, y0_o, x1_o, y1_o = orig_bbox
    assert x0 <= x0_o, f""New bbox is outisde of original. O:{x0_o}, N: {x0}""
    assert x1 >= x1_o, f""New bbox is outisde of original. O:{x1_o}, N: {x1}""
    assert y0 <= y0_o, f""New bbox is outisde of original. O:{y0_o}, N: {y0}""
    assert y1 >= y1_o, f""New bbox is outisde of original. O:{y1_o}, N: {y1}""

    x0, y0, width, height = [int(_) for _ in [x0, y0, width, height]]
    x1 = x0 + width
    y1 = y0 + height
    return np.array([x0, y0, x1, y1])


def is_keypoint_within_bbox(x0, y0, x1, y1, keypoint):
    keypoint = keypoint[:, :3]  # only nose + eyes are relevant
    kp_X = keypoint[0, :]
    kp_Y = keypoint[1, :]
    within_X = np.all(kp_X >= x0) and np.all(kp_X <= x1)
    within_Y = np.all(kp_Y >= y0) and np.all(kp_Y <= y1)
    return within_X and within_Y


def expand_bbox_simple(bbox, percentage):
    x0, y0, x1, y1 = bbox.astype(float)
    width = x1 - x0
    height = y1 - y0
    x_c = int(x0) + width // 2
    y_c = int(y0) + height // 2
    avg_size = max(width, height)
    new_width = avg_size * (1 + percentage)
    x0 = x_c - new_width // 2
    y0 = y_c - new_width // 2
    x1 = x_c + new_width // 2
    y1 = y_c + new_width // 2
    return np.array([x0, y0, x1, y1]).astype(int)


def pad_image(im, bbox, pad_value):
    x0, y0, x1, y1 = bbox
    if x0 < 0:
        pad_im = np.zeros((im.shape[0], abs(x0), im.shape[2]),
                          dtype=np.uint8) + pad_value
        im = np.concatenate((pad_im, im), axis=1)
        x1 += abs(x0)
        x0 = 0
    if y0 < 0:
        pad_im = np.zeros((abs(y0), im.shape[1], im.shape[2]),
                          dtype=np.uint8) + pad_value
        im = np.concatenate((pad_im, im), axis=0)
        y1 += abs(y0)
        y0 = 0
    if x1 >= im.shape[1]:
        pad_im = np.zeros(
            (im.shape[0], x1 - im.shape[1] + 1, im.shape[2]),
            dtype=np.uint8) + pad_value
        im = np.concatenate((im, pad_im), axis=1)
    if y1 >= im.shape[0]:
        pad_im = np.zeros(
            (y1 - im.shape[0] + 1, im.shape[1], im.shape[2]),
            dtype=np.uint8) + pad_value
        im = np.concatenate((im, pad_im), axis=0)
    return im[y0:y1, x0:x1]


def clip_box(bbox, im):
    bbox[0] = max(0, bbox[0])
    bbox[1] = max(0, bbox[1])
    bbox[2] = min(im.shape[1] - 1, bbox[2])
    bbox[3] = min(im.shape[0] - 1, bbox[3])
    return bbox


def cut_face(im, bbox, simple_expand=False, pad_value=0, pad_im=True):
    outside_im = (bbox < 0).any() or bbox[2] > im.shape[1] or bbox[3] > im.shape[0]
    if simple_expand or (outside_im and pad_im):
        return pad_image(im, bbox, pad_value)
    bbox = clip_box(bbox, im)
    x0, y0, x1, y1 = bbox
    return im[y0:y1, x0:x1]


def expand_bbox(
        bbox, imshape, simple_expand, default_to_simple=False,
        expansion_factor=0.35):
    assert bbox.shape == (4,), f""BBox shape was: {bbox.shape}""
    bbox = bbox.astype(float)
    if simple_expand:
        return expand_bbox_simple(bbox, 0.4)
    try:
        return expand_bounding_box(bbox, expansion_factor, imshape)
    except AssertionError:
        print(""defaulting to simple expand. Can generated worse results."")
        return expand_bbox_simple(bbox, expansion_factor * 2)
"
DeepPrivacy,cli.py,"import pathlib
import os
import typing
import argparse
from deep_privacy import logger
from deep_privacy.inference.deep_privacy_anonymizer import DeepPrivacyAnonymizer
from deep_privacy.build import build_anonymizer, available_models
video_suffix = ["".mp4""]
image_suffix = ["".jpg"", "".jpeg"", "".png""]


def recursive_find_file(folder: pathlib.Path,
                        suffixes: typing.List[str]
                        ) -> typing.List[pathlib.Path]:
    files = []
    for child in folder.iterdir():
        if not child.is_file():
            child_files = recursive_find_file(child, suffixes)
            files.extend(child_files)
        if child.suffix in suffixes:
            files.append(child)
    return files


def get_target_paths(source_paths: typing.List[pathlib.Path],
                     target_path: str,
                     default_dir: pathlib.Path):
    if not target_path is None:
        target_path = pathlib.Path(target_path)
        if len(source_paths) > 1:
            target_path.mkdir(exist_ok=True, parents=True)
            target_paths = []
            for source_path in source_paths:
                target_paths.append(target_path.joinpath(source_path.name))
            return target_paths
        else:
            target_path.parent.mkdir(exist_ok=True)
            return [target_path]
    logger.info(
        f""Found no target path. Setting to default output path: {default_dir}"")
    default_target_dir = default_dir
    target_path = default_target_dir
    target_path.mkdir(exist_ok=True, parents=True)
    target_paths = []
    for source_path in source_paths:
        if source_path.suffix in video_suffix:
            target_path = default_target_dir.joinpath(""anonymized_videos"")
        else:
            target_path = default_target_dir.joinpath(""anonymized_images"")
        target_path = target_path.joinpath(source_path.name)
        os.makedirs(target_path.parent, exist_ok=True)
        target_paths.append(target_path)
    return target_paths


def get_source_files(source_path: str):
    source_path = pathlib.Path(source_path)
    assert source_path.is_file() or source_path.is_dir(),\
        f""Did not find file or directory: {source_path}""
    if source_path.is_file():
        return [source_path]
    relevant_suffixes = image_suffix + video_suffix
    file_paths = recursive_find_file(source_path, relevant_suffixes)
    return file_paths


def init_anonymizer(cfg, generator):
    return DeepPrivacyAnonymizer(
        generator, **cfg.anonymizer, cfg=cfg)


def get_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""-c"", ""--config_path"", default=None,
        help=""Path to the config. If not None, will override model_type""
    )
    parser.add_argument(
        ""-m"", ""--model"", default=available_models[0],
        choices=available_models,
        help=""The anonymization model to be used.""
    )
    parser.add_argument(
        ""-s"", ""--source_path"",
        help=""Target path to infer. Can be video or image, or directory"",
        default=""test_examples/images""
    )
    parser.add_argument(
        ""-t"", ""--target_path"",
        help=""Target path to save anonymized result.\
                Defaults to subdirectory of config file.""
    )
    parser.add_argument(
        ""--step"", default=None, type=int,
        help=""Set validation checkpoint to load. Defaults to most recent""
    )
    parser.add_argument(
        ""--opts"", default=None, type=str,
        help='can override default settings. For example:\n' +
        '\t opts=""anonymizer.truncation_level=5, anonymizer.batch_size=32""')
    parser.add_argument(
        ""--start_time"", default=0, type=int,
        help=""Start time for anonymization in case of video input. By default, the whole video is anonymized""
    )
    parser.add_argument(
        ""--end_time"", default=None, type=int,
        help=""Start time for anonymization in case of video input. By default, the whole video is anonymized""
    )
    return parser


def main():
    parser = get_parser()
    args = parser.parse_args()    
    anonymizer, cfg = build_anonymizer(
        args.model, opts=args.opts, config_path=args.config_path,
        return_cfg=True)
    output_dir = cfg.output_dir
    source_paths = get_source_files(args.source_path)
    video_paths = [source_path for source_path in source_paths
                   if source_path.suffix in video_suffix]
    image_paths = [source_path for source_path in source_paths
                   if source_path.suffix in image_suffix]
    video_target_paths = []
    if len(video_paths) > 0:
        video_target_paths = get_target_paths(video_paths, args.target_path,
                                              output_dir)
    image_target_paths = []
    if len(image_paths) > 0:
        image_target_paths = get_target_paths(
            image_paths, args.target_path,
            output_dir)
    assert len(image_paths) == len(image_target_paths)
    assert len(video_target_paths) == len(video_paths)
    for video_path, video_target_path in zip(video_paths, video_target_paths):
        anonymizer.anonymize_video(video_path,
                                   video_target_path,
                                   start_time=args.start_time,
                                   end_time=args.end_time)
    if len(image_paths) > 0:
        anonymizer.anonymize_image_paths(image_paths, image_target_paths)


if __name__ == ""__main__"":
    main()
"
DeepPrivacy,torch_utils.py,"import numpy as np
import torch
import math


def get_device():
    if torch.cuda.is_available():
        return torch.device(""cuda"")
    return torch.device(""cpu"")


def image_to_numpy(images, to_uint8=False, denormalize=False):
    single_image = False
    if len(images.shape) == 3:
        single_image = True
        images = images[None]
    if denormalize:
        images = denormalize_img(images)
    images = images.detach().cpu().numpy()
    images = np.moveaxis(images, 1, -1)
    if to_uint8:
        images = (images * 255).astype(np.uint8)
    if single_image:
        return images[0]
    return images


def denormalize_img(image):
    image = (image + 1) / 2
    image = torch.clamp(image.float(), 0, 1)
    return image


def number_of_parameters(module: torch.nn.Module):
    count = 0
    for p in module.parameters():
        count += np.prod(p.shape)
    return count


def image_to_torch(image, cuda=True, normalize_img=False):
    single_image = len(image.shape) == 3
    if image.dtype == np.uint8:
        image = image.astype(np.float32)
        image /= 255
    else:
        assert image.dtype == np.float32
    if single_image:
        image = np.rollaxis(image, 2)
        image = image[None, :, :, :]
    else:
        image = np.moveaxis(image, -1, 1)
    image = torch.from_numpy(image).contiguous()
    if cuda:
        image = to_cuda(image)
    assert image.min() >= 0.0 and image.max() <= 1.0
    if normalize_img:
        image = image * 2 - 1
    return image


def mask_to_torch(mask: np.ndarray, cuda=True):
    assert mask.max() <= 1 and mask.min() >= 0
    mask = mask.squeeze()
    single_mask = len(mask.shape) == 2
    if single_mask:
        mask = mask[None]
    mask = mask[:, None, :, :]
    mask = torch.from_numpy(mask)
    if cuda:
        mask = to_cuda(mask)
    return mask


def _to_cuda(element):
    if isinstance(element, torch.nn.Module):
        return element.cuda()
    return element.cuda(non_blocking=True)


def to_cuda(elements):
    if torch.cuda.is_available():
        if isinstance(elements, tuple) or isinstance(elements, list):
            return [_to_cuda(x) for x in elements]
        return _to_cuda(elements)
    return elements


def isinf(tensor):
    if not isinstance(tensor, torch.Tensor):
        raise ValueError(""The argument is not a tensor"", str(tensor))
    return (tensor.abs() == math.inf).any()


def isnan(tensor):
    r""""""Returns a new tensor with boolean elements representing if each element
    is `NaN` or not.
    Arguments:
        tensor (Tensor): A tensor to check
    Returns:
        Tensor: A ``torch.ByteTensor`` containing a 1 at each location of `NaN`
        elements.
    Example::
        >>> torch.isnan(torch.tensor([1, float('nan'), 2]))
        tensor([ 0,  1,  0], dtype=torch.uint8)
    """"""
    if not isinstance(tensor, torch.Tensor):
        raise ValueError(""The argument is not a tensor"", str(tensor))
    return (tensor != tensor).any()


def finiteCheck(parameters):
    if isinstance(parameters, torch.Tensor):
        parameters = [parameters]
    parameters = list(filter(lambda p: p.grad is not None, parameters))
    for p in parameters:
        if isinf(p.grad.data):
            return False
        if isnan(p.grad.data):
            return False
    return True


def keypoint_to_numpy(keypoint):
    keypoint = keypoint.cpu()
    return keypoint.view(-1, 2).numpy()
"
DeepPrivacy,__init__.py,
DeepPrivacy,utils.py,"import cv2
import matplotlib
import matplotlib.pyplot as plt
import numpy as np


colors = list(matplotlib.colors.cnames.values())


def hex_to_rgb(h): return tuple(int(h[i:i + 2], 16) for i in (0, 2, 4))


colors = [hex_to_rgb(x[1:]) for x in colors]
colors = [(255, 0, 0)] + colors


def draw_faces_with_keypoints(
        im,
        im_bboxes,
        im_keypoints,
        radius=None,
        black_out_face=False,
        color_override=None
):
    im = im.copy()
    if im_keypoints is None:
        assert im_bboxes is not None, ""Image bboxes cannot be None.""
        im_keypoints = [None for i in range(len(im_bboxes))]
    if im_bboxes is None:
        im_bboxes = [None for i in range(len(im_keypoints))]
    if radius is None:
        radius = max(int(max(im.shape) * 0.0025), 1)
    for c_idx, (bbox, keypoint) in enumerate(zip(im_bboxes, im_keypoints)):
        color = color_override
        if color_override is None:
            color = colors[c_idx % len(colors)]

        if bbox is not None:
            x0, y0, x1, y1 = bbox
            if black_out_face:
                im[y0:y1, x0:x1, :] = 0
            else:
                im = cv2.rectangle(im, (x0, y0), (x1, y1), color)
        if keypoint is None:
            continue
        for x, y in keypoint:
            im = cv2.circle(im, (int(x), int(y)), radius, color)
    if not isinstance(im, np.ndarray):
        return im.get()
    return im


def np_make_image_grid(images, nrow, pad=2):
    height, width = images[0].shape[:2]
    ncol = int(np.ceil(len(images) / nrow))
    im_result = np.zeros((nrow * (height + pad), ncol * (width + pad), 3),
                         dtype=images[0].dtype)
    im_idx = 0
    for row in range(nrow):
        for col in range(ncol):
            if im_idx == len(images):
                break
            im = images[im_idx]
            im_idx += 1
            im_result[row * (pad + height): (row) * (pad + height) + height,
                      col * (pad + width): (col) * (pad + width) + width, :] = im
    return im_result


def add_text(im, x, y, text):
    font = cv2.FONT_HERSHEY_SIMPLEX
    bottomLeftCornerOfText = (x, y + 10)
    fontScale = .4
    fontColor = (255, 255, 255)
    backgroundColor = (0, 0, 0)
    lineType = 1

    cv2.putText(im, text,
                bottomLeftCornerOfText,
                font,
                fontScale,
                backgroundColor,
                lineType * 2)
    cv2.putText(im, text,
                bottomLeftCornerOfText,
                font,
                fontScale,
                fontColor,
                lineType)


def add_label_y(im, positions, labels):
    # positions [(x, y)]
    im = im.copy()
    assert len(positions) == len(labels)
    for pos, label in zip(positions, labels):
        add_text(im, 0, pos, label)
    return im


def plot_bbox(bbox):
    x0, y0, x1, y1 = bbox
    plt.plot([x0, x0, x1, x1, x0], [y0, y1, y1, y0, y0])


def pad_im_as(im, target_im):
    assert len(im.shape) == 3
    assert len(target_im.shape) == 3
    assert im.shape[0] <= target_im.shape[0]
    assert im.shape[1] <= target_im.shape[1],\
        f""{im.shape}, {target_im.shape}""
    pad_h = abs(im.shape[0] - target_im.shape[0]) // 2
    pad_w = abs(im.shape[1] - target_im.shape[1]) // 2
    im = np.pad(im, ((pad_h, pad_h), (pad_w, pad_w), (0, 0)))
    assert im.shape == target_im.shape
    return im
"
DeepPrivacy,__init__.py,
DeepPrivacy,metric_api.py,"import numpy as np
import skimage
import skimage.measure
import torch
import tqdm
import multiprocessing
try:
    from skimage.measure import compare_ssim, compare_psnr
except ImportError:
    from skimage.metrics import structural_similarity as compare_ssim
    from skimage.metrics import peak_signal_noise_ratio as compare_psnr
from .perceptual_similarity import PerceptualLoss
from deep_privacy import torch_utils
from .fid_pytorch import fid as fid_api
from typing import List, Dict


def compute_metrics(
        images1: np.ndarray, images2: np.ndarray,
        metrics: List[str]) -> Dict[str, float]:
    results = {}
    METRICS = {
        ""l1"": l1,
        ""l2"": l2,
        ""ssim"": ssim,
        ""psnr"": psnr,
        ""lpips"": lpips,
        ""fid"": fid
    }
    for metric in metrics:
        func = METRICS[metric]
        results[metric] = func(images1, images2)
    return results


def check_shape(images1: np.ndarray, images2: np.ndarray):
    assert len(images1.shape) == 4
    assert images1.shape == images2.shape
    assert images1.dtype == np.float32
    assert images2.dtype == np.float32
    assert images1.max() <= 1 and images1.min() >= 0
    assert images2.max() <= 1 and images2.min() >= 0


def l2(images1: np.ndarray, images2: np.ndarray):
    check_shape(images1, images2)
    difference = (images1 - images2)**2
    rmse = difference.reshape(difference.shape[0], -1)
    rmse = rmse.mean(axis=1) ** 0.5
    return rmse.mean()


def l1(images1: np.ndarray, images2: np.ndarray):
    check_shape(images1, images2)
    difference = abs(images1 - images2)
    return difference.mean()


def ssim(images1: np.ndarray, images2: np.ndarray):
    check_shape(images1, images2)
    mean_ssim = 0
    with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:
        jobs = []
        for img1, img2 in zip(images1, images2):
            s = pool.apply_async(
                compare_ssim, (img1, img2),
                dict(
                    data_range=1, multichannel=True, win_size=11,
                    gaussian_weights=True, sigma=1.5,
                    use_sample_covariance=False, K1=0.01 ** 2, K2=0.03 ** 2))
            jobs.append(s)
        for job in tqdm.tqdm(jobs):
            mean_ssim += job.get()
    return mean_ssim / images1.shape[0]


def psnr(images1: np.ndarray, images2: np.ndarray):
    check_shape(images1, images2)
    mean_psnr = 0
    for img1, img2 in zip(images1, images2):
        s = compare_psnr(
            img1, img2, data_range=1)
        mean_psnr += s.mean()
    return mean_psnr / images1.shape[0]


def lpips(images1: np.ndarray, images2: np.ndarray,
          batch_size: int = 64,
          metric_type: str = ""net-lin"",
          reduce=True):
    assert metric_type in [""net-lin"", ""l2"", ""ssim"", ]

    check_shape(images1, images2)
    n_batches = int(np.ceil(images1.shape[0] / batch_size))
    model = PerceptualLoss(
        model='net-lin', net='alex', use_gpu=torch.cuda.is_available())
    distances = np.zeros((images1.shape[0]), dtype=np.float32)
    for i in range(n_batches):
        start_idx = i * batch_size
        end_idx = (i + 1) * batch_size
        im1 = images1[start_idx:end_idx]
        im2 = images2[start_idx:end_idx]
        im1 = torch_utils.image_to_torch(im1, normalize_img=True)
        im2 = torch_utils.image_to_torch(im2, normalize_img=True)
        with torch.no_grad():
            dists = model(im1, im2, normalize=False).cpu().numpy().squeeze()
        distances[start_idx:end_idx] = dists
    if reduce:

        return distances.mean()
    assert batch_size == 1
    return distances


def fid(images1: np.ndarray, images2: np.ndarray,
        batch_size: int = 64):
    check_shape(images1, images2)
    fid = fid_api.calculate_fid(
        images1, images2, batch_size=batch_size, dims=2048)
    return fid


def compute_all_metrics(images1: np.ndarray, images2: np.ndarray):
    metrics = {}
    metrics[""L1""] = l1(images1, images2)
    metrics[""L2""] = l2(images1, images2)
    metrics[""PSNR""] = psnr(images1, images2)
    metrics[""SSIM""] = ssim(images1, images2)
    metrics[""LPIPS""] = lpips(images1, images2)
    return metrics


def print_all_metrics(images1: np.ndarray, images2: np.ndarray):
    metrics = compute_all_metrics(images1, images2)
    for m, v in metrics.items():
        print(f""{m}: {v}"")


if __name__ == ""__main__"":
    import argparse
    import pathlib
    from deep_privacy import file_util
    parser = argparse.ArgumentParser()
    parser.add_argument(""path1"")
    parser.add_argument(""path2"")
    args = parser.parse_args()
    path1 = pathlib.Path(args.path1)
    path2 = pathlib.Path(args.path2)
    filepaths1 = file_util.find_all_files(path1)
    filepaths2 = file_util.find_matching_files(path2, filepaths1)
    images1 = file_util.read_images(filepaths1)
    images2 = file_util.read_images(filepaths2)
    print_all_metrics(images1, images2)
"
DeepPrivacy,__init__.py,
DeepPrivacy,inception.py,"import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision

try:
    from torchvision.models.utils import load_state_dict_from_url
except ImportError:
    from torch.utils.model_zoo import load_url as load_state_dict_from_url

# Inception weights ported to Pytorch from
# http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz
FID_WEIGHTS_URL = 'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth'


class InceptionV3(nn.Module):
    """"""Pretrained InceptionV3 network returning feature maps""""""

    # Index of default block of inception to return,
    # corresponds to output of final average pooling
    DEFAULT_BLOCK_INDEX = 3

    # Maps feature dimensionality to their output blocks indices
    BLOCK_INDEX_BY_DIM = {
        64: 0,   # First max pooling features
        192: 1,  # Second max pooling featurs
        768: 2,  # Pre-aux classifier features
        2048: 3  # Final average pooling features
    }

    def __init__(self,
                 output_blocks=[DEFAULT_BLOCK_INDEX],
                 resize_input=True,
                 normalize_input=True,
                 requires_grad=False,
                 use_fid_inception=True):
        """"""Build pretrained InceptionV3

        Parameters
        ----------
        output_blocks : list of int
            Indices of blocks to return features of. Possible values are:
                - 0: corresponds to output of first max pooling
                - 1: corresponds to output of second max pooling
                - 2: corresponds to output which is fed to aux classifier
                - 3: corresponds to output of final average pooling
        resize_input : bool
            If true, bilinearly resizes input to width and height 299 before
            feeding input to model. As the network without fully connected
            layers is fully convolutional, it should be able to handle inputs
            of arbitrary size, so resizing might not be strictly needed
        normalize_input : bool
            If true, scales the input from range (0, 1) to the range the
            pretrained Inception network expects, namely (-1, 1)
        requires_grad : bool
            If true, parameters of the model require gradients. Possibly useful
            for finetuning the network
        use_fid_inception : bool
            If true, uses the pretrained Inception model used in Tensorflow's
            FID implementation. If false, uses the pretrained Inception model
            available in torchvision. The FID Inception model has different
            weights and a slightly different structure from torchvision's
            Inception model. If you want to compute FID scores, you are
            strongly advised to set this parameter to true to get comparable
            results.
        """"""
        super(InceptionV3, self).__init__()

        self.resize_input = resize_input
        self.normalize_input = normalize_input
        self.output_blocks = sorted(output_blocks)
        self.last_needed_block = max(output_blocks)

        assert self.last_needed_block <= 3, \
            'Last possible output block index is 3'

        self.blocks = nn.ModuleList()

        if use_fid_inception:
            inception = fid_inception_v3()
        else:
            inception = _inception_v3(pretrained=True)

        # Block 0: input to maxpool1
        block0 = [
            inception.Conv2d_1a_3x3,
            inception.Conv2d_2a_3x3,
            inception.Conv2d_2b_3x3,
            nn.MaxPool2d(kernel_size=3, stride=2)
        ]
        self.blocks.append(nn.Sequential(*block0))

        # Block 1: maxpool1 to maxpool2
        if self.last_needed_block >= 1:
            block1 = [
                inception.Conv2d_3b_1x1,
                inception.Conv2d_4a_3x3,
                nn.MaxPool2d(kernel_size=3, stride=2)
            ]
            self.blocks.append(nn.Sequential(*block1))

        # Block 2: maxpool2 to aux classifier
        if self.last_needed_block >= 2:
            block2 = [
                inception.Mixed_5b,
                inception.Mixed_5c,
                inception.Mixed_5d,
                inception.Mixed_6a,
                inception.Mixed_6b,
                inception.Mixed_6c,
                inception.Mixed_6d,
                inception.Mixed_6e,
            ]
            self.blocks.append(nn.Sequential(*block2))

        # Block 3: aux classifier to final avgpool
        if self.last_needed_block >= 3:
            block3 = [
                inception.Mixed_7a,
                inception.Mixed_7b,
                inception.Mixed_7c,
                nn.AdaptiveAvgPool2d(output_size=(1, 1))
            ]
            self.blocks.append(nn.Sequential(*block3))

        for param in self.parameters():
            param.requires_grad = requires_grad

    def forward(self, inp):
        """"""Get Inception feature maps

        Parameters
        ----------
        inp : torch.autograd.Variable
            Input tensor of shape Bx3xHxW. Values are expected to be in
            range (0, 1)

        Returns
        -------
        List of torch.autograd.Variable, corresponding to the selected output
        block, sorted ascending by index
        """"""
        outp = []
        x = inp

        if self.resize_input:
            x = F.interpolate(x,
                              size=(299, 299),
                              mode='bilinear',
                              align_corners=False)

        if self.normalize_input:
            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)

        for idx, block in enumerate(self.blocks):
            x = block(x)
            if idx in self.output_blocks:
                outp.append(x)

            if idx == self.last_needed_block:
                break

        return outp


def _inception_v3(*args, **kwargs):
    """"""Wraps `torchvision.models.inception_v3`

    Skips default weight inititialization if supported by torchvision version.
    See https://github.com/mseitzer/pytorch-fid/issues/28.
    """"""
    try:
        version = tuple(map(int, torchvision.__version__.split('.')[:2]))
    except ValueError:
        # Just a caution against weird version strings
        version = (0,)

    if version >= (0, 6):
        kwargs['init_weights'] = False

    return torchvision.models.inception_v3(*args, **kwargs)


def fid_inception_v3():
    """"""Build pretrained Inception model for FID computation

    The Inception model for FID computation uses a different set of weights
    and has a slightly different structure than torchvision's Inception.

    This method first constructs torchvision's Inception and then patches the
    necessary parts that are different in the FID Inception model.
    """"""
    inception = _inception_v3(num_classes=1008,
                              aux_logits=False,
                              pretrained=False)
    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)
    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)
    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)
    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)
    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)
    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)
    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)
    inception.Mixed_7b = FIDInceptionE_1(1280)
    inception.Mixed_7c = FIDInceptionE_2(2048)

    state_dict = load_state_dict_from_url(FID_WEIGHTS_URL, progress=True)
    inception.load_state_dict(state_dict)
    return inception


class FIDInceptionA(torchvision.models.inception.InceptionA):
    """"""InceptionA block patched for FID computation""""""

    def __init__(self, in_channels, pool_features):
        super(FIDInceptionA, self).__init__(in_channels, pool_features)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)

        branch5x5 = self.branch5x5_1(x)
        branch5x5 = self.branch5x5_2(branch5x5)

        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)

        # Patch: Tensorflow's average pool does not use the padded zero's in
        # its average calculation
        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,
                                   count_include_pad=False)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)


class FIDInceptionC(torchvision.models.inception.InceptionC):
    """"""InceptionC block patched for FID computation""""""

    def __init__(self, in_channels, channels_7x7):
        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)

        branch7x7 = self.branch7x7_1(x)
        branch7x7 = self.branch7x7_2(branch7x7)
        branch7x7 = self.branch7x7_3(branch7x7)

        branch7x7dbl = self.branch7x7dbl_1(x)
        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)

        # Patch: Tensorflow's average pool does not use the padded zero's in
        # its average calculation
        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,
                                   count_include_pad=False)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]
        return torch.cat(outputs, 1)


class FIDInceptionE_1(torchvision.models.inception.InceptionE):
    """"""First InceptionE block patched for FID computation""""""

    def __init__(self, in_channels):
        super(FIDInceptionE_1, self).__init__(in_channels)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)

        branch3x3 = self.branch3x3_1(x)
        branch3x3 = [
            self.branch3x3_2a(branch3x3),
            self.branch3x3_2b(branch3x3),
        ]
        branch3x3 = torch.cat(branch3x3, 1)

        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = [
            self.branch3x3dbl_3a(branch3x3dbl),
            self.branch3x3dbl_3b(branch3x3dbl),
        ]
        branch3x3dbl = torch.cat(branch3x3dbl, 1)

        # Patch: Tensorflow's average pool does not use the padded zero's in
        # its average calculation
        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,
                                   count_include_pad=False)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)


class FIDInceptionE_2(torchvision.models.inception.InceptionE):
    """"""Second InceptionE block patched for FID computation""""""

    def __init__(self, in_channels):
        super(FIDInceptionE_2, self).__init__(in_channels)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)

        branch3x3 = self.branch3x3_1(x)
        branch3x3 = [
            self.branch3x3_2a(branch3x3),
            self.branch3x3_2b(branch3x3),
        ]
        branch3x3 = torch.cat(branch3x3, 1)

        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = [
            self.branch3x3dbl_3a(branch3x3dbl),
            self.branch3x3dbl_3b(branch3x3dbl),
        ]
        branch3x3dbl = torch.cat(branch3x3dbl, 1)

        # Patch: The FID Inception model uses max pooling instead of average
        # pooling. This is likely an error in this specific Inception
        # implementation, as other Inception models use average pooling here
        # (which matches the description in the paper).
        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)
"
DeepPrivacy,fid.py,"#!/usr/bin/env python3
""""""Calculates the Frechet Inception Distance (FID) to evalulate GANs

The FID metric calculates the distance between two distributions of images.
Typically, we have summary statistics (mean & covariance matrix) of one
of these distributions, while the 2nd distribution is given by a GAN.

When run as a stand-alone program, it compares the distribution of
images that are stored as PNG/JPEG at a specified location with a
distribution given by summary statistics (in pickle format).

The FID is calculated by assuming that X_1 and X_2 are the activations of
the pool_3 layer of the inception net for generated samples and real world
samples respectively.

See --help to see further details.

Code apapted from https://github.com/bioinf-jku/TTUR to use PyTorch instead
of Tensorflow

Copyright 2018 Institute of Bioinformatics, JKU Linz

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
""""""
import pathlib
import numpy as np
import torch
from scipy import linalg
from torch.nn.functional import adaptive_avg_pool2d
from PIL import Image
from .inception import InceptionV3
from deep_privacy import torch_utils, file_util
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter


def imread(filename):
    """"""
    Loads an image file into a (height, width, 3) uint8 ndarray.
    """"""
    return np.asarray(Image.open(filename), dtype=np.uint8)[..., :3]


@torch.no_grad()
def get_activations(images: np.ndarray, model, batch_size=50, dims=2048):
    """"""Calculates the activations of the pool_3 layer for all images.

    Params:
    -- files       : List of image files paths
    -- model       : Instance of inception model
    -- batch_size  : Batch size of images for the model to process at once.
                     Make sure that the number of samples is a multiple of
                     the batch size, otherwise some samples are ignored. This
                     behavior is retained to match the original FID score
                     implementation.
    -- dims        : Dimensionality of features returned by Inception
    -- cuda        : If set to True, use GPU
    -- verbose     : If set to True and parameter out_step is given, the number
                     of calculated batches is reported.
    Returns:
    -- A numpy array of dimension (num images, dims) that contains the
       activations of the given tensor when feeding inception with the
       query tensor.
    """"""
    assert images.dtype == np.float32
    assert images.max() <= 1.0
    model.eval()

    if batch_size > len(images):
        batch_size = len(images)

    pred_arr = np.empty((len(images), dims))
    images = np.moveaxis(images, -1, 1)
    images = torch.from_numpy(images).float()

    for i in range(0, len(images), batch_size):
        start = i
        end = i + batch_size
        batch = images[start:end]

        batch = torch_utils.to_cuda(batch)

        pred = model(batch)[0]

        # If model output is not scalar, apply global spatial average pooling.
        # This happens if you choose a dimensionality not equal 2048.
        if pred.size(2) != 1 or pred.size(3) != 1:
            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))

        pred_arr[start:end] = pred.cpu().data.numpy().reshape(pred.size(0), -1)

    return pred_arr


def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):
    """"""Numpy implementation of the Frechet Distance.
    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)
    and X_2 ~ N(mu_2, C_2) is
            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).

    Stable version by Dougal J. Sutherland.

    Params:
    -- mu1   : Numpy array containing the activations of a layer of the
               inception net (like returned by the function 'get_predictions')
               for generated samples.
    -- mu2   : The sample mean over activations, precalculated on an
               representative data set.
    -- sigma1: The covariance matrix over activations for generated samples.
    -- sigma2: The covariance matrix over activations, precalculated on an
               representative data set.

    Returns:
    --   : The Frechet Distance.
    """"""

    mu1 = np.atleast_1d(mu1)
    mu2 = np.atleast_1d(mu2)

    sigma1 = np.atleast_2d(sigma1)
    sigma2 = np.atleast_2d(sigma2)

    assert mu1.shape == mu2.shape, \
        'Training and test mean vectors have different lengths'
    assert sigma1.shape == sigma2.shape, \
        'Training and test covariances have different dimensions'

    diff = mu1 - mu2

    # Product might be almost singular
    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)
    if not np.isfinite(covmean).all():
        msg = ('fid calculation produces singular product; '
               'adding %s to diagonal of cov estimates') % eps
        print(msg)
        offset = np.eye(sigma1.shape[0]) * eps
        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))

    # Numerical error might give slight imaginary component
    if np.iscomplexobj(covmean):
        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):
            m = np.max(np.abs(covmean.imag))
            raise ValueError('Imaginary component {}'.format(m))
        covmean = covmean.real

    tr_covmean = np.trace(covmean)

    return (diff.dot(diff) + np.trace(sigma1) +
            np.trace(sigma2) - 2 * tr_covmean)


def calculate_activation_statistics(images, model, batch_size=50,
                                    dims=2048):
    """"""Calculation of the statistics used by the FID.
    Params:
    -- files       : List of image files paths
    -- model       : Instance of inception model
    -- batch_size  : The images numpy array is split into batches with
                     batch size batch_size. A reasonable batch size
                     depends on the hardware.
    -- dims        : Dimensionality of features returned by Inception
    -- cuda        : If set to True, use GPU
    -- verbose     : If set to True and parameter out_step is given, the
                     number of calculated batches is reported.
    Returns:
    -- mu    : The mean over samples of the activations of the pool_3 layer of
               the inception model.
    -- sigma : The covariance matrix of the activations of the pool_3 layer of
               the inception model.
    """"""
    act = get_activations(images, model, batch_size, dims)
    mu = np.mean(act, axis=0)
    sigma = np.cov(act, rowvar=False)
    return mu, sigma


def calculate_fid(images1, images2, batch_size, dims):
    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]

    model = InceptionV3([block_idx])
    model = torch_utils.to_cuda(model)

    m1, s1 = calculate_activation_statistics(images1, model, batch_size)
    m2, s2 = calculate_activation_statistics(images2, model, batch_size)
    fid_value = calculate_frechet_distance(m1, s1, m2, s2)
    return fid_value


if __name__ == '__main__':

    parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)
    parser.add_argument('path', type=str, nargs=2,
                        help=('Path to the generated images or '
                              'to .npz statistic files'))
    parser.add_argument('--batch-size', type=int, default=50,
                        help='Batch size to use')
    parser.add_argument('--dims', type=int, default=2048,
                        choices=list(InceptionV3.BLOCK_INDEX_BY_DIM),
                        help=('Dimensionality of Inception features to use. '
                              'By default, uses pool3 features'))
    args = parser.parse_args()
    path1, path2 = [pathlib.Path(x) for x in args.path]
    paths1 = file_util.find_all_files(path1)
    paths2 = file_util.find_matching_files(path2, paths1)
    images1 = file_util.read_images(paths1)
    images2 = file_util.read_images(paths2)
    fid_value = calculate_fid(images1, images2, args.batch_size, args.dims)
    print('FID: ', fid_value)
"
DeepPrivacy,pretrained_networks.py,"from collections import namedtuple
import torch
from torchvision import models as tv


class squeezenet(torch.nn.Module):
    def __init__(self, requires_grad=False, pretrained=True):
        super(squeezenet, self).__init__()
        pretrained_features = tv.squeezenet1_1(pretrained=pretrained).features
        self.slice1 = torch.nn.Sequential()
        self.slice2 = torch.nn.Sequential()
        self.slice3 = torch.nn.Sequential()
        self.slice4 = torch.nn.Sequential()
        self.slice5 = torch.nn.Sequential()
        self.slice6 = torch.nn.Sequential()
        self.slice7 = torch.nn.Sequential()
        self.N_slices = 7
        for x in range(2):
            self.slice1.add_module(str(x), pretrained_features[x])
        for x in range(2, 5):
            self.slice2.add_module(str(x), pretrained_features[x])
        for x in range(5, 8):
            self.slice3.add_module(str(x), pretrained_features[x])
        for x in range(8, 10):
            self.slice4.add_module(str(x), pretrained_features[x])
        for x in range(10, 11):
            self.slice5.add_module(str(x), pretrained_features[x])
        for x in range(11, 12):
            self.slice6.add_module(str(x), pretrained_features[x])
        for x in range(12, 13):
            self.slice7.add_module(str(x), pretrained_features[x])
        if not requires_grad:
            for param in self.parameters():
                param.requires_grad = False

    def forward(self, X):
        h = self.slice1(X)
        h_relu1 = h
        h = self.slice2(h)
        h_relu2 = h
        h = self.slice3(h)
        h_relu3 = h
        h = self.slice4(h)
        h_relu4 = h
        h = self.slice5(h)
        h_relu5 = h
        h = self.slice6(h)
        h_relu6 = h
        h = self.slice7(h)
        h_relu7 = h
        vgg_outputs = namedtuple(
            ""SqueezeOutputs"", [
                'relu1', 'relu2', 'relu3', 'relu4', 'relu5', 'relu6', 'relu7'])
        out = vgg_outputs(
            h_relu1,
            h_relu2,
            h_relu3,
            h_relu4,
            h_relu5,
            h_relu6,
            h_relu7)

        return out


class alexnet(torch.nn.Module):
    def __init__(self, requires_grad=False, pretrained=True):
        super(alexnet, self).__init__()
        alexnet_pretrained_features = tv.alexnet(
            pretrained=pretrained).features
        self.slice1 = torch.nn.Sequential()
        self.slice2 = torch.nn.Sequential()
        self.slice3 = torch.nn.Sequential()
        self.slice4 = torch.nn.Sequential()
        self.slice5 = torch.nn.Sequential()
        self.N_slices = 5
        for x in range(2):
            self.slice1.add_module(str(x), alexnet_pretrained_features[x])
        for x in range(2, 5):
            self.slice2.add_module(str(x), alexnet_pretrained_features[x])
        for x in range(5, 8):
            self.slice3.add_module(str(x), alexnet_pretrained_features[x])
        for x in range(8, 10):
            self.slice4.add_module(str(x), alexnet_pretrained_features[x])
        for x in range(10, 12):
            self.slice5.add_module(str(x), alexnet_pretrained_features[x])
        if not requires_grad:
            for param in self.parameters():
                param.requires_grad = False

    def forward(self, X):
        h = self.slice1(X)
        h_relu1 = h
        h = self.slice2(h)
        h_relu2 = h
        h = self.slice3(h)
        h_relu3 = h
        h = self.slice4(h)
        h_relu4 = h
        h = self.slice5(h)
        h_relu5 = h
        alexnet_outputs = namedtuple(
            ""AlexnetOutputs"", [
                'relu1', 'relu2', 'relu3', 'relu4', 'relu5'])
        out = alexnet_outputs(h_relu1, h_relu2, h_relu3, h_relu4, h_relu5)

        return out


class vgg16(torch.nn.Module):
    def __init__(self, requires_grad=False, pretrained=True):
        super(vgg16, self).__init__()
        vgg_pretrained_features = tv.vgg16(pretrained=pretrained).features
        self.slice1 = torch.nn.Sequential()
        self.slice2 = torch.nn.Sequential()
        self.slice3 = torch.nn.Sequential()
        self.slice4 = torch.nn.Sequential()
        self.slice5 = torch.nn.Sequential()
        self.N_slices = 5
        for x in range(4):
            self.slice1.add_module(str(x), vgg_pretrained_features[x])
        for x in range(4, 9):
            self.slice2.add_module(str(x), vgg_pretrained_features[x])
        for x in range(9, 16):
            self.slice3.add_module(str(x), vgg_pretrained_features[x])
        for x in range(16, 23):
            self.slice4.add_module(str(x), vgg_pretrained_features[x])
        for x in range(23, 30):
            self.slice5.add_module(str(x), vgg_pretrained_features[x])
        if not requires_grad:
            for param in self.parameters():
                param.requires_grad = False

    def forward(self, X):
        h = self.slice1(X)
        h_relu1_2 = h
        h = self.slice2(h)
        h_relu2_2 = h
        h = self.slice3(h)
        h_relu3_3 = h
        h = self.slice4(h)
        h_relu4_3 = h
        h = self.slice5(h)
        h_relu5_3 = h
        vgg_outputs = namedtuple(
            ""VggOutputs"", [
                'relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])
        out = vgg_outputs(
            h_relu1_2,
            h_relu2_2,
            h_relu3_3,
            h_relu4_3,
            h_relu5_3)

        return out


class resnet(torch.nn.Module):
    def __init__(self, requires_grad=False, pretrained=True, num=18):
        super(resnet, self).__init__()
        if(num == 18):
            self.net = tv.resnet18(pretrained=pretrained)
        elif(num == 34):
            self.net = tv.resnet34(pretrained=pretrained)
        elif(num == 50):
            self.net = tv.resnet50(pretrained=pretrained)
        elif(num == 101):
            self.net = tv.resnet101(pretrained=pretrained)
        elif(num == 152):
            self.net = tv.resnet152(pretrained=pretrained)
        self.N_slices = 5

        self.conv1 = self.net.conv1
        self.bn1 = self.net.bn1
        self.relu = self.net.relu
        self.maxpool = self.net.maxpool
        self.layer1 = self.net.layer1
        self.layer2 = self.net.layer2
        self.layer3 = self.net.layer3
        self.layer4 = self.net.layer4

    def forward(self, X):
        h = self.conv1(X)
        h = self.bn1(h)
        h = self.relu(h)
        h_relu1 = h
        h = self.maxpool(h)
        h = self.layer1(h)
        h_conv2 = h
        h = self.layer2(h)
        h_conv3 = h
        h = self.layer3(h)
        h_conv4 = h
        h = self.layer4(h)
        h_conv5 = h

        outputs = namedtuple(
            ""Outputs"", [
                'relu1', 'conv2', 'conv3', 'conv4', 'conv5'])
        out = outputs(h_relu1, h_conv2, h_conv3, h_conv4, h_conv5)

        return out
"
DeepPrivacy,networks_basic.py,"import torch
import torch.nn as nn
from torch.autograd import Variable
from . import pretrained_networks as pn


def spatial_average(in_tens, keepdim=True):
    return in_tens.mean([2, 3], keepdim=keepdim)


def upsample(in_tens, out_H=64):  # assumes scale factor is same for H and W
    in_H = in_tens.shape[2]
    scale_factor = 1. * out_H / in_H

    return nn.Upsample(
        scale_factor=scale_factor, mode='bilinear', align_corners=False)(
        in_tens)


def normalize_tensor(in_feat, eps=1e-10):
    norm_factor = torch.sqrt(torch.sum(in_feat**2, dim=1, keepdim=True))
    return in_feat / (norm_factor + eps)


# Learned perceptual metric
class PNetLin(nn.Module):
    def __init__(
            self, pnet_type='vgg', pnet_rand=False, pnet_tune=False,
            use_dropout=True, spatial=False, version='0.1', lpips=True):
        super(PNetLin, self).__init__()

        self.pnet_type = pnet_type
        self.pnet_tune = pnet_tune
        self.pnet_rand = pnet_rand
        self.spatial = spatial
        self.lpips = lpips
        self.version = version
        self.scaling_layer = ScalingLayer()

        if(self.pnet_type in ['vgg', 'vgg16']):
            net_type = pn.vgg16
            self.chns = [64, 128, 256, 512, 512]
        elif(self.pnet_type == 'alex'):
            net_type = pn.alexnet
            self.chns = [64, 192, 384, 256, 256]
        elif(self.pnet_type == 'squeeze'):
            net_type = pn.squeezenet
            self.chns = [64, 128, 256, 384, 384, 512, 512]
        self.L = len(self.chns)

        self.net = net_type(
            pretrained=not self.pnet_rand,
            requires_grad=self.pnet_tune)

        if(lpips):
            self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)
            self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)
            self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)
            self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)
            self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)
            self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]
            if(self.pnet_type == 'squeeze'):  # 7 layers for squeezenet
                self.lin5 = NetLinLayer(self.chns[5], use_dropout=use_dropout)
                self.lin6 = NetLinLayer(self.chns[6], use_dropout=use_dropout)
                self.lins += [self.lin5, self.lin6]

    def forward(self, in0, in1, retPerLayer=False):
        # v0.0 - original release had a bug, where input was not scaled
        in0_input, in1_input = (
            self.scaling_layer(in0),
            self.scaling_layer(in1)) if self.version == '0.1' else(
            in0, in1)
        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)
        feats0, feats1, diffs = {}, {}, {}

        for kk in range(self.L):
            feats0[kk], feats1[kk] = normalize_tensor(
                outs0[kk]), normalize_tensor(outs1[kk])
            diffs[kk] = (feats0[kk] - feats1[kk])**2

        if(self.lpips):
            if(self.spatial):
                res = [
                    upsample(
                        self.lins[kk].model(
                            diffs[kk]),
                        out_H=in0.shape[2]) for kk in range(
                        self.L)]
            else:
                res = [
                    spatial_average(
                        self.lins[kk].model(
                            diffs[kk]),
                        keepdim=True) for kk in range(
                        self.L)]
        else:
            if(self.spatial):
                res = [
                    upsample(
                        diffs[kk].sum(
                            dim=1,
                            keepdim=True),
                        out_H=in0.shape[2]) for kk in range(
                        self.L)]
            else:
                res = [
                    spatial_average(
                        diffs[kk].sum(
                            dim=1,
                            keepdim=True),
                        keepdim=True) for kk in range(
                        self.L)]

        val = res[0]
        for l in range(1, self.L):
            val += res[l]

        if(retPerLayer):
            return (val, res)
        else:
            return val


class ScalingLayer(nn.Module):
    def __init__(self):
        super(ScalingLayer, self).__init__()
        self.register_buffer(
            'shift', torch.Tensor([-.030, -.088, -.188])
            [None, :, None, None])
        self.register_buffer('scale', torch.Tensor(
            [.458, .448, .450])[None, :, None, None])

    def forward(self, inp):
        return (inp - self.shift) / self.scale


class NetLinLayer(nn.Module):
    ''' A single linear layer which does a 1x1 conv '''

    def __init__(self, chn_in, chn_out=1, use_dropout=False):
        super(NetLinLayer, self).__init__()

        layers = [nn.Dropout(), ] if(use_dropout) else []
        layers += [nn.Conv2d(chn_in, chn_out, 1, stride=1,
                             padding=0, bias=False), ]
        self.model = nn.Sequential(*layers)


class Dist2LogitLayer(nn.Module):
    ''' takes 2 distances, puts through fc layers, spits out value between [0,1] (if use_sigmoid is True) '''

    def __init__(self, chn_mid=32, use_sigmoid=True):
        super(Dist2LogitLayer, self).__init__()

        layers = [nn.Conv2d(5, chn_mid, 1, stride=1, padding=0, bias=True), ]
        layers += [nn.LeakyReLU(0.2, True), ]
        layers += [nn.Conv2d(chn_mid, chn_mid, 1, stride=1,
                             padding=0, bias=True), ]
        layers += [nn.LeakyReLU(0.2, True), ]
        layers += [nn.Conv2d(chn_mid, 1, 1, stride=1, padding=0, bias=True), ]
        if(use_sigmoid):
            layers += [nn.Sigmoid(), ]
        self.model = nn.Sequential(*layers)

    def forward(self, d0, d1, eps=0.1):
        return self.model.forward(
            torch.cat(
                (d0, d1, d0 - d1, d0 / (d1 + eps),
                 d1 / (d0 + eps)),
                dim=1))


class BCERankingLoss(nn.Module):
    def __init__(self, chn_mid=32):
        super(BCERankingLoss, self).__init__()
        self.net = Dist2LogitLayer(chn_mid=chn_mid)
        # self.parameters = list(self.net.parameters())
        self.loss = torch.nn.BCELoss()

    def forward(self, d0, d1, judge):
        per = (judge + 1.) / 2.
        self.logit = self.net.forward(d0, d1)
        return self.loss(self.logit, per)

# L2, DSSIM metrics


class FakeNet(nn.Module):
    def __init__(self, use_gpu=True, colorspace='Lab'):
        super(FakeNet, self).__init__()
        self.use_gpu = use_gpu
        self.colorspace = colorspace


class L2(FakeNet):

    def forward(self, in0, in1, retPerLayer=None):
        assert(in0.size()[0] == 1)  # currently only supports batchSize 1

        if(self.colorspace == 'RGB'):
            (N, C, X, Y) = in0.size()
            value = torch.mean(
                torch.mean(
                    torch.mean(
                        (in0 - in1)**2,
                        dim=1).view(
                        N,
                        1,
                        X,
                        Y),
                    dim=2).view(
                    N,
                    1,
                    1,
                    Y),
                dim=3).view(N)
            return value
        elif(self.colorspace == 'Lab'):
            value = util.l2(
                util.tensor2np(
                    util.tensor2tensorlab(
                        in0.data, to_norm=False)), util.tensor2np(
                    util.tensor2tensorlab(
                        in1.data, to_norm=False)), range=100.).astype('float')
            ret_var = Variable(torch.Tensor((value,)))
            if(self.use_gpu):
                ret_var = ret_var.cuda()
            return ret_var


class DSSIM(FakeNet):

    def forward(self, in0, in1, retPerLayer=None):
        assert(in0.size()[0] == 1)  # currently only supports batchSize 1

        if(self.colorspace == 'RGB'):
            value = util.dssim(
                1. * util.tensor2im(in0.data),
                1. * util.tensor2im(in1.data),
                range=255.).astype('float')
        elif(self.colorspace == 'Lab'):
            value = util.dssim(
                util.tensor2np(
                    util.tensor2tensorlab(
                        in0.data, to_norm=False)), util.tensor2np(
                    util.tensor2tensorlab(
                        in1.data, to_norm=False)), range=100.).astype('float')
        ret_var = Variable(torch.Tensor((value,)))
        if(self.use_gpu):
            ret_var = ret_var.cuda()
        return ret_var


def print_network(net):
    num_params = 0
    for param in net.parameters():
        num_params += param.numel()
    print('Network', net)
    print('Total number of parameters: %d' % num_params)
"
DeepPrivacy,__init__.py,"
import numpy as np
try:
    from skimage.measure import compare_ssim
except ImportError:
    from skimage.metrics import structural_similarity as compare_ssim
import torch
from torch.autograd import Variable

from . import dist_model


class PerceptualLoss(torch.nn.Module):
    # VGG using our perceptually-learned weights (LPIPS metric)
    def __init__(self, model='net-lin', net='alex', colorspace='rgb',
                 spatial=False, use_gpu=True, gpu_ids=[0]):
        # def __init__(self, model='net', net='vgg', use_gpu=True): # ""default""
        # way of using VGG as a perceptual loss
        super(PerceptualLoss, self).__init__()
#        print('Setting up Perceptual loss...')
        self.use_gpu = use_gpu
        self.spatial = spatial
        self.gpu_ids = gpu_ids
        self.model = dist_model.DistModel()
        self.model.initialize(
            model=model,
            net=net,
            use_gpu=use_gpu,
            colorspace=colorspace,
            spatial=self.spatial,
            gpu_ids=gpu_ids)
#        print('...[%s] initialized'%self.model.name())
#        print('...Done')

    def forward(self, pred, target, normalize=False):
        """"""
        Pred and target are Variables.
        If normalize is True, assumes the images are between [0,1] and then scales them between [-1,+1]
        If normalize is False, assumes the images are already between [-1,+1]

        Inputs pred and target are Nx3xHxW
        Output pytorch Variable N long
        """"""

        if normalize:
            target = 2 * target - 1
            pred = 2 * pred - 1

        return self.model.forward(target, pred)


def l2(p0, p1, range=255.):
    return .5 * np.mean((p0 / range - p1 / range)**2)


def psnr(p0, p1, peak=255.):
    return 10 * np.log10(peak**2 / np.mean((1. * p0 - 1. * p1)**2))


def dssim(p0, p1, range=255.):
    return (1 - compare_ssim(p0, p1, data_range=range, multichannel=True)) / 2.


def rgb2lab(in_img, mean_cent=False):
    from skimage import color
    img_lab = color.rgb2lab(in_img)
    if(mean_cent):
        img_lab[:, :, 0] = img_lab[:, :, 0] - 50
    return img_lab


def tensor2np(tensor_obj):
    # change dimension of a tensor object into a numpy array
    return tensor_obj[0].cpu().float().numpy().transpose((1, 2, 0))


def np2tensor(np_obj):
    # change dimenion of np array into tensor array
    return torch.Tensor(np_obj[:, :, :, np.newaxis].transpose((3, 2, 0, 1)))


def tensor2tensorlab(image_tensor, to_norm=True, mc_only=False):
    # image tensor to lab tensor
    from skimage import color

    img = tensor2im(image_tensor)
    img_lab = color.rgb2lab(img)
    if(mc_only):
        img_lab[:, :, 0] = img_lab[:, :, 0] - 50
    if(to_norm and not mc_only):
        img_lab[:, :, 0] = img_lab[:, :, 0] - 50
        img_lab = img_lab / 100.

    return np2tensor(img_lab)


def tensorlab2tensor(lab_tensor, return_inbnd=False):
    from skimage import color
    import warnings
    warnings.filterwarnings(""ignore"")

    lab = tensor2np(lab_tensor) * 100.
    lab[:, :, 0] = lab[:, :, 0] + 50

    rgb_back = 255. * np.clip(color.lab2rgb(lab.astype('float')), 0, 1)
    if(return_inbnd):
        # convert back to lab, see if we match
        lab_back = color.rgb2lab(rgb_back.astype('uint8'))
        mask = 1. * np.isclose(lab_back, lab, atol=2.)
        mask = np2tensor(np.prod(mask, axis=2)[:, :, np.newaxis])
        return (im2tensor(rgb_back), mask)
    else:
        return im2tensor(rgb_back)


def rgb2lab(input):
    from skimage import color
    return color.rgb2lab(input / 255.)
"
DeepPrivacy,base_model.py,"import torch


class BaseModel():
    def __init__(self):
        pass

    def name(self):
        return 'BaseModel'

    def initialize(self, use_gpu=True, gpu_ids=[0]):
        self.use_gpu = use_gpu
        self.gpu_ids = gpu_ids

    def forward(self):
        pass

    def optimize_parameters(self):
        pass

    def get_current_visuals(self):
        return self.input

    def get_current_errors(self):
        return {}

    def save(self, label):
        pass

    # helper saving function that can be used by subclasses
    def save_network(self, network, path, network_label, epoch_label):
        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)
        save_path = os.path.join(path, save_filename)
        torch.save(network.state_dict(), save_path)

    # helper loading function that can be used by subclasses
    def load_network(self, network, network_label, epoch_label):
        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)
        save_path = os.path.join(self.save_dir, save_filename)
        print('Loading network from %s' % save_path)
        network.load_state_dict(torch.load(save_path))

    def update_learning_rate():
        pass

    def get_image_paths(self):
        return self.image_paths

    def save_done(self, flag=False):
        np.save(os.path.join(self.save_dir, 'done_flag'), flag)
        np.savetxt(
            os.path.join(
                self.save_dir, 'done_flag'), [
                flag, ], fmt='%i')
"
DeepPrivacy,dist_model.py,"import numpy as np
import torch
import os
from .base_model import BaseModel

from . import networks_basic as networks


class DistModel(BaseModel):
    def name(self):
        return self.model_name

    def initialize(
            self, model='net-lin', net='alex', colorspace='Lab',
            pnet_rand=False, pnet_tune=False, model_path=None, use_gpu=True,
            printNet=False, spatial=False, is_train=False, lr=.0001, beta1=0.5,
            version='0.1', gpu_ids=[0]):
        '''
        INPUTS
            model - ['net-lin'] for linearly calibrated network
                    ['net'] for off-the-shelf network
                    ['L2'] for L2 distance in Lab colorspace
                    ['SSIM'] for ssim in RGB colorspace
            net - ['squeeze','alex','vgg']
            model_path - if None, will look in weights/[NET_NAME].pth
            colorspace - ['Lab','RGB'] colorspace to use for L2 and SSIM
            use_gpu - bool - whether or not to use a GPU
            printNet - bool - whether or not to print network architecture out
            spatial - bool - whether to output an array containing varying distances across spatial dimensions
            spatial_shape - if given, output spatial shape. if None then spatial shape is determined automatically via spatial_factor (see below).
            spatial_factor - if given, specifies upsampling factor relative to the largest spatial extent of a convolutional layer. if None then resized to size of input images.
            spatial_order - spline order of filter for upsampling in spatial mode, by default 1 (bilinear).
            is_train - bool - [True] for training mode
            lr - float - initial learning rate
            beta1 - float - initial momentum term for adam
            version - 0.1 for latest, 0.0 was original (with a bug)
            gpu_ids - int array - [0] by default, gpus to use
        '''
        BaseModel.initialize(self, use_gpu=use_gpu, gpu_ids=gpu_ids)

        self.model = model
        self.net = net
        self.is_train = is_train
        self.spatial = spatial
        self.gpu_ids = gpu_ids
        self.model_name = '%s [%s]' % (model, net)

        if(self.model == 'net-lin'):  # pretrained net + linear layer
            self.net = networks.PNetLin(
                pnet_rand=pnet_rand,
                pnet_tune=pnet_tune,
                pnet_type=net,
                use_dropout=True,
                spatial=spatial,
                version=version,
                lpips=True)
            kw = {}
            if not use_gpu:
                kw['map_location'] = 'cpu'

            if(not is_train):
                #                print('Loading model from: %s'%model_path)
                state_dict = torch.hub.load_state_dict_from_url(
                    ""http://folk.ntnu.no/haakohu/checkpoints/perceptual_similarity/alex.pth"", **kw)
                self.net.load_state_dict(state_dict, strict=False)

        elif(self.model == 'net'):  # pretrained network
            self.net = networks.PNetLin(
                pnet_rand=pnet_rand, pnet_type=net, lpips=False)
        elif(self.model in ['L2', 'l2']):
            # not really a network, only for testing
            self.net = networks.L2(use_gpu=use_gpu, colorspace=colorspace)
            self.model_name = 'L2'
        elif(self.model in ['DSSIM', 'dssim', 'SSIM', 'ssim']):
            self.net = networks.DSSIM(use_gpu=use_gpu, colorspace=colorspace)
            self.model_name = 'SSIM'
        else:
            raise ValueError(""Model [%s] not recognized."" % self.model)

        self.parameters = list(self.net.parameters())

        if self.is_train:  # training mode
            # extra network on top to go from distances (d0,d1) => predicted
            # human judgment (h*)
            self.rankLoss = networks.BCERankingLoss()
            self.parameters += list(self.rankLoss.net.parameters())
            self.lr = lr
            self.old_lr = lr
            self.optimizer_net = torch.optim.Adam(
                self.parameters, lr=lr, betas=(beta1, 0.999))
        else:  # test mode
            self.net.eval()

        if(use_gpu):
            self.net.to(gpu_ids[0])
            self.net = torch.nn.DataParallel(self.net, device_ids=gpu_ids)
            if(self.is_train):
                self.rankLoss = self.rankLoss.to(
                    device=gpu_ids[0])  # just put this on GPU0

        if(printNet):
            print('---------- Networks initialized -------------')
            networks.print_network(self.net)
            print('-----------------------------------------------')

    def forward(self, in0, in1, retPerLayer=False):
        ''' Function computes the distance between image patches in0 and in1
        INPUTS
            in0, in1 - torch.Tensor object of shape Nx3xXxY - image patch scaled to [-1,1]
        OUTPUT
            computed distances between in0 and in1
        '''

        return self.net.forward(in0, in1, retPerLayer=retPerLayer)"
DeepPrivacy,__init__.py,"import argparse
from .base import Config


def default_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser()
    parser.add_argument(""config_path"")
    return parser
"
DeepPrivacy,base.py,"import pathlib
import tempfile
import shutil
import sys
import json
from importlib import import_module
from addict import Dict
from deep_privacy import logger


def isfloat(x):
    try:
        float(x)
        return True
    except ValueError:
        return False


class ConfigDict(Dict):

    def __missing__(self, name):
        raise KeyError(name)

    def __getattr__(self, name):
        try:
            value = super(ConfigDict, self).__getattr__(name)
        except KeyError:
            ex = AttributeError(""'{}' object has no attribute '{}'"".format(
                self.__class__.__name__, name))
        except Exception as e:
            ex = e
        else:
            return value
        raise ex


class Config(object):
    """"""A facility for config and config files.

    It supports common file formats as configs: python/json/yaml. The interface
    is the same as a dict object and also allows access config values as
    attributes.

    Example:
        >>> cfg = Config(dict(a=1, b=dict(b1=[0, 1])))
        >>> cfg.a
        1
        >>> cfg.b
        {'b1': [0, 1]}
        >>> cfg.b.b1
        [0, 1]
        >>> cfg = Config.fromfile('tests/data/config/a.py')
        >>> cfg.filename
        ""/home/kchen/projects/mmcv/tests/data/config/a.py""
        >>> cfg.item4
        'test'
        >>> cfg
        ""Config [path: /home/kchen/projects/mmcv/tests/data/config/a.py]: ""
        ""{'item1': [1, 2], 'item2': {'a': 0}, 'item3': True, 'item4': 'test'}""

    """"""

    @staticmethod
    def _py2dict(filepath: pathlib.Path):

        assert filepath.is_file(), filepath
        with tempfile.TemporaryDirectory() as temp_config_dir:
            shutil.copyfile(filepath,
                            pathlib.Path(temp_config_dir, '_tempconfig.py'))
            sys.path.insert(0, temp_config_dir)
            mod = import_module('_tempconfig')
            sys.path.pop(0)
            cfg_dict = {
                name: value
                for name, value in mod.__dict__.items()
                if not name.startswith('__') and name != ""os""
            }
            # delete imported module
            del sys.modules['_tempconfig']

        cfg_text = f""{filepath}\n""
        with open(filepath, 'r') as f:
            cfg_text += f.read()
        if '_base_config_' in cfg_dict:
            cfg_dir = filepath.parent
            base_filename = cfg_dict.pop('_base_config_')
            base_filename = base_filename if isinstance(
                base_filename, list) else [base_filename]

            cfg_dict_list = list()
            cfg_text_list = list()
            for f in base_filename:
                _cfg_dict, _cfg_text = Config._file2dict(
                    cfg_dir.joinpath(f))
                cfg_dict_list.append(_cfg_dict)
                cfg_text_list.append(_cfg_text)

            base_cfg_dict = dict()
            for c in cfg_dict_list:
                if len(base_cfg_dict.keys() & c.keys()) > 0:
                    raise KeyError('Duplicate key is not allowed among bases')
                base_cfg_dict.update(c)

            Config._merge_a_into_b(cfg_dict, base_cfg_dict)
            cfg_dict = base_cfg_dict

            # merge cfg_text
            cfg_text_list.append(cfg_text)
            cfg_text = '\n'.join(cfg_text_list)
        return cfg_dict, cfg_text

    @staticmethod
    def _json2dict(filepath: pathlib.Path):
        with open(filepath, ""r"") as fp:
            return json.load(fp)

    @staticmethod
    def _file2dict(filepath):
        filepath = pathlib.Path(filepath)
        if filepath.suffix == "".py"":
            return Config._py2dict(filepath)
        if filepath.suffix == "".json"":
            return Config._json2dict(filepath), None
        raise ValueError(""Expected json or python file:"", filepath)

    @staticmethod
    def _merge_a_into_b(a, b):
        # merge dict a into dict b. values in a will overwrite b.
        for k, v in a.items():
            if isinstance(v, dict) and k in b:
                if not isinstance(b[k], dict):
                    raise TypeError(
                        'Cannot inherit key {} from base!'.format(k))
                Config._merge_a_into_b(v, b[k])
            else:
                if not k in b:
                    logger.warn(
                        f""Writing a key without a default value: key={k}"")
                b[k] = v

    @staticmethod
    def fromfile(filepath):
        cfg_dict, cfg_text = Config._file2dict(filepath)
        return Config(cfg_dict, cfg_text=cfg_text, filename=filepath)

    def __init__(self, cfg_dict=None, cfg_text=None, filename=None):
        if cfg_dict is None:
            cfg_dict = dict()
        elif not isinstance(cfg_dict, dict):
            raise TypeError('cfg_dict must be a dict, but got {}'.format(
                type(cfg_dict)))

        super(Config, self).__setattr__('_cfg_dict', ConfigDict(cfg_dict))
        super(Config, self).__setattr__('_filename', filename)
        if cfg_text:
            text = cfg_text
        elif filename:
            with open(filename, 'r') as f:
                text = f.read()
        else:
            text = ''
        super(Config, self).__setattr__('_text', text)

    @property
    def filename(self):
        return self._filename

    @property
    def model_name(self):
        return pathlib.Path(self._filename).stem

    @property
    def text(self):
        return self._text

    @property
    def output_dir(self):
        parts = pathlib.Path(self.filename).parts
        parts = [
            pathlib.Path(p).stem for p in parts
            if ""configs"" not in p]
        return pathlib.Path(self._output_dir, *parts)

    @property
    def cache_dir(self):
        return pathlib.Path(self._cache_dir)

    def __repr__(self):
        cfg_dict = super(Config, self).__getattribute__('_cfg_dict')
        return json.dumps(cfg_dict, indent=2)
        return 'Config (path: {}): {}'.format(self.filename,
                                              self._cfg_dict.__repr__())

    def __len__(self):
        return len(self._cfg_dict)

    def __getattr__(self, name):
        return getattr(self._cfg_dict, name)

    def __getitem__(self, name):
        return self._cfg_dict.__getitem__(name)

    def __setattr__(self, name, value):
        if isinstance(value, dict):
            value = ConfigDict(value)
        self._cfg_dict.__setattr__(name, value)

    def __setitem__(self, name, value):
        if isinstance(value, dict):
            value = ConfigDict(value)
        self._cfg_dict.__setitem__(name, value)

    def __iter__(self):
        return iter(self._cfg_dict)

    def dump(self):
        filepath = self.output_dir.joinpath(""config_dump.json"")
        self.output_dir.mkdir(exist_ok=True, parents=True)
        cfg_dict = super(Config, self).__getattribute__('_cfg_dict')
        with open(filepath, ""w"") as fp:
            json.dump(cfg_dict, fp, indent=4)

    def merge_from_dict(self, options):
        """""" Merge list into cfg_dict

        Merge the dict parsed by MultipleKVAction into this cfg.
        Example,
            >>> options = {'model.backbone.depth': 50}
            >>> cfg = Config(dict(model=dict(backbone=dict(type='ResNet'))))
            >>> cfg.merge_from_dict(options)

        Args:
            options (dict): dict of configs to merge from.
        """"""
        option_cfg_dict = {}
        for full_key, v in options.items():
            d = option_cfg_dict
            key_list = full_key.split('.')
            for subkey in key_list[:-1]:
                d[subkey] = ConfigDict()
                d = d[subkey]
            subkey = key_list[-1]
            d[subkey] = v

        cfg_dict = super(Config, self).__getattribute__('_cfg_dict')
        Config._merge_a_into_b(option_cfg_dict, cfg_dict)

    def merge_from_str(self, opts: str):
        if opts is None:
            return
        b = {}
        for opt in opts.split("",""):
            try:
                key, value = opt.split("" "")
            except ValueError:
                key, value = opt.split(""="")
            if isfloat(value):
                value = float(value)
            b[key] = value
        self.merge_from_dict(b)


if __name__ == ""__main__"":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument(""filepath"")
    cfg = Config.fromfile(parser.parse_args().filepath)
    print(""Ouput directory"", cfg.output_dir)
    cfg.dump()
"
DeepPrivacy,build.py,"from deep_privacy.utils import Registry, build_from_cfg
from .utils import fast_collate, DataPrefetcher, progressive_decorator
from .transforms import build_transforms
import torch
DATASET_REGISTRY = Registry(""DATASET"")


def get_dataloader(cfg, imsize, get_transition_value, is_train):
    cfg_data = cfg.data_val
    if is_train:
        cfg_data = cfg.data_train
    if cfg_data.dataset.type == ""MNISTDataset"":
        assert cfg.models.pose_size == 0
    transform = build_transforms(cfg_data.transforms, imsize=imsize)
    dataset = build_from_cfg(
        cfg_data.dataset,
        DATASET_REGISTRY,
        imsize=imsize,
        transform=transform
    )
    batch_size = cfg.trainer.batch_size_schedule[imsize]
    dataloader = torch.utils.data.DataLoader(
        dataset,
        pin_memory=False,
        collate_fn=fast_collate,
        batch_size=batch_size,
        **cfg_data.loader
    )
    dataloader = DataPrefetcher(
        dataloader,
        infinite_loader=is_train
    )
    # If progressive growing, perform GPU image interpolation
    if not cfg.trainer.progressive.enabled:
        return dataloader
    if get_transition_value is not None:
        assert cfg.trainer.progressive.enabled
    dataloader.next = progressive_decorator(
        dataloader.next,
        get_transition_value)
    return dataloader


def build_dataloader_train(
        cfg,
        imsize,
        get_transition_value=None):
    return get_dataloader(
        cfg, imsize,
        get_transition_value,
        is_train=True
    )


def build_dataloader_val(
        cfg,
        imsize,
        get_transition_value=None):
    return get_dataloader(
        cfg, imsize,
        get_transition_value,
        is_train=False
    )


if __name__ == ""__main__"":
    import argparse
    from deep_privacy.config import Config
    from . import *
    from deep_privacy import torch_utils
    from deep_privacy.visualization.utils import draw_faces_with_keypoints, np_make_image_grid
    from PIL import Image
    parser = argparse.ArgumentParser()
    parser.add_argument(""config_path"")
    args = parser.parse_args()
    cfg = Config.fromfile(args.config_path)
    imsize = cfg.models.max_imsize
    dl_train = build_dataloader_val(cfg, imsize, lambda: 1)
    batch = next(iter(dl_train))
    im0 = batch[""condition""]
    im1 = batch[""img""]

    im = torch.cat((im1, im0), axis=-1)
    im = torch_utils.image_to_numpy(
        im, to_uint8=True, denormalize=True)
    if ""landmarks"" in batch:
        lm = batch[""landmarks""].cpu().numpy() * imsize
        lm = lm.reshape(lm.shape[0], -1, 2)
        im = [
            draw_faces_with_keypoints(_, None, im_keypoints=[lm[i]], radius=3)
            for i, _ in enumerate(im)
        ]
    print([_.shape for _ in im])
    im = np_make_image_grid(im, nrow=10)
    im = Image.fromarray(im)
    im.show()
    im.save(""example.png"")
"
DeepPrivacy,celebaHQ.py,"from .mask_util import generate_mask
from .custom import CustomDataset
from .build import DATASET_REGISTRY


@DATASET_REGISTRY.register_module
class CelebAHQDataset(CustomDataset):

    def __init__(self, *args, is_train, **kwargs):
        super().__init__(*args, **kwargs)
        self.is_train = is_train

    def _load_impaths(self):
        image_dir = self.dirpath.joinpath(str(self.imsize))
        image_paths = list(image_dir.glob(""*.png""))
        image_paths.sort(key=lambda x: int(x.stem))
        return image_paths

    def get_mask(self, idx):
        return generate_mask(
            (self.imsize, self.imsize), fixed_mask=not self.is_train)
"
DeepPrivacy,custom.py,"import pathlib
from deep_privacy import logger
from .utils import read_image


class CustomDataset:

    def __init__(self,
                 dirpath,
                 imsize: int,
                 transform,
                 percentage: float):
        dirpath = pathlib.Path(dirpath)
        self.dirpath = dirpath
        self.transform = transform
        self._percentage = percentage
        self.imsize = imsize
        assert self.dirpath.is_dir(),\
            f""Did not find dataset at: {dirpath}""
        self.image_paths = self._load_impaths()
        self.filter_images()

        logger.info(
            f""Dataset loaded from: {dirpath}. Number of samples:{len(self)}, imsize={imsize}"")

    def _load_impaths(self):
        image_dir = self.dirpath.joinpath(""images"", str(self.imsize))
        image_paths = list(image_dir.glob(""*.png""))
        assert len(image_paths) > 0,\
            f""Did not find images in: {image_dir}""
        image_paths.sort(key=lambda x: int(x.stem))
        return image_paths

    def get_mask(self, idx):
        raise NotImplementedError

    def __len__(self):
        return len(self.image_paths)

    def get_image(self, index):
        impath = self.image_paths[index]
        im = read_image(impath)
        return im

    def get_item(self, index):
        image = self.get_image(index)
        masks = self.get_mask(index)
        return {
            ""img"": image,
            ""mask"": masks,
        }

    def __getitem__(self, index):
        batch = self.get_item(index)
        if self.transform is None:
            return batch
        return self.transform(batch)

    def filter_images(self) -> None:
        if 0 < self._percentage < 1:
            num_images = max(1, int(len(self.image_paths) * self._percentage))
            self.image_paths = self.image_paths[:num_images]
"
DeepPrivacy,__init__.py,"from .build import build_dataloader_train, build_dataloader_val
from .fdf import FDFDataset, FDFDensePoseDataset
from .mnist import MNISTDataset
from .celebaHQ import CelebAHQDataset
from .places2 import Places2Dataset
"
DeepPrivacy,fdf.py,"import pathlib
import numpy as np
import torch
from .build import DATASET_REGISTRY
from .custom import CustomDataset


def load_torch(filepath: pathlib.Path):
    assert filepath.is_file(),\
        f""Did not find file. Looked at: {filepath}""
    return torch.load(filepath)


@DATASET_REGISTRY.register_module
class FDFDataset(CustomDataset):

    def __init__(self,
                 *args,
                 **kwargs):
        super().__init__(*args, **kwargs)
        self.load_landmarks()
        self.load_bounding_box()

    def load_bounding_box(self):
        filepath = self.dirpath.joinpath(
            ""bounding_box"", f""{self.imsize}.torch"")
        bbox = load_torch(filepath)
        self.bounding_boxes = bbox[:len(self)]
        assert len(self.bounding_boxes) == len(self)

    def load_landmarks(self):
        filepath = self.dirpath.joinpath(""landmarks.npy"")
        assert filepath.is_file(), \
            f""Did not find landmarks at: {filepath}""
        landmarks = np.load(filepath).reshape(-1, 7, 2)
        landmarks = landmarks.astype(np.float32)
        self.landmarks = landmarks[:len(self)]
        assert len(self.landmarks) == len(self),\
            f""Number of images: {len(self)}, landmarks: {len(landmarks)}""

    def get_mask(self, idx):
        mask = np.ones((self.imsize, self.imsize), dtype=np.bool)
        bounding_box = self.bounding_boxes[idx]
        x0, y0, x1, y1 = bounding_box
        mask[y0:y1, x0:x1] = 0

        return mask

    def get_item(self, index):
        batch = super().get_item(index)
        landmark = self.landmarks[index]
        batch[""landmarks""] = landmark
        return batch


@DATASET_REGISTRY.register_module
class FDFDensePoseDataset(FDFDataset):

    def __init__(
            self,
            *args,
            **kwargs):
        super().__init__(*args, **kwargs)
        self.load_landmarks()
        self.load_mask()

    def load_mask(self):
        filepath = self.dirpath.joinpath(""mask"", f""{self.imsize}.npy"")
        assert filepath.is_file(),\
            f""Did not find mask at: {filepath}""
        masks = np.load(filepath)
        assert len(masks) == len(self)
        assert masks.dtype == np.bool
        self.masks = masks

    def get_item(self, index):
        batch = super().get_item(index)
        landmark = self.landmarks[index]
        batch[""landmarks""] = landmark
        return batch

    def get_mask(self, idx):
        return self.masks[idx]


@DATASET_REGISTRY.register_module
class FDFRetinaNetPose(FDFDataset):

    def filter_images(self):
        super().filter_images()
        discared_images_fp = self.dirpath.joinpath(""discared_images.txt"")
        with open(discared_images_fp, ""r"") as f:
            discared_indices = f.readlines()
            discared_indices = [
                int(_.strip()) for _ in discared_indices
                if _.strip() != """"]
            discared_indices = set(discared_indices)
        keep_indices = set(range(len(self.image_paths)))
        keep_indices = keep_indices.difference(discared_indices)
        self._keep_indices = keep_indices
        self.image_paths = [self.image_paths[idx] for idx in keep_indices]

    def load_landmarks(self):
        filepath = self.dirpath.joinpath(""retinanet_landmarks.npy"")
        assert filepath.is_file(), \
            f""Did not find landmarks at: {filepath}""
        landmarks = np.load(filepath).reshape(-1, 5, 2)
        landmarks = landmarks.astype(np.float32)
        landmarks = landmarks[np.array(list(self._keep_indices)), ::]
        self.landmarks = landmarks[:len(self)]
        assert len(self.landmarks) == len(self),\
            f""Number of images: {len(self)}, landmarks: {len(landmarks)}""

    def load_bounding_box(self):
        filepath = self.dirpath.joinpath(
            ""bounding_box"", f""{self.imsize}.torch"")
        bbox = load_torch(filepath)
        bbox = bbox[torch.tensor(list(self._keep_indices)), ::]
        self.bounding_boxes = bbox[:len(self)]
        assert len(self.bounding_boxes) == len(self)
"
DeepPrivacy,utils.py,"import numpy as np
import torch
from PIL import Image, ImageOps
from deep_privacy.modeling.models.utils import get_transition_value


def read_image(filepath, format=None):
    """"""
    Read an image into the given format.
    Will apply rotation and flipping if the image has such exif information.

    Args:
        file_name (str): image file path
        format (str): one of the supported image modes in PIL, or ""BGR""

    Returns:
        image (np.ndarray): an HWC image in the given format.
    """"""
    image = Image.open(filepath)

    # capture and ignore this bug:
    # https://github.com/python-pillow/Pillow/issues/3973
    try:
        image = ImageOps.exif_transpose(image)
    except Exception:
        pass

    if format is not None:
        # PIL only supports RGB, so convert to RGB and flip channels over below
        conversion_format = format
        if format == ""BGR"":
            conversion_format = ""RGB""
        image = image.convert(conversion_format)
    image = np.asarray(image)
    if format == ""BGR"":
        # flip channels if needed
        image = image[:, :, ::-1]
    # PIL squeezes out the channel dimension for ""L"", so make it HWC
    if format == ""L"":
        image = np.expand_dims(image, -1)
    return image


def fast_collate(batch):
    has_landmark = ""landmarks"" in batch[0]
    imshape = batch[0][""img""].shape[:2]
    images = torch.zeros(
        (len(batch), 3, *imshape), dtype=torch.uint8)
    masks = torch.zeros(
        (len(batch), 1, *imshape), dtype=torch.bool)
    if has_landmark:
        landmark = batch[0][""landmarks""]
        landmarks = torch.zeros(
            (len(batch), *landmark.shape), dtype=torch.float32
        )
    for i, sample in enumerate(batch):
        img = sample[""img""]
        img = np.rollaxis(img, 2)
        images[i] += torch.from_numpy(img.copy())

        mask = torch.from_numpy(sample[""mask""].copy())
        masks[i, 0] += mask
        if has_landmark:
            landmark = sample[""landmarks""]
            landmarks[i] += torch.from_numpy(landmark)
    res = {""img"": images, ""mask"": masks}
    if has_landmark:
        res[""landmarks""] = landmarks
    return res


class DataPrefetcher:

    def __init__(self,
                 loader: torch.utils.data.DataLoader,
                 infinite_loader: bool):
        self.original_loader = loader
        self.stream = torch.cuda.Stream()
        self.loader = iter(self.original_loader)
        self.infinite_loader = infinite_loader

    def _preload(self):
        try:
            self.container = next(self.loader)
            self.stop_iteration = False
        except StopIteration:
            if self.infinite_loader:
                self.loader = iter(self.original_loader)
                return self._preload()
            self.stop_iteration = True
            return
        with torch.cuda.stream(self.stream):
            for key, item in self.container.items():
                self.container[key] = item.cuda(non_blocking=True).float()
            self.container[""img""] = self.container[""img""] * 2 / 255 - 1
            self.container[""condition""] = self.container[""img""] * self.container[""mask""]

    def __len__(self):
        return len(self.original_loader)

    def __next__(self):
        return self.next()

    def next(self):
        torch.cuda.current_stream().wait_stream(self.stream)
        if self.stop_iteration:
            raise StopIteration
        container = self.container
        self._preload()
        return container

    def __iter__(self):
        self.loader = iter(self.original_loader)
        self._preload()
        return self

    def num_images(self):
        assert self.original_loader.drop_last
        return len(self.original_loader) * self.original_loader.batch_size

    @property
    def batch_size(self):
        return self.original_loader.batch_size


@torch.no_grad()
def interpolate_mask(mask, transition_variable):
    y = torch.nn.functional.avg_pool2d(mask, 2)
    y = torch.nn.functional.interpolate(y, scale_factor=2, mode=""nearest"")
    mask = get_transition_value(y, mask, transition_variable)
    mask = (mask >= 0.5).float()
    return mask


@torch.no_grad()
def interpolate_image(images, transition_variable):
    assert images.max() <= 1
    y = torch.nn.functional.avg_pool2d(images * 255, 2) // 1
    y = torch.nn.functional.interpolate(y, scale_factor=2)
    images = get_transition_value(y, images, transition_variable)
    return images


@torch.no_grad()
def interpolate_landmarks(landmarks, transition_variable, imsize):
    prev_landmarks = (landmarks * imsize / 2) // 1
    prev_landmarks = prev_landmarks / (imsize * 2)
    cur_landmarks = (landmarks * imsize) // 1
    cur_landmarks = cur_landmarks / imsize
    return get_transition_value(
        prev_landmarks, cur_landmarks, transition_variable)


def progressive_decorator(func, get_transition_value):
    def decorator(*args, **kwargs):
        batch = func(*args, **kwargs)
        img = batch[""img""]
        batch[""img""] = interpolate_image(img, get_transition_value())
        batch[""mask""] = interpolate_mask(
            batch[""mask""],
            get_transition_value()
        )
        if ""landmarks"" in batch:
            imsize = img.shape[-1]
            landmarks = batch[""landmarks""]
            batch[""landmarks""] = interpolate_landmarks(
                landmarks, get_transition_value(),
                imsize
            )
        return batch
    return decorator
"
DeepPrivacy,places2.py,"import os
import numpy as np
import pathlib
from .mask_util import generate_mask
from .custom import CustomDataset
from .build import DATASET_REGISTRY


@DATASET_REGISTRY.register_module
class Places2Dataset(CustomDataset):

    def __init__(self, *args, is_train: bool, **kwargs):
        super().__init__(*args, **kwargs)
        self.is_train = is_train

    def _load_impaths(self):
        relevant_suffixes = ["".png"", "".jpg"", "".jpeg""]
        image_dir = self.dirpath
        image_paths = []
        for dirpath, dirnames, filenames in os.walk(image_dir):
            for filename in filenames:
                path = pathlib.Path(dirpath, filename)
                if path.suffix in relevant_suffixes:
                    assert path.is_file()
                    image_paths.append(path)
        # Name format of: Places365_test_00136999.jpg
        image_paths.sort(key=lambda x: int(x.stem.split(""_"")[-1]))
        return image_paths

    def get_image(self, *args, **kwargs):
        im = super().get_image(*args, **kwargs)
        if len(im.shape) == 2:
            im = im[:, :, None]
            im = np.repeat(im, 3, axis=-1)
        return im

    def get_mask(self, idx):
        return generate_mask(
            (self.imsize, self.imsize), fixed_mask=not self.is_train)
"
DeepPrivacy,mask_util.py,"import math
import numpy as np
import typing
from PIL import Image, ImageDraw


def random_bbox(img_shape: typing.Tuple[int]) -> typing.Tuple[int]:
    # Numbers taken from https://github.com/JiahuiYu/generative_inpainting/
    # No description given, looks like max height/width of bbox

    img_height, img_width = img_shape
    height_bbox, width_bbox = img_height // 2, img_width // 2
    maxX = img_height - height_bbox
    maxY = img_width - width_bbox
    x0 = int(np.random.uniform(low=0, high=maxX))
    y0 = int(np.random.uniform(low=0, high=maxY))
    return (x0, y0, x0 + width_bbox, y0 + height_bbox)


def get_bbox_mask(img_shape: typing.Tuple[int],
                  fixed_mask: bool) -> np.ndarray:
    img_height, img_width = img_shape
    if fixed_mask:
        assert img_height == img_width
        x0 = img_width // 4
        x1 = x0 + img_width // 2
        bbox = (x0, x0, x1, x1)
    else:
        bbox = random_bbox(img_shape)
    mask = np.ones((img_height, img_width), dtype=bool)
    x0, y0, x1, y1 = bbox
    mask[y0:y1, x0:x1] = 0
    return mask


# Adapted from:
# https://github.com/JiahuiYu/generative_inpainting/blob/master/inpaint_ops.py
# License: Creative Commons Attribution-NonCommercial 4.0 International
def brush_stroke_mask(img_shape: typing.Tuple[int]) -> np.ndarray:
    """"""Generate mask tensor from bbox.

    Returns:
        tf.Tensor: output with shape [1, H, W, 1]

    """"""
    img_height, img_width = img_shape
    min_num_vertex = 4
    max_num_vertex = 12
    mean_angle = 2 * math.pi / 5
    angle_range = 2 * math.pi / 15

    # Code was hard-coded to 256x256.
    min_width = 12 / 256 * img_width
    max_width = 40 / 256 * img_height

    def generate_mask(H, W):
        average_radius = math.sqrt(H * H + W * W) / 8
        mask = Image.new('L', (W, H), 0)

        for _ in range(np.random.randint(1, 4)):
            num_vertex = np.random.randint(min_num_vertex, max_num_vertex)
            angle_min = mean_angle - np.random.uniform(0, angle_range)
            angle_max = mean_angle + np.random.uniform(0, angle_range)
            angles = []
            vertex = []
            for i in range(num_vertex):
                if i % 2 == 0:
                    angles.append(
                        2 *
                        math.pi -
                        np.random.uniform(
                            angle_min,
                            angle_max))
                else:
                    angles.append(np.random.uniform(angle_min, angle_max))

            h, w = mask.size
            vertex.append(
                (int(
                    np.random.randint(
                        0, w)), int(
                    np.random.randint(
                        0, h))))
            for i in range(num_vertex):
                r = np.random.normal(average_radius, average_radius//2)
                r = np.clip(r, 0, 2*average_radius)
                new_x = np.clip(vertex[-1][0] + r * math.cos(angles[i]), 0, w)
                new_y = np.clip(vertex[-1][1] + r * math.sin(angles[i]), 0, h)
                vertex.append((int(new_x), int(new_y)))

            draw = ImageDraw.Draw(mask)
            width = int(np.random.uniform(min_width, max_width))
            draw.line(vertex, fill=1, width=width)
            for v in vertex:
                draw.ellipse((v[0] - width // 2,
                              v[1] - width // 2,
                              v[0] + width // 2,
                              v[1] + width // 2),
                             fill=1)

        if np.random.normal() > 0:
            mask.transpose(Image.FLIP_LEFT_RIGHT)
        if np.random.normal() > 0:
            mask.transpose(Image.FLIP_TOP_BOTTOM)
        mask = np.asarray(mask, np.bool)
        mask = np.reshape(mask, (H, W))
        return 1 - mask
    return generate_mask(img_shape[0], img_shape[1])


def generate_mask(img_shape: typing.Tuple[int],
                  fixed_mask: bool) -> np.ndarray:
    bbox_mask = get_bbox_mask(img_shape, fixed_mask)
    if fixed_mask:
        return bbox_mask
    brush_mask = brush_stroke_mask(img_shape)
    mask = np.logical_and(bbox_mask, brush_mask)
    return mask.squeeze()
"
DeepPrivacy,mnist.py,"import torchvision
import numpy as np
from .build import DATASET_REGISTRY


@DATASET_REGISTRY.register_module
class MNISTDataset(torchvision.datasets.MNIST):

    def __init__(
            self,
            dirpath,
            imsize,
            transform,
            train,
            **kwargs):
        super().__init__(dirpath, train=train, download=True)
        self.transform = transform
        self.imsize = imsize

    def get_mask(self):
        mask = np.ones((self.imsize, self.imsize), dtype=np.bool)
        offset = self.imsize // 4
        mask[offset:-offset, offset:-offset] = 0
        return mask

    def __getitem__(self, index):
        im, _target = super().__getitem__(index)
        im = im.resize((self.imsize, self.imsize))
        im = np.array(im)[:, :, None]
        im = im.repeat(3, -1)
        mask = self.get_mask()
        if self.transform:
            im = self.transform(im)

        return {
            ""img"": im,
            ""mask"": mask,
            ""landmarks"": np.zeros((0)).astype(np.float32)
        }
"
DeepPrivacy,build.py,"import torchvision
from deep_privacy.utils import Registry, build_from_cfg

TRANSFORM_REGISTRY = Registry(""TRANSFORM"")


def build_transforms(transforms, imsize):
    transforms = [
        build_from_cfg(t, TRANSFORM_REGISTRY, imsize=imsize)
        for t in transforms]
    return torchvision.transforms.Compose(transforms)
"
DeepPrivacy,transforms.py,"import numpy as np
import albumentations
import cv2
from .build import TRANSFORM_REGISTRY


@TRANSFORM_REGISTRY.register_module
class RandomFlip:

    def __init__(self, flip_ratio=None, **kwargs):
        self.flip_ratio = flip_ratio
        if self.flip_ratio is None:
            self.flip_ratio = 0.5
        assert 0 <= self.flip_ratio <= 1

    def __call__(self, container):
        if np.random.rand() > self.flip_ratio:
            return container
        img = container[""img""]
        container[""img""] = np.flip(img, axis=1)
        if ""landmarks"" in container:
            landmarks_XY = container[""landmarks""]
            landmarks_XY[:, 0] = 1 - landmarks_XY[:, 0]
            container[""landmarks""] = landmarks_XY
        if ""mask"" in container:
            mask = container[""mask""]
            mask = np.flip(mask, axis=1)
            container[""mask""] = mask
        return container


@TRANSFORM_REGISTRY.register_module
class FlattenLandmark:

    def __init__(self, *args, **kwargs):
        return

    def __call__(self, container, **kwargs):
        assert ""landmarks"" in container,\
            f""Did not find landmarks in container. {container.keys()}""
        landmarks_XY = container[""landmarks""]
        landmarks_XY = landmarks_XY.reshape(-1)
        landmarks_XY.clip(-1, 1)
        container[""landmarks""] = landmarks_XY
        return container


def _resize(im, imsize):
    min_size = min(im.shape[:2])
    factor = imsize / min_size
    new_size = [int(size * factor) + 1 for size in im.shape[:2]]
    im = albumentations.augmentations.functional.resize(im, *new_size)
    return im


@TRANSFORM_REGISTRY.register_module
class RandomCrop:

    def __init__(self, imsize, **kwargs):
        self.imsize = imsize

    def __call__(self, container, **kwargs):
        im = container[""img""]
        if any(size < self.imsize for size in im.shape[:2]):
            im = _resize(im, self.imsize)
        im = albumentations.augmentations.functional.random_crop(
            im, self.imsize, self.imsize, 0, 0)
        container[""img""] = im
        return container


@TRANSFORM_REGISTRY.register_module
class CenterCrop:

    def __init__(self, imsize, **kwargs):
        self.imsize = imsize

    def __call__(self, container, **kwargs):
        im = container[""img""]
        if any(size < self.imsize for size in im.shape[:2]):
            im = _resize(im, self.imsize)
        im = albumentations.augmentations.functional.center_crop(
            im, self.imsize, self.imsize)
        container[""img""] = im
        return container


@TRANSFORM_REGISTRY.register_module
class RandomResize:

    def __init__(self, resize_ratio, min_imsize: int, max_imsize: int,
                 imsize: int, **kwargs):
        self.resize_ratio = resize_ratio
        imsize = min(min_imsize, imsize)
        self.possible_shapes = []
        while imsize <= max_imsize:
            self.possible_shapes.append(imsize)
            imsize *= 2

    def __call__(self, container, **kwargs):
        if np.random.rand() > self.resize_ratio:
            return container
        im = container[""img""]
        shape = im.shape
        imsize = np.random.choice(self.possible_shapes)
        orig_imsize = im.shape[0]
        im = albumentations.augmentations.functional.resize(
            im, imsize, imsize, interpolation=cv2.INTER_LINEAR)
        im = albumentations.augmentations.functional.resize(
            im, orig_imsize, orig_imsize, interpolation=cv2.INTER_LINEAR)
        assert im.shape == shape
        container[""img""] = im
        return container
"
DeepPrivacy,__init__.py,"from .transforms import RandomFlip, FlattenLandmark, CenterCrop, RandomCrop
from .build import build_transforms
"
DeepPrivacy,registry.py,"from functools import partial


class Registry(object):

    def __init__(self, name):
        self._name = name
        self._module_dict = dict()

    def __repr__(self):
        format_str = self.__class__.__name__ + '(name={}, items={})'.format(
            self._name, list(self._module_dict.keys()))
        return format_str

    @property
    def name(self):
        return self._name

    @property
    def module_dict(self):
        return self._module_dict

    def get(self, key):
        obj = self._module_dict.get(key, None)
        if obj is None:
            raise KeyError(
                f'{key} is not in the {self._name} registry.')
        return obj

    def _register_module(self, module_class, force=False):
        """"""Register a module.

        Args:
            module (:obj:`nn.Module`): Module to be registered.
        """"""
        if not isinstance(module_class, type):
            raise TypeError('module must be a class, but got {}'.format(
                type(module_class)))
        module_name = module_class.__name__
        if not force and module_name in self._module_dict:
            raise KeyError('{} is already registered in {}'.format(
                module_name, self.name))
        self._module_dict[module_name] = module_class

    def register_module(self, cls=None, force=False):
        if cls is None:
            return partial(self.register_module, force=force)
        self._register_module(cls, force=force)
        return cls


def build_from_cfg(_cfg, registry, **kwargs):
    """"""Build a module from config dict.

    Args:
        cfg (dict): Config dict. It should at least contain the key ""type"".
        registry (:obj:`Registry`): The registry to search the type from.
        default_args (dict, optional): Default initialization arguments.

    Returns:
        obj: The constructed object.
    """"""
    assert isinstance(_cfg, dict) and 'type' in _cfg
    args = _cfg.copy()
    obj_type = args.pop('type')
    if isinstance(obj_type, str):
        obj_cls = registry.get(obj_type)
        if obj_cls is None:
            raise KeyError('{} is not in the {} registry'.format(
                obj_type, registry.name))
    elif isinstance(obj_type, type):
        obj_cls = obj_type
    else:
        raise TypeError('type must be a str or valid type, but got {}'.format(
            type(obj_type)))
    return obj_cls(**args, **kwargs)
"
DeepPrivacy,bufferless_videocapture.py,"import queue
import threading
import cv2


class BufferlessVideoCapture:

    def __init__(self, name, width=None, height=None):
        self.cap = cv2.VideoCapture(name)
        if width is not None and height is not None:
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)
        self.q = queue.Queue()
        t = threading.Thread(target=self._reader)
        t.daemon = True
        t.start()

    # read frames as soon as they are available, keeping only most recent one
    def _reader(self):
        while True:
            ret, frame = self.cap.read()
            if not ret:
                break
            if not self.q.empty():
                try:
                    self.q.get_nowait()   # discard previous (unprocessed) frame
                except queue.Empty:
                    pass
            self.q.put((ret, frame))

    def read(self):
        return self.q.get()
"
DeepPrivacy,__init__.py,"#from .utils import *
from .registry import Registry, build_from_cfg
from .utils import read_im
from .bufferless_videocapture import BufferlessVideoCapture
"
DeepPrivacy,utils.py,"import pathlib
import numpy as np
import cv2
try:
    from apex.amp._amp_state import _amp_state
except ImportError:
    pass


def amp_state_has_overflow():
    for loss_scaler in _amp_state.loss_scalers:
        if loss_scaler._has_overflow:
            return True
    return False


def read_im(impath: pathlib.Path, imsize: int = None):
    assert impath.is_file(),\
        f""Image path is not file: {impath}""
    im = cv2.imread(str(impath))[:, :, ::-1]
    if imsize is not None:
        im = cv2.resize(im, (imsize, imsize))
    if im.dtype == np.uint8:
        im = im.astype(np.float32) / 255
    assert im.max() <= 1 and im.min() >= 0
    return im
"
DeepPrivacy,__init__.py,
DeepPrivacy,build.py,"from deep_privacy.utils import Registry

CRITERION_REGISTRY = Registry(""CRITERION"")
"
DeepPrivacy,adversarial_loss.py,"import torch
from .build import CRITERION_REGISTRY
from typing import Dict, Tuple


class GanCriterion:

    NEED_REAL_SCORE_GENERATOR = False
    REQUIRES_D_SCORE = True

    def __init__(self, fake_index: int, *args, **kwargs):
        """"""
            fake_index: indicates which fake sample to use.
            Used in case for two-stage inpaintors (such as GatedConvolution)
        """"""
        self.fake_index = fake_index
        return

    def d_loss(self, batch: dict) -> Tuple[torch.Tensor, Dict[str, float]]:
        return None

    def g_loss(self, batch: dict) -> Tuple[torch.Tensor, Dict[str, float]]:
        return None


@CRITERION_REGISTRY.register_module
class WGANCriterion(GanCriterion):

    def d_loss(self, batch):
        real_scores = batch[""real_scores""]
        fake_scores = batch[""fake_scores""][self.fake_index]
        wasserstein_distance = 0
        for real, fake in zip(real_scores, fake_scores):
            wasserstein_distance += (real - fake)
        to_log = {
            ""wasserstein_distance"": wasserstein_distance.detach()
        }
        return (-wasserstein_distance).view(-1), to_log

    def g_loss(self, batch):
        fake_scores = batch[""fake_scores""][self.fake_index]
        g_loss = 0
        for fake_score in fake_scores:
            g_loss -= fake_score
        g_loss = g_loss.view(-1)
        to_log = dict(
            g_loss=g_loss
        )
        return g_loss, to_log


@CRITERION_REGISTRY.register_module
class RGANCriterion(GanCriterion):

    NEED_REAL_SCORE_GENERATOR = True

    def __init__(self):
        super().__init__()
        self.bce_stable = torch.nn.BCEWithLogitsLoss(reduction=""none"")

    def d_loss(self, batch):
        real_scores = batch[""real_scores""][:, 0]
        fake_scores = batch[""fake_scores""][self.fake_index][:, 0]
        wasserstein_distance = (real_scores - fake_scores).squeeze()
        target = torch.ones_like(real_scores)
        d_loss = self.bce_stable(real_scores - fake_scores, target)
        to_log = {
            ""wasserstein_distance"": wasserstein_distance.mean().detach(),
            ""d_loss"": d_loss.mean().detach()
        }
        return d_loss.view(-1), to_log

    def g_loss(self, batch):
        real_scores = batch[""real_scores""][:, 0]
        fake_scores = batch[""fake_scores""][self.fake_index][:, 0]
        target = torch.ones_like(real_scores)
        g_loss = self.bce_stable(fake_scores - real_scores, target)
        to_log = dict(
            g_loss=g_loss.mean()
        )
        return g_loss.view(-1), to_log


@CRITERION_REGISTRY.register_module
class RaGANCriterion(GanCriterion):

    NEED_REAL_SCORE_GENERATOR = True

    def __init__(self):
        super().__init__()
        self.bce_stable = torch.nn.BCEWithLogitsLoss(reduction=""none"")

    def d_loss(self, batch):
        real_scores = batch[""real_scores""][:, 0]
        fake_scores = batch[""fake_scores""][self.fake_index][:, 0]
        wasserstein_distance = (real_scores - fake_scores).squeeze()
        target = torch.ones_like(real_scores)
        target2 = torch.zeros_like(real_scores)
        d_loss = self.bce_stable(real_scores - fake_scores.mean(), target) + \
            self.bce_stable(fake_scores - real_scores.mean(), target2)
        to_log = {
            ""wasserstein_distance"": wasserstein_distance.mean().detach(),
            ""d_loss"": d_loss.mean().detach()
        }
        return (d_loss / 2).view(-1), to_log

    def g_loss(self, batch):
        real_scores = batch[""real_scores""][:, 0]
        fake_scores = batch[""fake_scores""][self.fake_index][:, 0]
        target = torch.ones_like(real_scores)
        target2 = torch.zeros_like(real_scores)
        g_loss = self.bce_stable(real_scores - fake_scores.mean(), target2) + \
            self.bce_stable(fake_scores - real_scores.mean(), target)
        to_log = dict(
            g_loss=g_loss.mean()
        )
        return (g_loss / 2).view(-1), to_log


@CRITERION_REGISTRY.register_module
class NonSaturatingCriterion(GanCriterion):

    def d_loss(self, batch):
        real_scores = batch[""real_scores""][:, 0]
        fake_scores = batch[""fake_scores""][self.fake_index][:, 0]
        wasserstein_distance = (real_scores - fake_scores).squeeze()
        loss = torch.nn.functional.softplus(-real_scores) \
            + torch.nn.functional.softplus(fake_scores)
        return loss.view(-1), dict(
            wasserstein_distance=wasserstein_distance.mean().detach(),
            d_loss=loss.mean()
        )

    def g_loss(self, batch):
        fake_scores = batch[""fake_scores""][self.fake_index][:, 0]
        loss = torch.nn.functional.softplus(-fake_scores).mean()
        return loss.view(-1), dict(
            g_loss=loss
        )
"
DeepPrivacy,__init__.py,"from .build import CRITERION_REGISTRY
from .adversarial_loss import WGANCriterion, RGANCriterion, RaGANCriterion
from .loss import GradientPenalty, EpsilonPenalty, PosePredictionPenalty, L1Loss
from .optimizer import LossOptimizer"
DeepPrivacy,loss.py,"import torch
from .build import CRITERION_REGISTRY
from deep_privacy.modeling import models
from .adversarial_loss import GanCriterion


@CRITERION_REGISTRY.register_module
class GradientPenalty(GanCriterion):

    def __init__(self,
                 lambd: float,
                 mask_region_only: bool,
                 norm: str,
                 distance: str,
                 discriminator,
                 lazy_regularization: bool,
                 lazy_reg_interval: int,
                 mask_decoder_gradient: bool,
                 *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.discriminator = discriminator
        if mask_decoder_gradient:
            assert isinstance(discriminator, models.UNetDiscriminator)
        self._mask_decoder_gradient = mask_decoder_gradient
        self.lazy_reg_interval = lazy_reg_interval
        self.mask_region_only = mask_region_only
        self._norm = norm
        self._lambd = lambd
        if lazy_regularization:
            self._lambd *= lazy_reg_interval
        else:
            self.lazy_reg_interval = 1
        self._distance = distance
        assert self._norm in [""L2"", ""Linf""]
        self.it = 0

    def clip(self, activation):
        if self._distance == ""clamp"":
            return torch.nn.functional.relu(activation)
        assert self._distance == ""L2""
        return activation.pow(2)

    def norm(self, grad):
        if self._norm == ""L2"":  # L2 Norm
            grad_norm = grad.norm(p=2, dim=1)
        else:  # Linfinity norm
            grad_abs = grad.abs()
            grad_norm, _ = torch.max(grad_abs, dim=1)
        return grad_norm

    def d_loss(self, batch):
        self.it += 1
        if self.it % self.lazy_reg_interval != 0:
            return None, None
        real_data = batch[""img""]
        fake_data = batch[""fake_data""][self.fake_index]
        mask = batch[""mask""]
        epsilon_shape = [real_data.shape[0]] + [1] * (real_data.dim() - 1)
        epsilon = torch.rand(epsilon_shape)
        epsilon = epsilon.to(fake_data.device, fake_data.dtype)
        real_data = real_data.to(fake_data.dtype)
        x_hat = epsilon * real_data + (1 - epsilon) * fake_data.detach()
        x_hat.requires_grad = True
        logits = self.discriminator.forward_fake(**batch, fake_img=x_hat)
        to_backward = 0
        to_log = {}
        for idx, logit in enumerate(logits):
            if self._mask_decoder_gradient and idx == 1:
                assert logit.shape == mask.shape
                logit = ((1 - mask) * logit).view(x_hat.shape[0], -1).sum(dim=1)
                denom = (1 - mask).view(x_hat.shape[0], -1).sum(dim=1) + 1e-7
                logit = (logit / denom)
#            logit = logit.sum()
            grad = torch.autograd.grad(
                outputs=logit,
                inputs=x_hat,
                grad_outputs=torch.ones_like(logit),
                create_graph=True,
                only_inputs=True
            )[0]
            if self.mask_region_only:
                mask = batch[""mask""]
                expected_shape = (real_data.shape[0], 1, *real_data.shape[2:])
                assert mask.shape == expected_shape, \
                    f""Expected shape: {expected_shape}. Got: {mask.shape}""
                grad = grad * (1 - mask)
            grad = grad.view(x_hat.shape[0], -1)
            grad_norm = self.norm(grad)
            gradient_pen = (grad_norm - 1)
            gradient_pen = self.clip(gradient_pen)
            to_backward += gradient_pen * self._lambd
            tag = ""gradient_penalty""
            if idx > 0:
                tag = f""{tag}_{idx}""
            to_log[tag] = gradient_pen.mean().detach()
        x_hat.requires_grad = False
        return to_backward.view(-1), to_log


@CRITERION_REGISTRY.register_module
class EpsilonPenalty(GanCriterion):

    def __init__(self, weight, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.weight = weight

    def d_loss(self, batch):
        real_scores = batch[""real_scores""]
        real_scores = real_scores
        epsilon_penalty = 0
        for real in real_scores:
            epsilon_penalty += real.pow(2)
        to_log = dict(
            epsilon_penalty=epsilon_penalty.mean().detach()
        )
        return epsilon_penalty.view(-1), to_log


@CRITERION_REGISTRY.register_module
class PosePredictionPenalty(GanCriterion):

    def __init__(self, weight):
        self.weight = weight

    def d_loss(self, batch):
        real_pose_pred = batch[""real_scores""][:, 1:]
        fake_pose_pred = batch[""fake_scores""][self.fake_index][:, 1:]
        landmarks = batch[""landmarks""].clone()
        # Normalize output to have a mean of 0
        landmarks = landmarks * 2 - 1
        real_pose_loss = (landmarks - real_pose_pred)**2
        fake_pose_loss = (landmarks - fake_pose_pred)**2
        to_log = dict(
            real_pose_loss=real_pose_loss.mean().detach(),
            fake_pose_loss=fake_pose_loss.mean().detach()
        )
        to_backward = ((real_pose_loss + fake_pose_loss) * 0.5)
        return to_backward.view(-1), to_log


@CRITERION_REGISTRY.register_module
class L1Loss(GanCriterion):

    REQUIRES_D_SCORE = False

    def __init__(self, weight, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.weight = weight

    def g_loss(self, batch: dict):
        real = batch[""img""]
        fake = batch[""fake_data""][self.fake_index]
        mask = batch[""mask""]
        l1_loss = torch.abs((real - fake) * (1 - mask)).view(real.shape[0], -1)
        denom = (1 - mask).view(real.shape[0], -1).sum(dim=1)
        l1_loss = l1_loss.sum(dim=1) / denom
        l1_loss = l1_loss * self.weight
        return l1_loss, dict(
            l1_loss=l1_loss.detach()
        )
"
DeepPrivacy,optimizer.py,"import torch
import typing
import numpy as np
from deep_privacy import torch_utils
from deep_privacy.modeling import models
from deep_privacy.utils import build_from_cfg
from .build import CRITERION_REGISTRY
from typing import Tuple
from .loss import GradientPenalty, GanCriterion
try:
    from apex import amp
    from apex.optimizers import FusedAdam
except ImportError:
    pass


class LossOptimizer:

    def __init__(self,
                 discriminator: models.discriminator.Discriminator,
                 generator: models.generator.Generator,
                 criterions_D: typing.List[GanCriterion],
                 criterions_G: typing.List[GanCriterion],
                 learning_rate: float,
                 amp_opt_level: str,
                 lazy_regularization: bool):
        self.generator = generator
        self.discriminator = discriminator
        self.criterions_D = criterions_D
        self.criterions_G = criterions_G
        self.it = 0
        self._amp_opt_level = amp_opt_level
        self._lazy_regularization = lazy_regularization
        self._learning_rate = learning_rate
        self.init_optimizers()
        # For Two-Stage inpaintors we might want a discriminator score for
        # several outputs
        self.required_D_index = list(set([
            c.fake_index for c in criterions_D + criterions_G
            if c.REQUIRES_D_SCORE]))

    def state_dict(self):
        return {
            ""d_optimizer"": self.d_optimizer.state_dict(),
            ""g_optimizer"": self.g_optimizer.state_dict(),
        }

    def load_state_dict(self, state_dict):
        self.d_optimizer.load_state_dict(state_dict[""d_optimizer""])
        self.g_optimizer.load_state_dict(state_dict[""g_optimizer""])

    @staticmethod
    def build_from_cfg(cfg, discriminator, generator):
        lazy_regularization = cfg.trainer.optimizer.lazy_regularization
        criterions_D = [
            build_from_cfg(
                criterion, CRITERION_REGISTRY,
                discriminator=discriminator,
                lazy_regularization=lazy_regularization)
            for criterion in cfg.discriminator_criterions.values()
            if criterion is not None
        ]
        criterions_G = [
            build_from_cfg(
                criterion, CRITERION_REGISTRY, discriminator=discriminator)
            for criterion in cfg.generator_criterions.values()
            if criterion is not None
        ]
        return LossOptimizer(
            discriminator, generator, criterions_D, criterions_G,
            **cfg.trainer.optimizer)

    def init_optimizers(self) -> Tuple[torch.nn.Module]:
        torch_utils.to_cuda(
            [self.generator, self.discriminator])
        betas_d = (0.0, 0.99)
        lr_d = self._learning_rate
        if self._lazy_regularization:
            lazy_interval = [
                criterion.lazy_reg_interval
                for criterion in self.criterions_D
                if isinstance(criterion, GradientPenalty)]
            assert len(lazy_interval) <= 1
            if len(lazy_interval) == 1:
                lazy_interval = lazy_interval[0]
                c = lazy_interval / (lazy_interval + 1)
                betas_d = [beta ** c for beta in betas_d]
                lr_d *= c

        self.d_optimizer = FusedAdam(self.discriminator.parameters(),
                                     lr=lr_d,
                                     betas=betas_d)
        self.g_optimizer = FusedAdam(self.generator.parameters(),
                                     lr=self._learning_rate,
                                     betas=(0.0, 0.99))

    def initialize_amp(self):
        """"""
            Have to call initialize AMP from trainer since it changes the reference to generator / discriminator?
        """"""
        [self.generator, self.discriminator], [self.g_optimizer, self.d_optimizer] = amp.initialize(
            [self.generator, self.discriminator],
            [self.g_optimizer, self.d_optimizer],
            opt_level=self._amp_opt_level,
            num_losses=len(self.criterions_D) + len(self.criterions_G),
            max_loss_scale=2.**17,
        )
        return self.generator, self.discriminator

    def step(self, batch):
        losses_d = self.step_D(batch)
        losses_g = self.step_G(batch)
        if losses_d is None or losses_g is None:
            return None
        self.it += 1
        losses = {**losses_d, **losses_g}
        return losses

    def _backward(self, batch, loss_funcs, model, optimizer, id_offset):
        log = {}
        for param in model.parameters():
            param.grad = None
        for i, loss_fnc in enumerate(loss_funcs):
            loss, to_log = loss_fnc(batch)
            if loss is None:
                continue
            log.update(to_log)
            retain_graph = len(loss_funcs) - 1 != i
            l_id = id_offset + i
            loss = loss.mean()
            with amp.scale_loss(loss, optimizer, loss_id=l_id) as scaled_loss:
                scaled_loss.backward(retain_graph=retain_graph)
        optimizer.step()
        return {key: item.mean().detach() for key, item in log.items()}

    def step_D(self, batch):
        # Forward discriminator
        if len(self.required_D_index) == 0:
            return {}
        with torch.no_grad():
            fake_data = self.generator.forward_train(**batch)
        real_scores = self.discriminator(
            **batch, with_pose=True)

        fake_scores = {}
        for idx in self.required_D_index:
            fake_scores[idx] = self.discriminator.forward_fake(
                **batch, with_pose=True, fake_img=fake_data[idx])

        batch = {key: item for key, item in batch.items()}
        batch[""fake_data""] = fake_data
        batch[""real_scores""] = real_scores
        batch[""fake_scores""] = fake_scores
        # Backward
        loss_funcs = [c.d_loss for c in self.criterions_D]
        log = self._backward(
            batch,
            loss_funcs, self.discriminator,
            self.d_optimizer,
            id_offset=0
        )
        for i in range(len(real_scores)):
            log[f""real_score{i}""] = real_scores[i].mean().detach()
        for _, fake_score in fake_scores.items():
            log[f""fake_score{i}""] = fake_score.mean().detach()
        return log

    def step_G(self, batch):
        for p in self.discriminator.parameters():
            p.requires_grad = False
        # Forward
        fake_data = self.generator.forward_train(**batch)
        fake_scores = {}
        for idx in self.required_D_index:
            fake_scores[idx] = self.discriminator.forward_fake(
                **batch, fake_img=fake_data[idx])
        batch = {key: item for key, item in batch.items()}
        batch[""fake_data""] = fake_data
        batch[""fake_scores""] = fake_scores
        self.mask = batch[""mask""]
        if any(c.NEED_REAL_SCORE_GENERATOR for c in self.criterions_G):
            real_scores = self.discriminator(**batch)
            batch[""real_scores""] = real_scores
        loss_funcs = [c.g_loss for c in self.criterions_G]
        log = self._backward(
            batch,
            loss_funcs,
            self.generator,
            self.g_optimizer,
            id_offset=len(self.criterions_D),
        )
        del self.mask
        for p in self.discriminator.parameters():
            p.requires_grad = True
        return log
"
DeepPrivacy,build.py,"from deep_privacy.utils import build_from_cfg, Registry
from .utils import NetworkWrapper

DISCRIMINATOR_REGISTRY = Registry(""DISCRIMINATOR_REGISTRY"")
GENERATOR_REGISTRY = Registry(""GENERATOR_REGISTRY"")


def build_discriminator(
        cfg,
        data_parallel):
    discriminator = build_from_cfg(
        cfg.models.discriminator, DISCRIMINATOR_REGISTRY,
        cfg=cfg,
        max_imsize=cfg.models.max_imsize,
        pose_size=cfg.models.pose_size,
        image_channels=cfg.models.image_channels,
        conv_size=cfg.models.conv_size)
    if data_parallel:
        discriminator = NetworkWrapper(discriminator)
    discriminator = extend_model(cfg, discriminator)
    return discriminator


def build_generator(
        cfg,
        data_parallel):
    generator = build_from_cfg(
        cfg.models.generator, GENERATOR_REGISTRY,
        cfg=cfg,
        max_imsize=cfg.models.max_imsize,
        conv_size=cfg.models.conv_size,
        image_channels=cfg.models.image_channels,
        pose_size=cfg.models.pose_size)
    if data_parallel:
        generator = NetworkWrapper(generator)
    generator = extend_model(cfg, generator)
    return generator


def extend_model(cfg, model):
    while model.current_imsize < cfg.models.min_imsize:
        model.extend()
    if cfg.trainer.progressive.enabled:
        return model
    while model.current_imsize < cfg.models.max_imsize:
        model.extend()
    return model
"
DeepPrivacy,__init__.py,"from .build import build_discriminator, build_generator
from .base import ProgressiveBase
from .utils import NetworkWrapper
from .discriminator import Discriminator
from .generator import Generator
try:
    from .experimental import UNetDiscriminator
except ImportError:
    pass
"
DeepPrivacy,iconv.py,"import torch
from . import layers

eps = 1e-2


pconv_settings = {
    4: {""dilation"": 1, ""kernel_size"": 3},
    8: {""dilation"": 1, ""kernel_size"": 3},
    16: {""dilation"": 1, ""kernel_size"": 5},
    32: {""dilation"": 1, ""kernel_size"": 5},
    64: {""dilation"": 1, ""kernel_size"": 5},
    128: {""dilation"": 1, ""kernel_size"": 5},
    256: {""dilation"": 1, ""kernel_size"": 5},
    None: {""dilation"": 1, ""kernel_size"": 5}
}


class DownSample(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.pool = torch.nn.AvgPool2d(2)

    def forward(self, _inp):
        x, mask, *args = _inp
        new_mask = self.pool(mask)
        x = x * mask
        x = self.pool(x) / (new_mask + eps)
        if len(args) > 0:
            return (x, new_mask, *args)
        return x, new_mask


class ExpectedValue(torch.nn.Module):

    def __init__(self, num_channels: int, resolution: int):
        super().__init__()
        stride = 1
        kernel_size = pconv_settings[resolution][""kernel_size""]
        dilation = pconv_settings[resolution][""dilation""]
        padding = layers.get_padding(kernel_size, dilation, stride)
        self._resolution = resolution
        if dilation > 1:
            self.avg_pool = torch.nn.Conv2d(
                1, 1, kernel_size=kernel_size,
                stride=stride,
                padding=padding,
                bias=False,
                dilation=dilation
            )
            self.avg_pool.weight.requires_grad = False
            self.avg_pool.weight.data = self.avg_pool.weight.data.zero_() + 1 / kernel_size**2
        else:
            self.avg_pool = torch.nn.AvgPool2d(
                kernel_size, stride, padding=padding)
        self.input_updater = layers.Conv2d(
            num_channels, num_channels, kernel_size, stride, padding,
            groups=num_channels, dilation=dilation, wsconv=True
        )

    def extra_repr(self, *args, **kwargs):
        return f""resolution: {self._resolution}""

    def forward(self, x, mask):
        weighted = x * mask
        weighted, _ = self.input_updater((weighted, None))
        prob_sum = self.avg_pool(mask) + eps
        return weighted / prob_sum


class IConv(layers.Conv2d):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1,
                 padding=None, dilation=1, resolution=None, *args, **kwargs):
        super().__init__(
            in_channels, out_channels, kernel_size, stride,
            padding, dilation, *args, **kwargs)
        self.return_mask = True
        self.conv1x1 = kernel_size == 1 and padding in [0, None]

        self.expected_value_updater = ExpectedValue(
            in_channels, resolution=resolution)
        if self.conv1x1:
            return
        self.kernel_size = kernel_size

        self.mask_updater = torch.nn.Conv2d(
            1, 1,
            self.kernel_size,
            padding=layers.get_padding(kernel_size, dilation, 1),
            dilation=dilation, bias=False)
        self.mask_updater.weight.data = self.mask_updater.weight.data.zero_() + 1
        self.mask_activation = torch.nn.Sigmoid()

    def forward(self, _inp):
        x, mask_in = _inp
        expected_x = self.expected_value_updater(x, mask_in)
        predicted_x = x * mask_in + expected_x * (1 - mask_in)
        output, mask_in = super().forward((predicted_x, mask_in))
        if self.conv1x1:
            return output, mask_in

        new_mask = self.mask_updater(mask_in)
        new_mask = self.mask_activation(new_mask)
        new_mask = (new_mask - 0.5) * 2 + 1e-6
        new_mask = new_mask.clamp(0, 1)  # Fix potential roundoff errors.
        assert output.shape[2:] == new_mask.shape[2:],\
            f""Output shape: {output.shape}, new_mask: {new_mask.shape}""
        return output, new_mask
"
DeepPrivacy,utils.py,"import torch

batch_indexes = {

}
pose_indexes = {

}


def transition_features(x_old, x_new, transition_variable):
    assert x_old.shape == x_new.shape, ""Old shape: {}, New: {}"".format(
        x_old.shape, x_new.shape)
    return torch.lerp(x_old, x_new, transition_variable)


def get_transition_value(x_old, x_new, transition_variable):
    assert x_old.shape == x_new.shape, ""Old shape: {}, New: {}"".format(
        x_old.shape, x_new.shape)
    return torch.lerp(x_old, x_new, transition_variable)


def generate_pose_channel_images(
        min_imsize, max_imsize, device, pose_information, dtype):
    pose_information = pose_information.clone()
    batch_size = pose_information.shape[0]
    assert pose_information.shape[1] > 0, pose_information.shape
    num_poses = pose_information.shape[1] // 2
    pose_x = pose_information[:, range(
        0, pose_information.shape[1], 2)].view(-1)
    pose_y = pose_information[:, range(
        1, pose_information.shape[1], 2)].view(-1)
    assert batch_size <= 256, \
        f""Overflow error for batch size > 256. Was: {batch_size}""
    if (max_imsize, batch_size) not in batch_indexes.keys():
        batch_indexes[(max_imsize, batch_size)] = torch.cat(
            [torch.ones(num_poses, dtype=torch.long) * k for k in range(batch_size)])
        pose_indexes[(max_imsize, batch_size)] = torch.arange(
            0, num_poses).repeat(batch_size)
    batch_idx = batch_indexes[(max_imsize, batch_size)]
    pose_idx = pose_indexes[(max_imsize, batch_size)].clone()
    # All poses that are outside image, we move to the last pose channel
    illegal_mask = ((pose_x < 0) + (pose_x >= 1.0) +
                    (pose_y < 0) + (pose_y >= 1.0)) != 0
    pose_idx[illegal_mask] = num_poses
    pose_x[illegal_mask] = 0
    pose_y[illegal_mask] = 0
    pose_images = {}
    imsize = min_imsize
    while imsize <= max_imsize:
        new_im = torch.zeros((batch_size, num_poses + 1, imsize, imsize),
                             dtype=dtype, device=device)

        px = (pose_x * imsize).long()
        py = (pose_y * imsize).long()
        new_im[batch_idx, pose_idx, py, px] = 1
        new_im = new_im[:, :-1]  # Remove ""throwaway"" channel
        pose_images[imsize] = new_im
        imsize *= 2
    return pose_images


class NetworkWrapper(torch.nn.Module):

    def __init__(self, network):
        super().__init__()
        self.network = network
        if torch.cuda.is_available() and torch.cuda.device_count() > 1:
            self.forward_block = torch.nn.DataParallel(
                self.network
            )
        else:
            self.forward_block = self.network

    def forward(self, *inputs, **kwargs):
        return self.forward_block(*inputs, **kwargs)

    def extend(self):
        self.network.extend()

    def update_transition_value(self, value):
        self.network.transition_value = value

    def new_parameters(self):
        return self.network.new_parameters()

    def state_dict(self):
        return self.network.state_dict()

    def load_state_dict(self, ckpt):
        self.network.load_state_dict(ckpt)

    @property
    def current_imsize(self):
        return self.network.current_imsize

    def forward_fake(self, condition, mask, landmarks=None,
                     fake_img=None, with_pose=False, **kwargs):
        return self(
            fake_img, condition, mask, landmarks, with_pose=with_pose
        )

    def generate_latent_variable(self, *args, **kwargs):
        return self.network.generate_latent_variable(*args, **kwargs)

    def forward_train(self, *args, **kwargs):
        return [self(*args, **kwargs)]

    def update_beta(self, *args, **kwargs):
        return self.network.update_beta(*args, **kwargs)

    @property
    def ra_beta(self):
        return self.network.ra_beta

    def update_ra(self, *args, **kwargs):
        return self.network.update_ra(*args, **kwargs)

    @property
    def z_shape(self):
        return self.network.z_shape
"
DeepPrivacy,blocks.py,"import torch.nn as nn
import numpy as np
import torch
from typing import List
from . import layers
from . import iconv


def get_conv(ctype, post_act):
    type2conv = {
        ""conv"": layers.Conv2d,
        ""iconv"": iconv.IConv,
        ""gconv"": GatedConv
    }
    # Do not apply for output layer
    if not post_act and ctype in [""gconv"", ""iconv""]:
        return type2conv[""conv""]
    assert ctype in type2conv
    return type2conv[ctype]


def build_base_conv(
        conv2d_config, post_act: bool, *args, **kwargs) -> nn.Conv2d:
    for k, v in conv2d_config.conv.items():
        assert k not in kwargs
        kwargs[k] = v
    # Demodulation should not be used for output layers.
    demodulation = conv2d_config.normalization == ""demodulation"" and post_act
    kwargs[""demodulation""] = demodulation
    conv = get_conv(conv2d_config.conv.type, post_act)
    return conv(*args, **kwargs)


def build_post_activation(in_channels, conv2d_config) -> List[nn.Module]:
    _layers = []
    negative_slope = conv2d_config.leaky_relu_nslope
    _layers.append(layers.LeakyReLU(negative_slope, inplace=True))
    if conv2d_config.normalization == ""pixel_wise"":
        _layers.append(layers.PixelwiseNormalization())
    return _layers


def build_avgpool(conv2d_config, kernel_size) -> nn.AvgPool2d:
    if conv2d_config.conv.type == ""iconv"":
        return iconv.DownSample()
    return layers.AvgPool2d(kernel_size)


def build_convact(conv2d_config, *args, **kwargs):
    conv = build_base_conv(conv2d_config, True, *args, **kwargs)
    out_channels = conv.out_channels
    post_act = build_post_activation(out_channels, conv2d_config)
    return nn.Sequential(conv, *post_act)


class ConvAct(nn.Module):

    def __init__(self, conv2d_config, *args, **kwargs):
        super().__init__()
        self._conv2d_config = conv2d_config
        conv = build_base_conv(conv2d_config, True, *args, **kwargs)
        self.in_channels = conv.in_channels
        self.out_channels = conv.out_channels
        _layers = [conv]
        _layers.extend(build_post_activation(self.out_channels, conv2d_config))
        self.layers = nn.Sequential(*_layers)

    def forward(self, _inp):
        return self.layers(_inp)


class GatedConv(layers.Conv2d):

    def __init__(self, in_channels, out_channels, *args, **kwargs):
        out_channels *= 2
        super().__init__(in_channels, out_channels, *args, **kwargs)
        assert self.out_channels % 2 == 0
        self.lrelu = nn.LeakyReLU(0.2, inplace=True)
        self.sigmoid = nn.Sigmoid()

    def conv2d_forward(self, x, weight, bias=True):
        x_ = super().conv2d_forward(x, weight, bias)
        x = x_[:, :self.out_channels // 2]
        y = x_[:, self.out_channels // 2:]
        x = self.lrelu(x)
        y = y.sigmoid()
        assert x.shape == y.shape, f""{x.shape}, {y.shape}""
        return x * y


class BasicBlock(nn.Module):

    def __init__(
            self, conv2d_config, resolution: int, in_channels: int,
            out_channels: List[int], residual: bool):
        super().__init__()
        assert len(out_channels) == 2
        self._resolution = resolution
        self._residual = residual
        self.out_channels = out_channels
        _layers = []
        _in_channels = in_channels
        for out_ch in out_channels:
            conv = build_base_conv(
                conv2d_config, True, _in_channels, out_ch, kernel_size=3,
                resolution=resolution)
            _layers.append(conv)
            _layers.extend(build_post_activation(_in_channels, conv2d_config))
            _in_channels = out_ch
        self.layers = nn.Sequential(*_layers)
        if self._residual:
            self.residual_conv = build_base_conv(
                conv2d_config, post_act=False, in_channels=in_channels,
                out_channels=out_channels[-1],
                kernel_size=1, padding=0)
            self.const = 1 / np.sqrt(2)

    def forward(self, _inp):
        x, mask, batch = _inp
        y = x
        mask_ = mask
        assert y.shape[-1] == self._resolution or y.shape[-1] == 1
        y, mask = self.layers((x, mask))
        if self._residual:
            residual, mask_ = self.residual_conv((x, mask_))
            y = (y + residual) * self.const
            mask = (mask + mask_) * self.const
        return y, mask, batch

    def extra_repr(self):
        return f""Residual={self._residual}, Resolution={self._resolution}""


class PoseNormalize(nn.Module):

    @torch.no_grad()
    def forward(self, x):
        return x * 2 - 1


class ScalarPoseFCNN(nn.Module):

    def __init__(self, pose_size, hidden_size,
                 output_shape):
        super().__init__()
        pose_size = pose_size
        self._hidden_size = hidden_size
        output_size = np.prod(output_shape)
        self.output_shape = output_shape
        self.pose_preprocessor = nn.Sequential(
            PoseNormalize(),
            layers.Linear(pose_size, hidden_size),
            nn.LeakyReLU(.2),
            layers.Linear(hidden_size, output_size),
            nn.LeakyReLU(.2)
        )

    def forward(self, _inp):
        x, mask, batch = _inp
        pose_info = batch[""landmarks""]
        del batch[""landmarks""]
        pose = self.pose_preprocessor(pose_info)
        pose = pose.view(-1, *self.output_shape)
        if x.shape[0] == 1 and x.shape[2] == 1 and x.shape[3] == 1:
            # Analytical normalization propagation
            pose = pose.mean(dim=2, keepdim=True).mean(dim=3, keepdims=True)
        x = torch.cat((x, pose), dim=1)
        return x, mask, batch

    def __repr__(self):
        return "" "".join([
            self.__class__.__name__,
            f""hidden_size={self._hidden_size}"",
            f""output shape={self.output_shape}""
        ])


class Attention(nn.Module):

    def __init__(self, in_channels):
        super(Attention, self).__init__()
        # Channel multiplier
        self.in_channels = in_channels
        self.theta = layers.Conv2d(
            self.in_channels, self.in_channels // 8, kernel_size=1, padding=0,
            bias=False)
        self.phi = layers.Conv2d(
            self.in_channels, self.in_channels // 8, kernel_size=1, padding=0,
            bias=False)
        self.g = layers.Conv2d(
            self.in_channels, self.in_channels // 2, kernel_size=1, padding=0,
            bias=False)
        self.o = layers.Conv2d(
            self.in_channels // 2, self.in_channels, kernel_size=1, padding=0,
            bias=False)
        # Learnable gain parameter
        self.gamma = nn.Parameter(torch.tensor(0.), requires_grad=True)

    def forward(self, _inp):
        x, mask, batch = _inp
        # Apply convs
        theta, _ = self.theta((x, None))
        phi = nn.functional.max_pool2d(self.phi((x, None))[0], [2, 2])
        g = nn.functional.max_pool2d(self.g((x, None))[0], [2, 2])
        # Perform reshapes
        theta = theta.view(-1, self.in_channels // 8, x.shape[2] * x.shape[3])
        phi = phi.view(-1, self.in_channels // 8, x.shape[2] * x.shape[3] // 4)
        g = g.view(-1, self.in_channels // 2, x.shape[2] * x.shape[3] // 4)
        # Matmul and softmax to get attention maps
        beta = nn.functional.softmax(torch.bmm(theta.transpose(1, 2), phi), -1)
        # Attention map times g path

        o = self.o((torch.bmm(g, beta.transpose(1, 2)).view(-1,
                                                            self.in_channels // 2, x.shape[2], x.shape[3]), None))[0]
        return self.gamma * o + x, mask, batch
"
DeepPrivacy,layers.py,"import torch
import torch.nn as nn
import numpy as np


def get_padding(kernel_size: int, dilation: int, stride: int):
    out = (dilation * (kernel_size - 1) - 1) / 2 + 1
    return int(np.floor(out))


class Conv2d(nn.Conv2d):

    def __init__(self, in_channels, out_channels, kernel_size, stride=1,
                 padding=None, dilation=1, groups=1,
                 bias=True, padding_mode='zeros',
                 demodulation=False, wsconv=False, gain=1,
                 *args, **kwargs):
        if padding is None:
            padding = get_padding(kernel_size, dilation, stride)
        super().__init__(
            in_channels, out_channels, kernel_size, stride, padding, dilation,
            groups, bias, padding_mode)
        self.demodulation = demodulation
        self.wsconv = wsconv
        if self.wsconv:
            fan_in = np.prod(self.weight.shape[1:]) / self.groups
            self.ws_scale = gain / np.sqrt(fan_in)
            nn.init.normal_(self.weight)
        if bias:
            nn.init.constant_(self.bias, val=0)
        assert not self.padding_mode == ""circular"",\
            ""conv2d_forward does not support circular padding. Look at original pytorch code""

    def _get_weight(self):
        weight = self.weight
        if self.wsconv:
            weight = self.ws_scale * weight
        if self.demodulation:
            demod = torch.rsqrt(weight.pow(2).sum([1, 2, 3]) + 1e-7)
            weight = weight * demod.view(self.out_channels, 1, 1, 1)
        return weight

    def conv2d_forward(self, x, weight, bias=True):
        bias_ = None
        if bias:
            bias_ = self.bias
        return nn.functional.conv2d(x, weight, bias_, self.stride,
                                    self.padding, self.dilation, self.groups)

    def forward(self, _inp):
        x, mask = _inp
        weight = self._get_weight()
        return self.conv2d_forward(x, weight), mask

    def __repr__(self):
        return "", "".join([
            super().__repr__(),
            f""Demodulation={self.demodulation}"",
            f""Weight Scale={self.wsconv}"",
            f""Bias={self.bias is not None}""
        ])


class LeakyReLU(nn.LeakyReLU):

    def forward(self, _inp):
        x, mask = _inp
        return super().forward(x), mask


class AvgPool2d(nn.AvgPool2d):

    def forward(self, _inp):
        x, mask, *args = _inp
        x = super().forward(x)
        mask = super().forward(mask)
        if len(args) > 0:
            return (x, mask, *args)
        return x, mask


def up(x):
    if x.shape[0] == 1 and x.shape[2] == 1 and x.shape[3] == 1:
        # Analytical normalization
        return x
    return nn.functional.interpolate(
        x, scale_factor=2, mode=""nearest"")


class NearestUpsample(nn.Module):

    def forward(self, _inp):
        x, mask, *args = _inp
        x = up(x)
        mask = up(mask)
        if len(args) > 0:
            return (x, mask, *args)
        return x, mask


class PixelwiseNormalization(nn.Module):

    def forward(self, _inp):
        x, mask = _inp
        norm = torch.rsqrt((x**2).mean(dim=1, keepdim=True) + 1e-7)
        return x * norm, mask


class Linear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features)
        self.linear = nn.Linear(in_features, out_features)
        fanIn = in_features
        self.wtScale = 1 / np.sqrt(fanIn)

        nn.init.normal_(self.weight)
        nn.init.constant_(self.bias, val=0)

    def _get_weight(self):
        return self.weight * self.wtScale

    def forward_linear(self, x, weight):
        return nn.functional.linear(x, weight, self.bias)

    def forward(self, x):
        return self.forward_linear(x, self._get_weight())


class OneHotPoseConcat(nn.Module):

    def forward(self, _inp):
        x, mask, batch = _inp
        landmarks = batch[""landmarks_oh""]
        res = x.shape[-1]
        landmark = landmarks[res]
        x = torch.cat((x, landmark), dim=1)
        del batch[""landmarks_oh""][res]
        return x, mask, batch


def transition_features(x_old, x_new, transition_variable):
    assert x_old.shape == x_new.shape,\
        ""Old shape: {}, New: {}"".format(x_old.shape, x_new.shape)
    return torch.lerp(x_old.float(), x_new.float(), transition_variable)


class TransitionBlock(nn.Module):

    def forward(self, _inp):
        x, mask, batch = _inp
        x = transition_features(
            batch[""x_old""], x, batch[""transition_value""])
        mask = transition_features(
            batch[""mask_old""], mask, batch[""transition_value""])
        del batch[""x_old""]
        del batch[""mask_old""]
        return x, mask, batch
"
DeepPrivacy,base.py,"import torch
from torch import nn
from . import blocks, layers
from deep_privacy import torch_utils


def transition_features(x_old, x_new, transition_variable):
    assert x_old.shape == x_new.shape,\
        ""Old shape: {}, New: {}"".format(x_old.shape, x_new.shape)
    return torch.lerp(x_old, x_new, transition_variable)


class Module(nn.Module):

    def __init__(self, *args, **kwargs):
        super().__init__()

    def extra_repr(self):
        num_params = torch_utils.number_of_parameters(self) / 10**6

        return f""Number of parameters: {num_params:.3f}M""


class ProgressiveBase(Module):

    def __init__(self, cfg, *args, **kwargs):
        super().__init__()
        self.cfg = cfg
        self.transition_value = 1.0
        self.min_fmap_resolution = min(self.cfg.models.conv_size.keys())
        self.current_imsize = self.min_fmap_resolution
        self.transition_step = 0
        self.progressive_enabled = self.cfg.trainer.progressive.enabled
        self.conv_size = self.cfg.models.conv_size
        self.conv_size = {int(k): v for k, v in self.conv_size.items()}

    def extend(self):
        self.transition_step += 1
        self.current_imsize *= 2
        for child in self.children():
            if isinstance(child, ProgressiveBase):
                child.extend()

    def update_transition_value(self, value: float):
        self.transition_value = value

    def conv_channel_size(self):
        return self.conv_size[self.current_imsize]

    def prev_conv_channel_size(self):
        return self.conv_size[self.current_imsize // 2]

    def state_dict(self, *args, **kwargs):
        return {
            ""transition_step"": self.transition_step,
            ""transition_value"": self.transition_value,
            ""parameters"": super().state_dict(*args, **kwargs)
        }

    def load_state_dict(self, ckpt):
        for i in range(ckpt[""transition_step""] - self.transition_step):
            self.extend()
        self.transition_value = ckpt[""transition_value""]
        super().load_state_dict(ckpt[""parameters""])


class FromRGB(ProgressiveBase):

    def __init__(self, cfg, conv2d_config, in_channels, current_imsize=None):
        super().__init__(cfg)
        if current_imsize is not None:
            self.current_imsize = current_imsize
        self.progressive = cfg.trainer.progressive.enabled
        self._conv2d_config = conv2d_config
        self._in_channels = in_channels
        self.conv = blocks.ConvAct(
            conv2d_config, in_channels,
            self.conv_channel_size(), kernel_size=1, padding=0
        )
        self.old_conv = nn.Sequential()

    def extend(self):
        super().extend()
        if self.progressive:
            self.old_conv = nn.Sequential(
                layers.AvgPool2d(kernel_size=2),
                self.conv
            )
        self.conv = blocks.ConvAct(
            self._conv2d_config, self._in_channels,
            self.conv_channel_size(), 1, padding=0
        )

    def forward(self, _inp):
        x_, mask_, batch = _inp
        x, mask = self.conv((x_, mask_))
        if not self.progressive:
            return (x, mask, batch)
        x_old, mask_old = self.old_conv((x_, mask_))
        batch[""x_old""] = x_old
        batch[""mask_old""] = mask_old
        return x, mask, batch
"
DeepPrivacy,deep_privacy_v1.py,"import torch.nn as nn
import torch
from .. import layers, blocks
from ..utils import generate_pose_channel_images
from ..build import GENERATOR_REGISTRY
from .progressive_generator import DecoderUpsample
from .gblocks import LatentVariableConcat


class ConvAct(nn.Module):

    def __init__(self, conv2d_config, in_channels, out_channels, kernel_size):
        super().__init__()
        self.conv = nn.Sequential(
            blocks.build_base_conv(
                conv2d_config,
                True,
                in_channels,
                out_channels,
                kernel_size),
            *blocks.build_post_activation(
                out_channels,
                conv2d_config))

    def forward(self, _inp):
        x, mask, batch = _inp
        x, mask = self.conv((x, mask))
        return (x, mask, batch)


class UnetSkipConnection(nn.Module):

    def __init__(self, cfg, in_channels: int,
                 out_channels: int, resolution: int):
        super().__init__()
        self.cfg = cfg
        conv2d_config = self.cfg.models.generator.conv2d_config
        self._in_channels = in_channels
        self._out_channels = out_channels
        self._resolution = resolution

        self.conv = blocks.ConvAct(
            conv2d_config, in_channels, out_channels,
            kernel_size=1, padding=0)

    def forward(self, _inp):
        x, mask, batch = _inp
        skip_x, skip_mask = batch[""unet_features""][self._resolution]

        del batch[""unet_features""][self._resolution]
        landmarks = batch[""landmarks_oh""]
        res = x.shape[-1]
        landmark = landmarks[res]
        x = torch.cat((x, skip_x, landmark), dim=1)
        del batch[""landmarks_oh""][res]
        x, mask = self.conv((x, mask))
        return x, mask, batch


@GENERATOR_REGISTRY.register_module
class DeepPrivacyV1(nn.Module):

    def __init__(self, cfg, conv2d_config: dict, *args, **kwargs):
        super().__init__()
        self.cfg = cfg
        self.current_imsize = 128
        self.init_decoder(conv2d_config)
        self.init_encoder(conv2d_config)
        self.z_shape = cfg.models.generator.z_shape

    def init_encoder(self, conv2d_config):
        encoder = nn.ModuleList()
        from_rgb = ConvAct(conv2d_config, 3, 128, 1)
        encoder.add_module(""from_rgb"", from_rgb)
        imsize = 128
        block5 = blocks.BasicBlock(
            conv2d_config, imsize, 128, [
                256, 256], residual=False)
        block4 = blocks.BasicBlock(
            conv2d_config, imsize // 2, 256, [512, 512],
            residual=False)
        block3 = blocks.BasicBlock(
            conv2d_config, imsize // 4, 512, [512, 512],
            residual=False)
        block2 = blocks.BasicBlock(
            conv2d_config, imsize // 8, 512, [512, 512],
            residual=False)
        block1 = blocks.BasicBlock(
            conv2d_config, imsize // 16, 512, [512, 512],
            residual=False)
        block0 = blocks.BasicBlock(
            conv2d_config, imsize // 32, 512, [512, 512],
            residual=False)
        encoder.add_module(""basic_block5"", block5)
        encoder.add_module(""downsample5"", layers.AvgPool2d(2))
        encoder.add_module(""basic_block4"", block4)
        encoder.add_module(""downsample4"", layers.AvgPool2d(2))
        encoder.add_module(""basic_block3"", block3)
        encoder.add_module(""downsample3"", layers.AvgPool2d(2))
        encoder.add_module(""basic_block2"", block2)
        encoder.add_module(""downsample2"", layers.AvgPool2d(2))
        encoder.add_module(""basic_block1"", block1)
        encoder.add_module(""downsample1"", layers.AvgPool2d(2))
        encoder.add_module(""basic_block0"", block0)
        self.encoder = encoder

    def init_decoder(self, conv2d_config):
        decoder = nn.ModuleList()
        imsize = 128
        decoder.add_module(""pose_concat0"", layers.OneHotPoseConcat())
        decoder.add_module(
            ""latent_concate"",
            LatentVariableConcat(conv2d_config))
        conv = ConvAct(conv2d_config, 512 + 7 + 32, 512, 1)
        decoder.add_module(""conv1x1"", conv)

        block0 = blocks.BasicBlock(
            conv2d_config, imsize // 32, 512, [512, 512], False)
        block1 = blocks.BasicBlock(
            conv2d_config, imsize // 16, 512, [512, 512], False)
        block2 = blocks.BasicBlock(
            conv2d_config, imsize // 8, 512, [512, 512], False)
        block3 = blocks.BasicBlock(
            conv2d_config, imsize // 4, 512, [512, 512], False)
        block4 = blocks.BasicBlock(
            conv2d_config, imsize // 2, 512, [256, 256], False)
        block5 = blocks.BasicBlock(
            conv2d_config, imsize, 256, [
                128, 128], False)
        decoder.add_module(""basic_block0"", block0)
        decoder.add_module(""upsample0"", DecoderUpsample())
        decoder.add_module(
            ""skip_connection1"",
            UnetSkipConnection(
                self.cfg,
                512 * 2 + 7,
                512,
                imsize // 16))

        decoder.add_module(""basic_block1"", block1)
        decoder.add_module(""upsample1"", DecoderUpsample())
        decoder.add_module(
            ""skip_connection2"",
            UnetSkipConnection(
                self.cfg,
                512 * 2 + 7,
                512,
                imsize // 8))

        decoder.add_module(""basic_block2"", block2)
        decoder.add_module(""upsample2"", DecoderUpsample())
        decoder.add_module(
            ""skip_connection3"",
            UnetSkipConnection(
                self.cfg,
                512 * 2 + 7,
                512,
                imsize // 4))

        decoder.add_module(""basic_block3"", block3)
        decoder.add_module(""upsample3"", DecoderUpsample())
        decoder.add_module(
            ""skip_connection4"",
            UnetSkipConnection(
                self.cfg,
                512 * 2 + 7,
                512,
                imsize // 2))

        decoder.add_module(""basic_block4"", block4)
        decoder.add_module(""upsample4"", DecoderUpsample())
        decoder.add_module(
            ""skip_connection5"",
            UnetSkipConnection(
                self.cfg,
                256 * 2 + 7,
                256,
                imsize))

        decoder.add_module(""basic_block5"", block5)
        self.to_rgb = blocks.build_base_conv(conv2d_config, False, 128, 3, 1)

        self.decoder = decoder

    def generate_latent_variable(self, *args):
        if len(args) == 1:
            x_in = args[0]

            return torch.randn(x_in.shape[0], *self.z_shape,
                               device=x_in.device,
                               dtype=x_in.dtype)
        elif len(args) == 3:
            batch_size, device, dtype = args
            return torch.randn(batch_size, *self.z_shape,
                               device=device,
                               dtype=dtype)
        raise ValueError(
            f""Expected either x_in or (batch_size, device, dtype. Got: {args}"")

    def forward_encoder(self, x, mask, batch):
        unet_features = {}
        for module in self.encoder:
            x, mask, batch = module((x, mask, batch))
            if isinstance(module, blocks.BasicBlock):
                unet_features[module._resolution] = (x, mask)
        return x, mask, unet_features

    def forward_decoder(self, x, mask, batch):
        for module in self.decoder:
            x, mask, batch = module((x, mask, batch))
        return x, mask

    def forward(
            self,
            condition,
            mask, landmarks, z=None, **kwargs):
        if z is None:
            z = self.generate_latent_variable(condition)
        landmarks_oh = None
        landmarks_oh = generate_pose_channel_images(
            4, self.current_imsize, condition.device, landmarks,
            condition.dtype)
        batch = dict(
            landmarks=landmarks,
            landmarks_oh=landmarks_oh,
            z=z)
        x, mask, unet_features = self.forward_encoder(condition, mask, batch)
        batch = dict(
            landmarks=landmarks,
            landmarks_oh=landmarks_oh,
            z=z,
            unet_features=unet_features)
        x, mask = self.forward_decoder(x, mask, batch)
        x, mask = self.to_rgb((x, mask))
#        x = condition * orig_mask + (1 - orig_mask) * x
        return x
"
DeepPrivacy,gblocks.py,"import torch
import numpy as np
import torch.nn as nn
from .. import blocks


class LatentVariableConcat(nn.Module):

    def __init__(self, conv2d_config):
        super().__init__()

    def forward(self, _inp):
        x, mask, batch = _inp
        z = batch[""z""]
        x = torch.cat((x, z), dim=1)
        return (x, mask, batch)


class UnetSkipConnection(nn.Module):

    def __init__(self, conv2d_config: dict, in_channels: int,
                 out_channels: int, resolution: int,
                 residual: bool, enabled: bool):
        super().__init__()
        self.use_iconv = conv2d_config.conv.type == ""iconv""
        self._in_channels = in_channels
        self._out_channels = out_channels
        self._resolution = resolution
        self._enabled = enabled
        self._residual = residual
        if self.use_iconv:
            self.beta0 = torch.nn.Parameter(torch.tensor(1.))
            self.beta1 = torch.nn.Parameter(torch.tensor(1.))
        else:
            if self._residual:
                self.conv = blocks.build_base_conv(
                    conv2d_config, False, in_channels // 2,
                    out_channels, kernel_size=1, padding=0)
            else:
                self.conv = blocks.ConvAct(
                    conv2d_config, in_channels, out_channels,
                    kernel_size=1, padding=0)

    def forward(self, _inp):
        if not self._enabled:
            return _inp
        x, mask, batch = _inp
        skip_x, skip_mask = batch[""unet_features""][self._resolution]
        assert x.shape == skip_x.shape, (x.shape, skip_x.shape)
        del batch[""unet_features""][self._resolution]
        if self.use_iconv:
            denom = skip_mask * self.beta0.relu() + mask * self.beta1.relu() + 1e-8
            gamma = skip_mask * self.beta0.relu() / denom
            x = skip_x * gamma + (1 - gamma) * x
            mask = skip_mask * gamma + (1 - gamma) * mask
        else:
            if self._residual:
                skip_x, skip_mask = self.conv((skip_x, skip_mask))
                x = (x + skip_x) / np.sqrt(2)
                if self._probabilistic:
                    mask = (mask + skip_mask) / np.sqrt(2)
            else:
                x = torch.cat((x, skip_x), dim=1)
                x, mask = self.conv((x, mask))
        return x, mask, batch

    def __repr__(self):
        return "" "".join([
            self.__class__.__name__,
            f""In channels={self._in_channels}"",
            f""Out channels={self._out_channels}"",
            f""Residual: {self._residual}"",
            f""Enabled: {self._enabled}""
            f""IConv: {self.use_iconv}""
        ])
"
DeepPrivacy,progressive_generator.py,"import torch.nn as nn
import torch
from .. import layers, blocks
from ..build import GENERATOR_REGISTRY
from .base import RunningAverageGenerator
from .gblocks import LatentVariableConcat, UnetSkipConnection
from ..base import ProgressiveBase, FromRGB
from ..utils import generate_pose_channel_images


class DecoderUpsample(layers.NearestUpsample):

    def forward(self, _inp):
        x_old, mask_old, batch = _inp
        x, mask = super().forward((x_old, mask_old))
        batch[""x_old""] = x
        batch[""mask_old""] = mask
        return x, mask, batch


class ToRGB(ProgressiveBase):

    def __init__(self, cfg):
        super().__init__(cfg)
        self.current_imsize = cfg.models.generator.min_fmap_resolution
        self.min_fmap_resolution = self.current_imsize
        self.conv = blocks.build_base_conv(
            cfg.models.generator.conv2d_config,
            post_act=False,
            in_channels=self.conv_channel_size(),
            out_channels=cfg.models.image_channels,
            kernel_size=1, padding=0
        )
        self.old_conv = nn.Sequential()

    def extend(self):
        super().extend()
        self.old_conv = self.conv
        self.conv = blocks.build_base_conv(
            self.cfg.models.generator.conv2d_config,
            post_act=False,
            in_channels=self.conv_channel_size(),
            out_channels=self.cfg.models.image_channels,
            kernel_size=1, padding=0
        )

    def forward(self, _inp):
        x, mask, batch = _inp
        x, mask = self.conv((x, mask))
        if not self.progressive_enabled:
            return (x, mask, batch)
        x_old, mask_old = batch[""x_old""], batch[""mask_old""]
        x_old, mask_old = self.old_conv((x_old, mask_old))
        batch[""x_old""] = x_old
        batch[""mask_old""] = mask_old
        return x, mask, batch


@GENERATOR_REGISTRY.register_module
class Generator(RunningAverageGenerator, ProgressiveBase):

    def __init__(self, cfg, *args, **kwargs):
        super().__init__(cfg=cfg, *args, **kwargs)
        self.current_imsize = cfg.models.generator.min_fmap_resolution
        self.min_fmap_resolution = self.current_imsize
        # Attributes
        conv2d_config = self.cfg.models.generator.conv2d_config
        self.concat_input_mask = conv2d_config.conv.type in [""conv"", ""gconv""]

        self._one_hot_pose = (
            not self.cfg.models.generator.scalar_pose_input
            and self.cfg.models.pose_size > 0)
        self._init_decoder()
        self._init_encoder()

    def _init_encoder(self):
        self.encoder = nn.ModuleList([
        ])
        frgb = FromRGB(
            self.cfg, self.cfg.models.generator.conv2d_config,
            in_channels=self.cfg.models.image_channels + self.concat_input_mask * 2,
            current_imsize=self.current_imsize)
        self.encoder.add_module(""from_rgb"", frgb)
        self.encoder.add_module(
            ""basic_block0"", blocks.BasicBlock(
                self.cfg.models.generator.conv2d_config, self.current_imsize,
                self.conv_channel_size(),
                [self.conv_channel_size(), self.conv_channel_size()],
                residual=self.cfg.models.generator.residual)
        )

    def _init_decoder(self):
        self.decoder = nn.ModuleList([])
        self.decoder.add_module(
            ""latent_variable_concat"", LatentVariableConcat(self.conv2d_config)
        )
        if self.cfg.models.generator.scalar_pose_input:
            m = self.min_fmap_resolution
            pose_shape = (16, m, m)
            self.decoder.add_module(
                ""pose_fcnn"", blocks.ScalarPoseFCNN(
                    self.cfg.models.pose_size, 128, pose_shape))
        elif self.cfg.models.pose_size != 0:
            self.decoder.add_module(""pose_concat0"", layers.OneHotPoseConcat())
        self.decoder.add_module(
            ""basic_block0"", self.create_up_block(
                resolution=self.min_fmap_resolution))
        self.decoder.add_module(""to_rgb"", ToRGB(self.cfg))

    def extend_decoder(self):
        to_rgb = [_ for _ in self.decoder if isinstance(_, ToRGB)][0]
        to_rgb.extend()
        decoder = nn.ModuleList([])
        for name, module in self.decoder.named_children():
            if isinstance(
                    module, ToRGB) or isinstance(
                    module, layers.TransitionBlock):
                continue
            decoder.add_module(name, module)
        self.decoder = decoder
        i = self.transition_step
        self.decoder.add_module(f""upsample{i}"", DecoderUpsample())
        if self.cfg.models.generator.unet.enabled:
            self.decoder.add_module(
                f""skip_connection{i}"", UnetSkipConnection(
                    self.conv2d_config, self.prev_conv_channel_size() * 2,
                    self.prev_conv_channel_size(),
                    self.current_imsize,
                    **self.cfg.models.generator.unet))
        if self._one_hot_pose:
            self.decoder.add_module(
                f""pose_concat{i}"", layers.OneHotPoseConcat())
        self.decoder.add_module(
            f""basic_block{i}"", self.create_up_block(self.current_imsize))
        self.decoder.add_module(
            ""to_rgb"", to_rgb
        )
        if self.progressive_enabled:
            self.decoder.add_module(
                ""transition_block"", layers.TransitionBlock())

    def extend_encoder(self):
        from_rgb, *old_blocks = self.encoder
        from_rgb.extend()
        encoder = nn.ModuleList([])
        encoder.add_module(""from_rgb"", from_rgb)
        i = self.transition_step
        # New block
        encoder.add_module(
            f""basic_block{i}"", self.create_down_block(self.current_imsize))
        encoder.add_module(f""downsample{i}"", blocks.build_avgpool(
            self.cfg.models.generator.conv2d_config, kernel_size=2))
        if self.progressive_enabled:
            encoder.add_module(""transition_block"", layers.TransitionBlock())

        for name, module in self.encoder.named_children():
            if isinstance(name, layers.TransitionBlock):
                continue
            if isinstance(name, FromRGB):
                continue
            encoder.add_module(name, module)
        self.encoder = encoder

    def extend(self):
        super().extend()
        self.extend_encoder()
        self.extend_decoder()

    def create_down_block(self, resolution):
        return blocks.BasicBlock(
            self.cfg.models.generator.conv2d_config, self.current_imsize,
            self.conv_channel_size(),
            [self.conv_channel_size(), self.prev_conv_channel_size()],
            residual=self.cfg.models.generator.residual)

    def create_up_block(self, resolution):
        if self.current_imsize == self.min_fmap_resolution:
            start_size = self.conv_channel_size() + self.z_shape[0]
        else:
            start_size = self.prev_conv_channel_size()
        if self.cfg.models.generator.scalar_pose_input:
            if self.current_imsize == self.min_fmap_resolution:
                start_size += 16
        else:
            start_size += self.cfg.models.pose_size // 2
        return blocks.BasicBlock(
            self.conv2d_config, self.current_imsize,
            start_size, [start_size, self.conv_channel_size()],
            residual=self.cfg.models.generator.residual)

    def forward_encoder(self, x, mask, batch):
        if self.concat_input_mask:
            x = torch.cat((x, mask, 1 - mask), dim=1)
        unet_features = {}
        for module in self.encoder:
            x, mask, batch = module((x, mask, batch))
            if isinstance(module, blocks.BasicBlock):
                unet_features[module._resolution] = (x, mask)
        return x, mask, unet_features

    def forward_decoder(self, x, mask, batch):
        for module in self.decoder:
            x, mask, batch = module((x, mask, batch))
        return x, mask

    def forward(
            self,
            condition,
            mask, landmarks=None, z=None,
            **kwargs):
        if z is None:
            z = self.generate_latent_variable(condition)
        landmarks_oh = None
        if self._one_hot_pose:
            landmarks_oh = generate_pose_channel_images(
                4, self.current_imsize, condition.device, landmarks,
                condition.dtype)
        batch = dict(
            landmarks=landmarks,
            landmarks_oh=landmarks_oh,
            z=z,
            transition_value=self.transition_value)
        orig_mask = mask
        mask = self._get_input_mask(condition, mask)

        x, mask, unet_features = self.forward_encoder(condition, mask, batch)

        batch = dict(
            landmarks=landmarks,
            landmarks_oh=landmarks_oh,
            z=z,
            unet_features=unet_features,
            transition_value=self.transition_value)
        x, mask = self.forward_decoder(x, mask, batch)

        if self.cfg.models.generator.use_skip:
            x = condition * orig_mask + (1 - orig_mask) * x
        return x


if __name__ == ""__main__"":
    from deep_privacy.config import Config, default_parser
    args = default_parser().parse_args()
    cfg = Config.fromfile(args.config_path)

    g = Generator(cfg).cuda()
    g.extend()
    g.cuda()
    imsize = g.current_imsize
    batch = dict(
        mask=torch.ones((8, 1, imsize, imsize)).cuda(),
        condition=torch.randn((8, 3, imsize, imsize)).cuda(),
        landmarks=torch.randn((8, 14)).cuda()
    )
"
DeepPrivacy,msg_generator.py,"import torch
import torch.nn as nn
from .. import layers, blocks
from ..build import GENERATOR_REGISTRY
from .base import RunningAverageGenerator
from .gblocks import LatentVariableConcat, UnetSkipConnection


@GENERATOR_REGISTRY.register_module
class MSGGenerator(RunningAverageGenerator):

    def __init__(
            self, max_imsize: int, conv_size: dict,
            image_channels: int,
            min_fmap_resolution: int,
            residual: bool,
            pose_size: int,
            unet: dict,
            *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.removable_hooks = []
        self.rgb_convolutions = nn.ModuleDict()
        self.max_imsize = max_imsize
        self._image_channels = image_channels
        self._min_fmap_resolution = min_fmap_resolution
        self._residual = residual
        self._pose_size = pose_size
        self.current_imsize = max_imsize
        self._unet_cfg = unet
        self.concat_input_mask = self.conv2d_config.conv.type in [""conv"", ""gconv""]
        self.res2channels = {int(k): v for k, v in conv_size.items()}
        self._init_decoder()
        self._init_encoder()

    def _init_encoder(self):
        self.encoder = nn.ModuleList()
        imsize = self.max_imsize
        self.from_rgb = blocks.build_convact(
            self.conv2d_config,
            in_channels=self._image_channels + self.concat_input_mask*2,
            out_channels=self.res2channels[imsize],
            kernel_size=1)
        while imsize >= self._min_fmap_resolution:
            current_size = self.res2channels[imsize]
            next_size = self.res2channels[max(imsize//2, self._min_fmap_resolution)]
            block = blocks.BasicBlock(
                self.conv2d_config, imsize, current_size,
                [current_size, next_size], self._residual)
            self.encoder.add_module(f""basic_block{imsize}"", block)
            if imsize != self._min_fmap_resolution:
                self.encoder.add_module(
                    f""downsample{imsize}"", layers.AvgPool2d(2))
            imsize //= 2

    def _init_decoder(self):
        self.decoder = nn.ModuleList()
        self.decoder.add_module(
            ""latent_concat"", LatentVariableConcat(self.conv2d_config))
        if self._pose_size > 0:
            m = self._min_fmap_resolution
            pose_shape = (16, m, m)
            pose_fcnn = blocks.ScalarPoseFCNN(self._pose_size, 128, pose_shape)
            self.decoder.add_module(""pose_fcnn"", pose_fcnn)
        imsize = self._min_fmap_resolution
        self.rgb_convolutions = nn.ModuleDict()
        while imsize <= self.max_imsize:
            current_size = self.res2channels[max(imsize//2, self._min_fmap_resolution)]
            start_size = current_size
            if imsize == self._min_fmap_resolution:
                start_size += self.z_shape[0]
                if self._pose_size > 0:
                    start_size += 16
            else:
                self.decoder.add_module(f""upsample{imsize}"", layers.NearestUpsample())
                skip = UnetSkipConnection(
                    self.conv2d_config, current_size*2, current_size, imsize,
                    **self._unet_cfg)
                self.decoder.add_module(f""skip_connection{imsize}"", skip)
            next_size = self.res2channels[imsize]
            block = blocks.BasicBlock(
                self.conv2d_config, imsize, start_size, [start_size, next_size],
                residual=self._residual)
            self.decoder.add_module(f""basic_block{imsize}"", block)

            to_rgb = blocks.build_base_conv(
                self.conv2d_config, False, in_channels=next_size,
                out_channels=self._image_channels, kernel_size=1)
            self.rgb_convolutions[str(imsize)] = to_rgb
            imsize *= 2
        self.norm_constant = len(self.rgb_convolutions)

    def forward_decoder(self, x, mask, batch):
        imsize_start = max(x.shape[-1] // 2, 1)
        rgb = torch.zeros(
            (x.shape[0], self._image_channels,
             imsize_start, imsize_start),
            dtype=x.dtype, device=x.device)
        mask_size = 1
        mask_out = torch.zeros(
            (x.shape[0], mask_size,
             imsize_start, imsize_start),
            dtype=x.dtype, device=x.device)
        imsize = self._min_fmap_resolution // 2
        for module in self.decoder:
            x, mask, batch = module((x, mask, batch))
            if isinstance(module, blocks.BasicBlock):
                imsize *= 2
                rgb = layers.up(rgb)
                mask_out = layers.up(mask_out)
                conv = self.rgb_convolutions[str(imsize)]
                rgb_, mask_ = conv((x, mask))
                assert rgb_.shape == rgb.shape,\
                    f""rgb_ {rgb_.shape}, rgb: {rgb.shape}""
                rgb = rgb + rgb_
        return rgb / self.norm_constant, mask_out

    def forward_encoder(self, x, mask, batch):
        if self.concat_input_mask:
            x = torch.cat((x, mask, 1 - mask), dim=1)
        unet_features = {}
        x, mask = self.from_rgb((x, mask))
        for module in self.encoder:
            x, mask, batch = module((x, mask, batch))
            if isinstance(module, blocks.BasicBlock):
                unet_features[module._resolution] = (x, mask)
        return x, mask, unet_features

    def forward(
            self,
            condition,
            mask, landmarks=None, z=None,
            **kwargs):
        if z is None:
            z = self.generate_latent_variable(condition)
        batch = dict(
            landmarks=landmarks,
            z=z)
        orig_mask = mask
        mask = self._get_input_mask(condition, mask)
        x, mask, unet_features = self.forward_encoder(condition, mask, batch)
        batch = dict(
            landmarks=landmarks,
            z=z,
            unet_features=unet_features)
        x, mask = self.forward_decoder(x, mask, batch)
        x = condition * orig_mask + (1 - orig_mask) * x
        return x

    def load_state_dict(self, state_dict, strict=True):
        if ""parameters"" in state_dict:
            state_dict = state_dict[""parameters""]
        old_checkpoint = any(""basic_block0"" in key for key in state_dict)
        if not old_checkpoint:
            return super().load_state_dict(state_dict, strict=strict)
        mapping = {}
        imsize = self._min_fmap_resolution
        i = 0
        while imsize <= self.max_imsize:
            old_key = f""decoder.basic_block{i}.""
            new_key = f""decoder.basic_block{imsize}.""
            mapping[old_key] = new_key
            if i >= 1:
                old_key = old_key.replace(""basic_block"", ""skip_connection"")
                new_key = new_key.replace(""basic_block"", ""skip_connection"")
                mapping[old_key] = new_key
            mapping[old_key] = new_key
            old_key = f""encoder.basic_block{i}.""
            new_key = f""encoder.basic_block{imsize}.""
            mapping[old_key] = new_key
            old_key = ""from_rgb.conv.layers.0.""
            new_key = ""from_rgb.0.""
            mapping[old_key] = new_key
            i += 1
            imsize *= 2
        new_sd = {}
        for key, value in state_dict.items():
            old_key = key
            if ""from_rgb"" in key:
                new_sd[key.replace(""encoder."", """").replace("".conv.layers"", """")] = value
                continue
            for subkey, new_subkey in mapping.items():
                if subkey in key:
                    old_key = key
                    key = key.replace(subkey, new_subkey)

                    break
            if ""decoder.to_rgb"" in key:
                continue

            new_sd[key] = value
        return super().load_state_dict(new_sd, strict=strict)
        

if __name__ == ""__main__"":
    from deep_privacy.config import Config, default_parser
    args = default_parser().parse_args()
    cfg = Config.fromfile(args.config_path)

    g = MSGGenerator(cfg).cuda()
    g.extend()
    g.cuda()
    imsize = g.current_imsize
    batch = dict(
        mask=torch.ones((8, 1, imsize, imsize)).cuda(),
        condition=torch.randn((8, 3, imsize, imsize)).cuda(),
        landmarks=torch.randn((8, 14)).cuda()
    )
    print(g(**batch).shape)
"
DeepPrivacy,__init__.py,"from .progressive_generator import Generator
from .msg_generator import MSGGenerator
from .deep_privacy_v1 import DeepPrivacyV1
"
DeepPrivacy,base.py,"import torch
from ..base import Module


class RunningAverageGenerator(Module):

    def __init__(self, z_shape, conv2d_config: dict, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.z_shape = z_shape
        self.conv2d_config = conv2d_config

    def update_beta(self, batch_size: int):
        self.ra_beta = 0.5 ** (batch_size / (10 * 1000))

    @torch.no_grad()
    def update_ra(self, normal_generator):
        """"""
            Update running average generator
        """"""
        for avg_param, cur_param in zip(self.parameters(),
                                        normal_generator.parameters()):
            assert avg_param.shape == cur_param.shape
            avg_param.data = self.ra_beta * avg_param + \
                (1 - self.ra_beta) * cur_param

    def forward_train(self, *args, **kwargs):
        return [self(*args, **kwargs)]

    def generate_latent_variable(self, *args):
        if len(args) == 1:
            x_in = args[0]

            return torch.randn(x_in.shape[0], *self.z_shape,
                               device=x_in.device,
                               dtype=x_in.dtype)
        elif len(args) == 3:
            batch_size, device, dtype = args
            return torch.randn(batch_size, *self.z_shape,
                               device=device,
                               dtype=dtype)
        raise ValueError(
            f""Expected either x_in or (batch_size, device, dtype. Got: {args}"")

    def _get_input_mask(self, condition, mask):
        return mask
"
DeepPrivacy,__init__.py,"from .original import Discriminator
"
DeepPrivacy,original.py,"import torch
import numpy as np
import torch.nn as nn
from ..base import ProgressiveBase, FromRGB
from .. import layers, blocks
from ..utils import generate_pose_channel_images
from ..build import DISCRIMINATOR_REGISTRY


def get_conv_size(cfg, size):
    size = size * (2**0.5)
    size = size * cfg.models.discriminator.conv_multiplier
    if cfg.models.generator.conv2d_config.conv.type == ""gconv"":
        size *= (2**0.5)
    return int(np.ceil(size / 8) * 8)


class FromRGB(FromRGB):

    def conv_channel_size(self):
        size = super().conv_channel_size()
        return get_conv_size(self.cfg, size)

    def prev_conv_channel_size(self):
        size = super().conv_channel_size()
        return get_conv_size(self.cfg, size)


@DISCRIMINATOR_REGISTRY.register_module
class Discriminator(ProgressiveBase):

    def __init__(self, cfg, *args, **kwargs):
        super().__init__(cfg)
        self.min_fmap_resolution = cfg.models.discriminator.min_fmap_resolution
        self.current_imsize = self.min_fmap_resolution
        self._use_pose = self.cfg.models.pose_size > 0
        self._one_hot_pose = (
            self._use_pose and
            not self.cfg.models.discriminator.scalar_pose_input)
        self.layers = nn.Sequential()
        self.layers.add_module(
            ""from_rgb"", FromRGB(
                cfg, cfg.models.discriminator.conv2d_config,
                in_channels=cfg.models.image_channels * 2 + 1,
                current_imsize=self.min_fmap_resolution))
        if self._one_hot_pose:
            self.layers.add_module(
                ""pose_concat0"", layers.OneHotPoseConcat())

        # Last convolution has kernel size (4, 4)
        first_block = self.build_block()
        layers_ = list(first_block.layers)
        last_conv_idx = [
            i for i, x in enumerate(layers_)
            if isinstance(x, layers.Conv2d)][-1]
        layers_[last_conv_idx] = blocks.build_base_conv(
            cfg.models.discriminator.conv2d_config,
            True,
            first_block.out_channels[-2],
            first_block.out_channels[-1],
            kernel_size=(4, 4), padding=0)
        first_block.layers = nn.Sequential(*layers_)
        self.layers.add_module(""basic_block0"", first_block)
        num_outputs = 1
        res = self.min_fmap_resolution - 3
        self.output_layer = layers.Linear(
            self.conv_channel_size() * res**2, num_outputs)

    def conv_channel_size(self):
        size = super().conv_channel_size()
        return get_conv_size(self.cfg, size)

    def prev_conv_channel_size(self):
        size = super().prev_conv_channel_size()
        return get_conv_size(self.cfg, size)

    def build_block(self):
        end_size = self.conv_channel_size()
        if self.current_imsize != 4:
            end_size = self.prev_conv_channel_size()
        start_size = self.conv_channel_size()
        if self._use_pose:
            pose_imsize = self.cfg.models.discriminator.scalar_pose_input_imsize
            if self._one_hot_pose:
                start_size += self.cfg.models.pose_size // 2
            elif pose_imsize == self.current_imsize:
                start_size += 1
        # No residual layer for last block.
        residual = self.cfg.models.discriminator.residual and self.current_imsize != self.min_fmap_resolution
        return blocks.BasicBlock(
            self.cfg.models.discriminator.conv2d_config,
            self.current_imsize, start_size, [start_size, end_size],
            residual=residual
        )

    def extend(self):
        super().extend()
        from_rgb, *_layers = list(self.layers.children())
        from_rgb.extend()
        _layers = nn.Sequential()
        _layers.add_module(""from_rgb"", from_rgb)
        i = self.transition_step
        pose_imsize = self.cfg.models.discriminator.scalar_pose_input_imsize
        if self._one_hot_pose:
            _layers.add_module(f""pose_concat{i}"", layers.OneHotPoseConcat())
        elif self._use_pose and pose_imsize == self.current_imsize:
            output_shape = (1, pose_imsize, pose_imsize)
            pose_fcnn = blocks.ScalarPoseFCNN(
                self.cfg.models.pose_size, 64, output_shape)
            _layers.add_module(""pose_fcnn"", pose_fcnn)

        _layers.add_module(f""basic_block{i}"", self.build_block())
        _layers.add_module(
            f""downsample{i}"", layers.AvgPool2d(kernel_size=2))
        if self.progressive_enabled:
            _layers.add_module(""transition_block"", layers.TransitionBlock())
        for name, module in self.layers.named_children():
            if isinstance(module, FromRGB):
                continue
            if isinstance(module, layers.TransitionBlock):
                continue
            _layers.add_module(name, module)
        self.layers = _layers

    def forward_fake(self, condition, mask, landmarks=None,
                     fake_img=None, with_pose=False, **kwargs):
        return self(
            fake_img, condition, mask, landmarks, with_pose=with_pose
        )

    def forward(
            self, img, condition, mask, landmarks=None,
            with_pose=False, **kwargs):
        landmarks_oh = None
        if self._one_hot_pose:
            landmarks_oh = generate_pose_channel_images(
                4, self.current_imsize, condition.device, landmarks,
                condition.dtype)
        batch = dict(
            landmarks_oh=landmarks_oh,
            landmarks=landmarks,
            transition_value=self.transition_value)
        x = torch.cat((img, condition, mask), dim=1)
        x, mask, batch = self.layers((x, mask, batch))
        x = x.view(x.shape[0], -1)
        x = self.output_layer(x)
        return [x]


if __name__ == ""__main__"":
    from deep_privacy.config import Config, default_parser
    args = default_parser().parse_args()
    cfg = Config.fromfile(args.config_path)

    g = Discriminator(cfg).cuda()
    [g.extend() for i in range(3)]
    g.cuda()
    print(g)
    imsize = g.current_imsize
    batch = dict(
        img=torch.randn((8, 3, imsize, imsize)).cuda(),
        mask=torch.ones((8, 1, imsize, imsize)).cuda(),
        condition=torch.randn((8, 3, imsize, imsize)).cuda(),
        landmarks=torch.randn((8, 14)).cuda()
    )
    print(g(**batch).shape)
"
DeepPrivacy,anonymizer.py,"import moviepy.editor as mp
import cv2
import numpy as np
import pathlib
import typing
from deep_privacy.visualization import utils as vis_utils
from deep_privacy.detection import build_detector, ImageAnnotation


class Anonymizer:

    def __init__(self,
                 cfg,
                 detector_cfg,
                 *args, **kwargs):
        super().__init__()
        self.cfg = cfg
        self.detector = build_detector(
            detector_cfg, generator_imsize=cfg.models.max_imsize)

    def anonymize(self, im):
        return self.detect_and_anonymize_images([im])[0]

    def get_detections(
            self,
            images: typing.List[np.ndarray],
            im_bboxes: typing.List[np.ndarray] = None,
    ) -> typing.List[ImageAnnotation]:
        image_annotations = self.detector.get_detections(
            images, im_bboxes=im_bboxes,
        )

        return image_annotations

    def detect_and_anonymize_images(
            self,
            images: typing.List[np.ndarray],
            im_bboxes: typing.List[np.ndarray] = None,
            return_annotations: bool = False):
        image_annotations = self.get_detections(
            images, im_bboxes)

        anonymized_images = self.anonymize_images(
            images,
            image_annotations
        )
        if return_annotations:
            return anonymized_images, image_annotations
        return anonymized_images

    def anonymize_images(self,
                         images: np.ndarray,
                         image_annotations: typing.List[ImageAnnotation]):
        raise NotImplementedError

    def anonymize_image_paths(self,
                              image_paths: typing.List[pathlib.Path],
                              save_paths: typing.List[pathlib.Path],
                              im_bboxes=None):
        images = [cv2.imread(str(p))[:, :, ::-1] for p in image_paths]
        anonymized_images, image_annotations = self.detect_and_anonymize_images(
            images, im_bboxes, return_annotations=True)

        for image_idx, (new_path, anon_im) in enumerate(
                zip(save_paths, anonymized_images)):
            new_path.parent.mkdir(exist_ok=True, parents=True)
            annotation = image_annotations[image_idx]
            annotated_im = images[image_idx]
            for face_idx in range(len(annotation)):
                annotated_im = annotated_im * annotation.get_mask(face_idx)[:, :, None]
            annotated_im = vis_utils.draw_faces_with_keypoints(
                annotated_im,
                annotation.bbox_XYXY,
                annotation.keypoints
            )
            cv2.imwrite(str(new_path), anon_im[:, :, ::-1])
            print(""Saving to:"", new_path)

            to_save = np.concatenate((annotated_im, anon_im), axis=1)
            new_name = new_path.stem + ""_detected_left_anonymized_right.jpg""
            debug_impath = new_path.parent.joinpath(new_name)
            cv2.imwrite(str(debug_impath), to_save[:, :, ::-1])

    def anonymize_video(self, video_path: pathlib.Path,
                        target_path: pathlib.Path,
                        start_time=None,
                        end_time=None):
        # Read original video
        original_video = mp.VideoFileClip(str(video_path))
        fps = original_video.fps
        total_frames = int(original_video.duration * original_video.fps)
        start_time = 0 if start_time is None else start_time
        end_time = original_video.duration if end_time is None else end_time
        assert start_time <= end_time, f""Start frame{start_time} has to be smaller than end frame {end_time}""
        assert end_time <= original_video.duration,\
            f""End frame ({end_time}) is larger than number of frames {original_video.duration}""
        print(*[
            ""="" * 80,
            ""Anonymizing video."",
            f""Duration: {original_video.duration}. Total frames: {total_frames}, FPS: {fps}"",
            f""Anonymizing from: {start_time}({start_time*fps})s, to: {end_time}({end_time*fps})s""
        ], sep=""\n"")
        self.frame_idx = 0

        start = original_video.subclip(0, start_time)
        end = original_video.subclip(end_time)
        anonymized_video = original_video.subclip(start_time, end_time)
        anonymized_video = anonymized_video.fl_image(self.anonymize)
        anonymized_video = mp.concatenate_videoclips([start, anonymized_video, end])


        anonymized_video.write_videofile(
            str(target_path),
            fps=original_video.fps,
            audio_codec='aac')
"
DeepPrivacy,deep_privacy_anonymizer.py,"import numpy as np
import torch
import deep_privacy.torch_utils as torch_utils
import cv2
import pathlib
import typing
from deep_privacy.detection.detection_api import ImageAnnotation
from .anonymizer import Anonymizer
from . import infer


def batched_iterator(batch, batch_size):
    k = list(batch.keys())[0]
    num_samples = len(batch[k])
    num_batches = int(np.ceil(num_samples / batch_size))
    for idx in range(num_batches):
        start = batch_size * idx
        end = start + batch_size
        yield {
            key: torch_utils.to_cuda(arr[start:end])
            for key, arr in batch.items()
        }


class DeepPrivacyAnonymizer(Anonymizer):

    def __init__(self, generator, batch_size, save_debug,
                 fp16_inference: bool,
                 truncation_level=5, **kwargs):
        super().__init__(**kwargs)
        self.inference_imsize = self.cfg.models.max_imsize
        self.batch_size = batch_size
        self.pose_size = self.cfg.models.pose_size
        self.generator = generator
        self.truncation_level = truncation_level
        self.save_debug = save_debug
        self.fp16_inference = fp16_inference
        self.debug_directory = pathlib.Path("".debug"", ""inference"")
        self.debug_directory.mkdir(exist_ok=True, parents=True)

    @torch.no_grad()
    def _get_face(self, batch):
        keys = [""condition"", ""mask"", ""landmarks"", ""z""]
        forward = [batch[k] for k in keys]
#        print([x.shape for x in forward])
        with torch.cuda.amp.autocast(enabled=self.fp16_inference):
            return self.generator(*forward).cpu()

    @torch.no_grad()
    def anonymize_images(self,
                         images: np.ndarray,
                         image_annotations: typing.List[ImageAnnotation]
                         ) -> typing.List[np.ndarray]:
        anonymized_images = []
        for im_idx, image_annotation in enumerate(image_annotations):
            # pre-process
            imsize = self.inference_imsize
            condition = torch.zeros(
                (len(image_annotation), 3, imsize, imsize),
                dtype=torch.float32)
            mask = torch.zeros((len(image_annotation), 1, imsize, imsize))
            landmarks = torch.empty(
                (len(image_annotation), self.pose_size), dtype=torch.float32)
            for face_idx in range(len(image_annotation)):
                face, mask_ = image_annotation.get_face(face_idx, imsize)
                condition[face_idx] = torch_utils.image_to_torch(
                    face, cuda=False, normalize_img=True
                )
                mask[face_idx, 0] = torch.from_numpy(mask_).float()
                kp = image_annotation.aligned_keypoint(face_idx)
                landmarks[face_idx] = kp[:, :self.pose_size]
            img = condition
            condition = condition * mask
            z = infer.truncated_z(
                condition, self.cfg.models.generator.z_shape,
                self.truncation_level)
            batches = dict(
                condition=condition,
                mask=mask,
                landmarks=landmarks,
                z=z,
                img=img
            )
            # Inference
            anonymized_faces = np.zeros((
                len(image_annotation), imsize, imsize, 3), dtype=np.float32)
            for idx, batch in enumerate(
                    batched_iterator(batches, self.batch_size)):
                face = self._get_face(batch)
                face = torch_utils.image_to_numpy(
                    face, to_uint8=False, denormalize=True)
                start = idx * self.batch_size
                anonymized_faces[start:start + self.batch_size] = face
            anonymized_image = image_annotation.stitch_faces(anonymized_faces)
            anonymized_images.append(anonymized_image)
            if self.save_debug:
                num_faces = len(batches[""condition""])
                for face_idx in range(num_faces):
                    orig_face = torch_utils.image_to_numpy(
                        batches[""img""][face_idx], denormalize=True, to_uint8=True)
                    condition = torch_utils.image_to_numpy(
                        batches[""condition""][face_idx],
                        denormalize=True, to_uint8=True)
                    fake_face = anonymized_faces[face_idx]
                    fake_face = (fake_face * 255).astype(np.uint8)
                    to_save = np.concatenate(
                        (orig_face, condition, fake_face), axis=1)
                    filepath = self.debug_directory.joinpath(
                        f""im{im_idx}_face{face_idx}.png"")
                    cv2.imwrite(str(filepath), to_save[:, :, ::-1])

        return anonymized_images

    def use_mask(self):
        return self.generator.use_mask
"
DeepPrivacy,__init__.py,
DeepPrivacy,inpaint_inference.py,"import numpy as np
import torch
import tqdm
from deep_privacy import torch_utils
from .infer import truncated_z


def inpaint_images(
        images: np.ndarray, masks: np.ndarray,
        generator):
    z = None
    fakes = torch.zeros(
        (images.shape[0], images.shape[-1], images.shape[1], images.shape[2]),
        dtype=torch.float32)
    masks = pre_process_masks(masks)
    inputs = [im * mask for im, mask in zip(images, masks)]
    images = [
        torch_utils.image_to_torch(im, cuda=False, normalize_img=True)
        for im in images]
    masks = [torch_utils.mask_to_torch(mask, cuda=False) for mask in masks]
    with torch.no_grad():
        for idx, (im, mask) in enumerate(
                tqdm.tqdm(zip(images, masks), total=len(images))):
            im = torch_utils.to_cuda(im)
            mask = torch_utils.to_cuda(mask)
            assert im.shape[0] == mask.shape[0]
            assert im.shape[2:] == mask.shape[2:],\
                f""im shape: {im.shape}, mask shape: {mask.shape}""
            z = truncated_z(im, generator.z_shape, 0)
            condition = mask * im
            fake = generator(condition, mask, z)
            fakes[idx:(idx + 1)] = fake.cpu()
    fakes = torch_utils.image_to_numpy(fakes, denormalize=True) * 255
    return fakes, inputs


def pre_process_masks(masks: np.ndarray):
    if masks.shape[-1] == 4:
        masks = masks[:, :, :3]
    if masks.shape[-1] == 3:
        masks = masks.mean(axis=-1, keepdims=True)
    masks = (masks > 0).astype(np.float32)
    masks = np.ascontiguousarray(masks)
    return masks
"
DeepPrivacy,infer.py,"import argparse
import torch
import numpy as np
from deep_privacy.engine.checkpointer import get_checkpoint, load_checkpoint_from_url
from typing import List
from deep_privacy import config, logger
import deep_privacy.torch_utils as torch_utils
from deep_privacy import modeling


def init_generator(cfg, ckpt=None):
    g = modeling.models.build_generator(cfg, data_parallel=False)
    if ckpt is not None:
        g.load_state_dict(ckpt[""running_average_generator""])
    g.eval()
    torch_utils.to_cuda(g)
    return g


def infer_parser() -> argparse.ArgumentParser:
    parser = config.default_parser()
    parser.add_argument(
        ""-s"", ""--source_path"",
        help=""Target to infer"",
        default=""test_examples/images""
    )
    parser.add_argument(
        ""-t"", ""--target_path"",
        help=""Target path to save anonymized result.\
                Defaults to subdirectory of config file.""
    )
    parser.add_argument(
        ""--step"", default=None, type=int,
        help=""Set validation checkpoint to load. Defaults to most recent""
    )
    return parser


def load_model_from_checkpoint(
        cfg,
        validation_checkpoint_step: int = None,
        include_discriminator=False):
    try:
        ckpt = get_checkpoint(cfg.output_dir, validation_checkpoint_step)
    except FileNotFoundError:
        cfg.model_url = f'{cfg.model_url}'.replace(""http://"", ""https://"", 1)
        ckpt = None
        ckpt = load_checkpoint_from_url(cfg.model_url)
    if ckpt is None:
        logger.warn(f""Could not find checkpoint. {cfg.output_dir}"")
    generator = init_generator(cfg, ckpt)
    generator = jit_wrap(generator, cfg)
    if include_discriminator:
        discriminator = modeling.models.build_discriminator(
            cfg, data_parallel=False)
        discriminator.load_state_dict(ckpt[""D""])
        discriminator = torch_utils.to_cuda(discriminator)
        return generator, discriminator
    return generator


def jit_wrap(generator, cfg):
    """"""
        Torch JIT wrapper for accelerated inference
    """"""
    if not cfg.anonymizer.jit_trace:
        return generator
    imsize = cfg.models.max_imsize
    x_in = torch.randn((1, 3, imsize, imsize))
    example_inp = dict(
        condition=x_in,
        mask=torch.randn((1, 1, imsize, imsize)).bool().float(),
        landmarks=torch.randn((1, cfg.models.pose_size)),
        z=truncated_z(x_in, cfg.models.generator.z_shape, 5)
    )
    example_inp = {k: torch_utils.to_cuda(v) for k, v in example_inp.items()}
    generator = torch.jit.trace(
        generator,
        (example_inp[""condition""],
         example_inp[""mask""],
         example_inp[""landmarks""],
         example_inp[""z""]),
        optimize=True)
    return generator


def truncated_z(x_in: torch.Tensor, z_shape, truncation_level: float):
    z_shape = ((x_in.shape[0], *z_shape))
    if truncation_level == 0:
        return torch.zeros(z_shape, dtype=x_in.dtype, device=x_in.device)

    z = torch.randn(z_shape, device=x_in.device, dtype=x_in.dtype)
    while z.abs().max() >= truncation_level:
        mask = z.abs() >= truncation_level
        z_ = torch.randn(z_shape, device=x_in.device, dtype=x_in.dtype)
        z[mask] = z_[mask]
    return z


def infer_images(
        dataloader, generator, truncation_level: float,
        verbose=False,
        return_condition=False) -> List[np.ndarray]:
    imshape = (generator.current_imsize, generator.current_imsize, 3)
    real_images = np.empty(
        (dataloader.num_images(), *imshape), dtype=np.float32)
    fake_images = np.empty_like(real_images)
    if return_condition:
        conditions = np.empty_like(fake_images)
    batch_size = dataloader.batch_size
    generator.eval()
    dl_iter = iter(dataloader)
    if verbose:
        import tqdm
        dl_iter = tqdm.tqdm(dl_iter)
    with torch.no_grad():
        for idx, batch in enumerate(dl_iter):
            real_data = batch[""img""]
            z = truncated_z(real_data, generator.z_shape, truncation_level)
            fake_data = generator(**batch, z=z)
            start = idx * batch_size
            end = start + len(real_data)
            real_data = torch_utils.image_to_numpy(real_data, denormalize=True)
            fake_data = torch_utils.image_to_numpy(fake_data, denormalize=True)
            real_images[start:end] = real_data
            fake_images[start:end] = fake_data
            if return_condition:
                conditions[start:end] = torch_utils.image_to_numpy(
                    batch[""condition""], denormalize=True)
    generator.train()
    if return_condition:
        return real_images, fake_images, conditions
    return real_images, fake_images
"
DeepPrivacy,base_trainer.py,"import torch
import numpy as np
import weakref
import collections
from .checkpointer import Checkpointer
from . import hooks
from deep_privacy import logger


torch.manual_seed(0)
np.random.seed(0)
torch.backends.cudnn.benchmark = True


class BaseTrainer:

    def __init__(self, output_dir: str):
        self.hooks: collections.OrderedDict[str, hooks.HookBase] = {}
        self.sigterm_received = False
        self.checkpointer = Checkpointer(output_dir)
        self.checkpointer.trainer = self

    def register_hook(self, key: str, hook: hooks.HookBase):
        assert key not in self.hooks
        self.hooks[key] = hook
        # To avoid circular reference, hooks and trainer cannot own each other.
        # This normally does not matter, but will cause memory leak if the
        # involved objects contain __del__:
        # See
        # http://engineering.hearsaysocial.com/2013/06/16/circular-references-in-python/
        assert isinstance(hook, hooks.HookBase)
        hook.trainer = weakref.proxy(self)

    def before_extend(self):
        for hook in self.hooks.values():
            hook.before_extend()

    def before_train(self):
        for hook in self.hooks.values():
            hook.before_train()

    def before_step(self):
        for hook in self.hooks.values():
            hook.before_step()

    def after_step(self):
        for hook in self.hooks.values():
            hook.after_step()

    def after_extend(self):
        for hook in self.hooks.values():
            hook.after_extend()

    def state_dict(self) -> dict:
        state_dict = {}
        for key, hook in self.hooks.items():
            hsd = hook.state_dict()
            if hsd is not None:
                state_dict[key] = hook.state_dict()
        return state_dict

    def load_state_dict(self, state_dict: dict):
        for key, hook in self.hooks.items():
            if hook.state_dict() is None:
                continue
            hook.load_state_dict(state_dict[key])

    def save_checkpoint(self, filepath=None, max_keep=2):
        logger.info(f""Saving checkpoint to: {filepath}"")
        state_dict = self.state_dict()
        self.checkpointer.save_checkpoint(
            state_dict, filepath, max_keep)

    def load_checkpoint(self):
        if not self.checkpointer.checkpoint_exists():
            return
        state_dict = self.checkpointer.load_checkpoint()
        self.load_state_dict(state_dict)
"
DeepPrivacy,checkpointer.py,"import torch
from deep_privacy import logger
import pathlib


def _get_map_location():
    if not torch.cuda.is_available():
        logger.warn(
            ""Cuda is not available. Forcing map checkpoint to be loaded into CPU."")
        return ""cpu""
    return None


def load_checkpoint_from_url(model_url: str):
    if model_url is None:
        return None
    return torch.hub.load_state_dict_from_url(
        model_url, map_location=_get_map_location())


def load_checkpoint(ckpt_dir_or_file: pathlib.Path) -> dict:
    if ckpt_dir_or_file.is_dir():
        with open(ckpt_dir_or_file.joinpath('latest_checkpoint')) as f:
            ckpt_path = f.readline().strip()
            ckpt_path = ckpt_dir_or_file.joinpath(ckpt_path)
    else:
        ckpt_path = ckpt_dir_or_file
    if not ckpt_path.is_file():
        raise FileNotFoundError(f""Did not find path: {ckpt_path}"")
    ckpt = torch.load(ckpt_path, map_location=_get_map_location())
    logger.info(f""Loaded checkpoint from {ckpt_path}"")
    return ckpt


def _get_checkpoint_path(
        output_dir: str, validation_checkpoint_step: int = None):
    if validation_checkpoint_step is None:
        return pathlib.Path(output_dir, ""checkpoints"")
    step = validation_checkpoint_step * 10**6
    path = pathlib.Path(
        output_dir, ""validation_checkpoints"", f""step_{step}.ckpt"")
    return path


def get_checkpoint(
        output_dir: str, validation_checkpoint_step: int = None):
    path = _get_checkpoint_path(output_dir, validation_checkpoint_step)
    return load_checkpoint(path)


def get_previous_checkpoints(directory: pathlib.Path) -> list:
    if directory.is_file():
        directory = directory.parent
    list_path = directory.joinpath(""latest_checkpoint"")
    list_path.touch(exist_ok=True)
    with open(list_path) as fp:
        ckpt_list = fp.readlines()
    return [_.strip() for _ in ckpt_list]


def get_checkpoint_step(output_dir: str, validation_checkpoint_step: int):
    if validation_checkpoint_step is not None:
        return validation_checkpoint_step
    directory = _get_checkpoint_path(output_dir)
    ckpt_path = get_previous_checkpoints(directory)[0]
    print(ckpt_path)
    ckpt_path = pathlib.Path(ckpt_path)
    step = ckpt_path.stem.replace(""step_"", """")
    step = step.replace("".ckpt"", """")
    return int(step)


class Checkpointer:

    def __init__(self, output_dir: str):
        self.checkpoint_dir = pathlib.Path(
            output_dir, ""checkpoints"")
        self.checkpoint_dir.mkdir(exist_ok=True, parents=True)

    def save_checkpoint(
            self,
            state_dict: dict,
            filepath: pathlib.Path = None,
            max_keep=2):
        if filepath is None:
            global_step = self.trainer.global_step
            filename = f""step_{global_step}.ckpt""
            filepath = self.checkpoint_dir.joinpath(filename)
        list_path = filepath.parent.joinpath(""latest_checkpoint"")
        torch.save(state_dict, filepath)
        previous_checkpoints = get_previous_checkpoints(filepath)
        if filepath.name not in previous_checkpoints:
            previous_checkpoints = [filepath.name] + previous_checkpoints
        if len(previous_checkpoints) > max_keep:
            for ckpt in previous_checkpoints[max_keep:]:
                path = self.checkpoint_dir.joinpath(ckpt)
                if path.exists():
                    logger.info(f""Removing old checkpoint: {path}"")
                    path.unlink()
        previous_checkpoints = previous_checkpoints[:max_keep]
        with open(list_path, 'w') as fp:
            fp.write(""\n"".join(previous_checkpoints))
        logger.info(f""Saved checkpoint to: {filepath}"")

    def checkpoint_exists(self) -> bool:
        num_checkpoints = len(list(self.checkpoint_dir.glob(""*.ckpt"")))
        return num_checkpoints > 0

    def load_checkpoint(self) -> dict:
        checkpoint = load_checkpoint(self.checkpoint_dir)
        return checkpoint
"
DeepPrivacy,__init__.py,"from .trainer import Trainer
from .progressive_trainer import ProgressiveTrainer
"
DeepPrivacy,progressive_trainer.py,"import numpy as np
from deep_privacy import torch_utils, logger
from .trainer import Trainer
from deep_privacy.dataset import build_dataloader_train, build_dataloader_val


class ProgressiveTrainer(Trainer):

    def __init__(self, cfg):
        self.prev_transition = 0
        self.transition_iters = cfg.trainer.progressive.transition_iters
        self.transition_value = None
        super().__init__(cfg)

    def state_dict(self) -> dict:
        state_dict = super().state_dict()
        state_dict.update({
            ""prev_transition"": self.prev_transition
        })
        return state_dict

    def load_state_dict(self, state_dict: dict) -> None:
        super().load_state_dict(state_dict)
        self.prev_transition = state_dict[""prev_transition""]

    def _grow_phase(self):
        # Log transition value here to not create misguiding representation on
        # tensorboard
        if self.transition_value is not None:
            logger.log_variable(
                ""stats/transition-value"", self.get_transition_value())

        self._update_transition_value()
        transition_iters = self.transition_iters
        minibatch_repeats = self.cfg.trainer.progressive.minibatch_repeats
        next_transition = self.prev_transition + transition_iters
        num_batches = (next_transition - self.global_step) / self.batch_size()
        num_batches = int(np.ceil(num_batches))
        num_repeats = int(np.ceil(num_batches / minibatch_repeats))
        logger.info(
            f""Starting grow phase for imsize={self.current_imsize()}"" +
            f"" Training for {num_batches} batches with batch size: {self.batch_size()}"")
        for it in range(num_repeats):
            for _ in range(min(minibatch_repeats,
                               num_batches - it * minibatch_repeats)):
                self.train_step()
            self._update_transition_value()
        # Check that grow phase happens at correct spot
        assert self.global_step >= self.prev_transition + transition_iters,\
            f""Global step: {self.global_step}, batch size: {self.batch_size()}, prev_transition: {self.prev_transition}"" +\
            f"" transition iters: {transition_iters}""
        assert self.global_step - self.batch_size() <= self.prev_transition + transition_iters,\
            f""Global step: {self.global_step}, batch size: {self.batch_size()}, prev_transition: {self.prev_transition}"" +\
            f"" transition iters: {transition_iters}""

    def _update_transition_value(self):
        if self._get_phase() == ""stability"":
            self.transition_value = 1.0
        else:
            remaining = self.global_step - self.prev_transition
            v = remaining / self.transition_iters
            assert 0 <= v <= 1
            self.transition_value = v
        self.generator.update_transition_value(self.transition_value)
        self.discriminator.update_transition_value(self.transition_value)
        self.RA_generator.update_transition_value(self.transition_value)
        logger.log_variable(
            ""stats/transition-value"", self.get_transition_value())

    def get_transition_value(self):
        return self.transition_value

    def _stability_phase(self):
        self._update_transition_value()
        assert self.get_transition_value() == 1.0

        if self.prev_transition == 0:
            next_transition = self.transition_iters
        else:
            next_transition = self.prev_transition + self.transition_iters * 2

        num_batches = (next_transition - self.global_step) / self.batch_size()
        num_batches = int(np.ceil(num_batches))
        assert num_batches > 0
        logger.info(
            f""Starting stability phase for imsize={self.current_imsize()}"" +
            f"" Training for {num_batches} batches with batch size: {self.batch_size()}"")
        for it in range(num_batches):
            self.train_step()

    def _get_phase(self):
        # Initial training pahse
        if self.global_step < self.transition_iters:
            return ""stability""
        # Last phase
        if self.current_imsize() == self.cfg.models.max_imsize:
            if self.global_step >= self.prev_transition + self.transition_iters:
                return ""stability""
            return ""grow""
        if self.global_step < self.prev_transition + self.transition_iters:
            return ""grow""
        assert self.prev_transition + self.transition_iters <= self.global_step
        assert self.global_step <= self.prev_transition + self.transition_iters * 2
        return ""stability""

    def train_infinite(self):
        self._update_transition_value()
        while True:
            self.train_step()

    def train(self):
        self.before_train()
        while self.current_imsize() != self.cfg.models.max_imsize:
            if self._get_phase() == ""grow"":
                self._grow_phase()
            else:
                self._stability_phase()
                self.grow_models()
                self.prev_transition = self.global_step
        else:
            if self._get_phase() == ""grow"":
                self._grow_phase()
        self.train_infinite()

    def grow_models(self):
        self.before_extend()
        self.discriminator.extend()
        self.generator.extend()
        self.RA_generator.extend()
        self.RA_generator = torch_utils.to_cuda(self.RA_generator)
        del self.dataloader_train, self.dataloader_val
        self.load_dataset()
        self.init_optimizer()
        self.after_extend()

    def load_dataset(self):
        self.dataloader_train = iter(build_dataloader_train(
            self.cfg,
            self.current_imsize(),
            self.get_transition_value))
        self.dataloader_val = build_dataloader_val(
            self.cfg,
            self.current_imsize(),
            self.get_transition_value)
"
DeepPrivacy,trainer.py,"import torch
from deep_privacy import torch_utils
from deep_privacy import logger
from deep_privacy.dataset import build_dataloader_train, build_dataloader_val
from deep_privacy.modeling import loss, models
from .base_trainer import BaseTrainer
from .hooks import build_hooks


class Trainer(BaseTrainer):

    def __init__(self, cfg):
        self.cfg = cfg
        super().__init__(cfg.output_dir)

        build_hooks(cfg, self)
        self.global_step = 0
        logger.init(cfg.output_dir)
        self.init_models()
        self.init_optimizer()
        self.load_checkpoint()

    def state_dict(self):
        state_dict = {
            ""D"": self.discriminator.state_dict(),
            ""G"": self.generator.state_dict(),
            ""optimizer"": self.loss_optimizer.state_dict(),
            ""global_step"": self.global_step,
            ""running_average_generator"": self.RA_generator.state_dict()
        }
        state_dict.update(super().state_dict())
        return state_dict

    def load_state_dict(self, state_dict: dict):
        self.global_step = state_dict[""global_step""]
        logger.update_global_step(self.global_step)
        self.discriminator.load_state_dict(state_dict[""D""])
        self.generator.load_state_dict(state_dict[""G""])
        self.RA_generator.load_state_dict(
            state_dict[""running_average_generator""]
        )
        self.RA_generator = torch_utils.to_cuda(self.RA_generator)
        self.init_optimizer()
        self.loss_optimizer.load_state_dict(state_dict[""optimizer""])
        super().load_state_dict(state_dict)

    def batch_size(self) -> int:
        batch_size_schedule = self.cfg.trainer.batch_size_schedule
        return batch_size_schedule[self.current_imsize()]

    def init_models(self):
        self.discriminator = models.build_discriminator(
            self.cfg, data_parallel=torch.cuda.device_count() > 1)
        self.generator = models.build_generator(
            self.cfg, data_parallel=torch.cuda.device_count() > 1)
        self.RA_generator = models.build_generator(
            self.cfg, data_parallel=torch.cuda.device_count() > 1)
        self.RA_generator = torch_utils.to_cuda(self.RA_generator)
        self.RA_generator.load_state_dict(self.generator.state_dict())
        logger.info(str(self.generator))
        logger.info(str(self.discriminator))
        logger.log_variable(
            ""stats/discriminator_parameters"",
            torch_utils.number_of_parameters(self.discriminator))
        logger.log_variable(
            ""stats/generator_parameters"",
            torch_utils.number_of_parameters(self.generator))

    def current_imsize(self) -> int:
        return self.generator.current_imsize

    def train_step(self):
        self.before_step()
        batch = next(self.dataloader_train)
        logger.update_global_step(self.global_step)
        to_log = self.loss_optimizer.step(batch)
        while to_log is None:
            to_log = self.loss_optimizer.step(batch)
            self.hooks[""StatsLogger""].num_skipped_steps += 1
        to_log = {f""loss/{key}"": item for key, item in to_log.items()}
        self.hooks[""StatsLogger""].to_log = to_log
        self.after_step()
        self.global_step += self.batch_size()

    def load_dataset(self):
        self.dataloader_train = iter(build_dataloader_train(
            self.cfg,
            self.current_imsize(),
            None))
        self.dataloader_val = build_dataloader_val(
            self.cfg,
            self.current_imsize(),
            None)

    def init_optimizer(self):
        self.loss_optimizer = loss.LossOptimizer.build_from_cfg(
            self.cfg, self.discriminator, self.generator
        )
        self.generator, self.discriminator = self.loss_optimizer.initialize_amp()
        logger.log_variable(
            ""stats/learning_rate"", self.loss_optimizer._learning_rate)


    def before_train(self):
        self.load_dataset()
        super().before_train()

    def before_step(self):
        logger.update_global_step(self.global_step)
        super().before_step()

    def train(self):
        self.before_train()
        while True:
            self.train_step()
"
DeepPrivacy,hooks.py,"import signal
import pathlib
from deep_privacy import logger
from .base import HookBase, HOOK_REGISTRY


@HOOK_REGISTRY.register_module
class RunningAverageHook(HookBase):

    def before_train(self):
        self.update_beta()

    def update_beta(self):
        batch_size = self.trainer.batch_size()
        g = self.trainer.RA_generator
        g.update_beta(
            batch_size
        )
        logger.log_variable(""stats/running_average_decay"", g.ra_beta)

    def before_extend(self):
        self.update_beta()

    def after_extend(self):
        self.update_beta()

    def after_step(self):
        rae_generator = self.trainer.RA_generator
        generator = self.trainer.generator
        rae_generator.update_ra(generator)


@HOOK_REGISTRY.register_module
class CheckpointHook(HookBase):

    def __init__(
            self,
            ims_per_checkpoint: int,
            output_dir: pathlib.Path):
        self.ims_per_checkpoint = ims_per_checkpoint
        self.next_validation_checkpoint = ims_per_checkpoint
        self.validation_checkpoint_dir = pathlib.Path(
            output_dir, ""validation_checkpoints"")
        self.transition_checkpoint_dir = pathlib.Path(
            output_dir, ""transition_checkpoints"")
        self.validation_checkpoint_dir.mkdir(exist_ok=True, parents=True)
        self.transition_checkpoint_dir.mkdir(exist_ok=True, parents=True)

    def after_step(self):
        if self.global_step() >= self.next_validation_checkpoint:
            self.next_validation_checkpoint += self.ims_per_checkpoint
            self.trainer.save_checkpoint()

        self.save_validation_checkpoint()

    def state_dict(self):
        return {""next_validation_checkpoint"": self.next_validation_checkpoint}

    def load_state_dict(self, state_dict: dict):
        next_validation_checkpoint = state_dict[""next_validation_checkpoint""]
        self.next_validation_checkpoint = next_validation_checkpoint

    def save_validation_checkpoint(self):
        checkpoints = [12, 20, 30, 40, 50]
        for checkpoint_step in checkpoints:
            checkpoint_step = checkpoint_step * 10**6
            previous_global_step = self.global_step() - self.trainer.batch_size()
            if self.global_step() >= checkpoint_step and previous_global_step < checkpoint_step:
                logger.info(""Saving global checkpoint for validation"")
                filepath = self.validation_checkpoint_dir.joinpath(
                    f""step_{self.global_step()}.ckpt""
                )
                self.trainer.save_checkpoint(
                    filepath, max_keep=len(checkpoints) + 1)

    def before_extend(self):
        filepath = self.transition_checkpoint_dir.joinpath(
            f""imsize_{self.current_imsize()}.ckpt""
        )
        self.trainer.save_checkpoint(filepath)


@HOOK_REGISTRY.register_module
class SigTermHook(HookBase):

    def __init__(self):
        self.sigterm_received = False
        signal.signal(signal.SIGINT, self.handle_sigterm)
        signal.signal(signal.SIGTERM, self.handle_sigterm)

    def handle_sigterm(self, signum, frame):
        logger.info(
            ""[SIGTERM RECEVIED] Received sigterm. Stopping train after step."")
        self.sigterm_received = True
        exit()

    def after_step(self):
        if self.sigterm_received:
            logger.info(""[SIGTERM RECEIVED] Stopping train."")
            self.trainer.save_checkpoint(max_keep=3)
            exit()
"
DeepPrivacy,__init__.py,"from .hooks import RunningAverageHook, CheckpointHook, SigTermHook
from .log_hooks import ImageSaveHook, MetricHook, StatsLogger
from .base import build_hooks, HookBase
"
DeepPrivacy,log_hooks.py,"import torch
import logging
import time
from deep_privacy import torch_utils, logger
from deep_privacy.metrics import metric_api
from .base import HookBase, HOOK_REGISTRY
from deep_privacy.inference import infer
try:
    from apex import amp
except ImportError:
    pass


@HOOK_REGISTRY.register_module
class ImageSaveHook(HookBase):

    def __init__(self, ims_per_save: int, n_diverse_samples: int):
        self.ims_per_save = ims_per_save
        self.next_save_point = self.ims_per_save
        self.before_images = None
        self._n_diverse_samples = n_diverse_samples

    def state_dict(self):
        return {
            ""next_save_point"": self.next_save_point,
            ""before_images"": self.before_images}

    def load_state_dict(self, state_dict: dict):
        self.next_save_point = state_dict[""next_save_point""]
        self.before_images = state_dict[""before_images""]

    def after_step(self):
        if self.global_step() >= self.next_save_point:
            self.next_save_point += self.ims_per_save
            self.save_fake_images(True)
            self.save_fake_images(False)

    def save_fake_images(self, validation: bool):
        g = self.trainer.generator
        if validation:
            g = self.trainer.RA_generator
        fake_data, real_data, condition = self.get_images(g)
        fake_data = fake_data[:64]
        logger.save_images(
            ""fakes"", fake_data, denormalize=True, nrow=8,
            log_to_validation=validation)
        logger.save_images(
            ""reals"", real_data[:64], denormalize=True, log_to_writer=False,
            nrow=8,
            log_to_validation=validation)
        condition = condition[:64]
        logger.save_images(
            ""condition"", condition, log_to_writer=False, denormalize=True,
            nrow=8,
            log_to_validation=validation)
        self.save_images_diverse()

    def get_images(self, g):
        g.eval()
        batch = next(iter(self.trainer.dataloader_val))
        z = g.generate_latent_variable(batch[""img""]).zero_()
        with torch.no_grad():
            fake_data_sample = g(**batch,
                                 z=z)
        g.train()
        return fake_data_sample, batch[""img""], batch[""condition""]

    @torch.no_grad()
    def save_images_diverse(self):
        """"""
            Generates images with several latent variables
        """"""
        g = self.trainer.RA_generator
        g.eval()
        batch = next(iter(self.trainer.dataloader_val))
        batch = {k: v[:8] for k, v in batch.items()}
        fakes = [batch[""condition""].cpu()]
        for i in range(self._n_diverse_samples):
            z = g.generate_latent_variable(batch[""img""])
            fake = g(**batch, z=z)
            fakes.append(fake.cpu())
        fakes = torch.cat(fakes)
        logger.save_images(
            ""diverse"", fakes, log_to_validation=True, nrow=8, denormalize=True)
        g.train()

    def before_extend(self):
        transition_value = 1
        self.trainer.RA_generator.update_transition_value(
            transition_value
        )
        fake_data, real_data, condition = self.get_images(
            self.trainer.RA_generator
        )
        before_images = [
            torch_utils.denormalize_img(x[:8])
            for x in [real_data, fake_data, condition]
        ]
        before_images = torch.cat((before_images), dim=0)
        self.before_images = before_images.cpu()

    def after_extend(self):
        transition_value = 0
        self.trainer.RA_generator.update_transition_value(
            transition_value
        )
        fake_data, real_data, condition = self.get_images(
            self.trainer.RA_generator
        )

        after_images = [
            torch_utils.denormalize_img(x[:8])
            for x in [real_data, fake_data, condition]
        ]
        after_images = torch.cat((after_images), dim=0)
        after_images = torch.nn.functional.avg_pool2d(after_images, 2)
        after_images = after_images.cpu()
        assert after_images.shape == self.before_images.shape
        diff = self.before_images - after_images
        to_save = torch.cat(
            (self.before_images, after_images, diff), dim=2)
        imsize = after_images.shape[-1]
        imname = f""transition/from_{imsize}""
        logger.save_images(imname, to_save,
                           log_to_writer=True, nrow=8 * 3)
        self.before_images = None


@HOOK_REGISTRY.register_module
class MetricHook(HookBase):

    def __init__(
            self,
            ims_per_log: int,
            fid_batch_size: int,
            lpips_batch_size: int,
            min_imsize_to_calculate: int):
        self.next_check = ims_per_log
        self.num_ims_per_fid = ims_per_log
        self.lpips_batch_size = lpips_batch_size
        self.fid_batch_size = fid_batch_size
        self.min_imsize_to_calculate = min_imsize_to_calculate

    def state_dict(self):
        return {""next_check"": self.next_check}

    def load_state_dict(self, state_dict: dict):
        self.next_check = state_dict[""next_check""]

    def after_step(self):
        if self.global_step() >= self.next_check:
            self.next_check += self.num_ims_per_fid
            if self.current_imsize() >= self.min_imsize_to_calculate:
                self.calculate_fid()

    def calculate_fid(self):
        logger.info(""Starting calculation of FID value"")
        generator = self.trainer.RA_generator
        real_images, fake_images = infer.infer_images(
            self.trainer.dataloader_val, generator,
            truncation_level=0
        )
        """"""
        # Remove FID calculation as holy shit this is expensive.
        cfg = self.trainer.cfg
        identifier = f""{cfg.dataset_type}_{cfg.data_val.dataset.percentage}_{self.current_imsize()}""
        transition_value = self.trainer.RA_generator.transition_value
        fid_val = metric_api.fid(
            real_images, fake_images,
            batch_size=self.fid_batch_size)
        logger.log_variable(""stats/fid"", np.mean(fid_val),
                            log_level=logging.INFO)
        """"""

        l1 = metric_api.l1(real_images, fake_images)
        l2 = metric_api.l1(real_images, fake_images)
        psnr = metric_api.psnr(real_images, fake_images)
        lpips = metric_api.lpips(
            real_images, fake_images, self.lpips_batch_size)
        logger.log_variable(""stats/l1"", l1, log_level=logging.INFO)
        logger.log_variable(""stats/l2"", l2, log_level=logging.INFO)
        logger.log_variable(""stats/psnr"", psnr, log_level=logging.INFO)
        logger.log_variable(""stats/lpips"", lpips, log_level=logging.INFO)


@HOOK_REGISTRY.register_module
class StatsLogger(HookBase):

    def __init__(
            self,
            num_ims_per_log: int):
        self.num_ims_per_log = num_ims_per_log
        self.next_log_point = self.num_ims_per_log
        self.start_time = time.time()
        self.num_skipped_steps = 0

    def state_dict(self):
        return {
            ""total_time"": (time.time() - self.start_time),
            ""num_skipped_steps"": self.num_skipped_steps
        }

    def load_state_dict(self, state_dict: dict):
        self.start_time = time.time() - state_dict[""total_time""]
        self.num_skipped_steps = state_dict[""num_skipped_steps""]

    def before_train(self):
        self.batch_start_time = time.time()
        self.log_dictionary({""stats/batch_size"": self.trainer.batch_size()})

    def log_dictionary(self, to_log: dict):
        logger.log_dictionary(to_log)

    def after_step(self):
        has_gradient_penalty = ""loss/gradient_penalty"" in self.to_log
        if has_gradient_penalty or self.global_step() >= self.next_log_point:
            self.log_stats()
            self.log_dictionary(self.to_log)
            self.log_loss_scales()
            self.next_log_point = self.global_step() + self.num_ims_per_log

    def log_stats(self):
        time_spent = time.time() - self.batch_start_time
        num_steps = self.global_step() - self.next_log_point + self.num_ims_per_log
        num_steps = max(num_steps, 1)
        nsec_per_img = time_spent / num_steps
        total_time = (time.time() - self.start_time) / 60
        to_log = {
            ""stats/nsec_per_img"": nsec_per_img,
            ""stats/batch_size"": self.trainer.batch_size(),
            ""stats/training_time_minutes"": total_time,
        }
        self.batch_start_time = time.time()
        self.log_dictionary(to_log)

    def log_loss_scales(self):
        to_log = {f'amp/loss_scale_{loss_idx}': loss_scaler._loss_scale
                  for loss_idx, loss_scaler in enumerate(amp._amp_state.loss_scalers)}
        to_log['amp/num_skipped_gradients'] = self.num_skipped_steps
        self.log_dictionary(to_log)
"
DeepPrivacy,base.py,"from deep_privacy.utils import Registry, build_from_cfg

HOOK_REGISTRY = Registry(""HOOKS"")


def build_hooks(cfg, trainer):
    for _hook in cfg.trainer.hooks:
        if _hook.type == ""CheckpointHook"":
            hook = build_from_cfg(
                _hook, HOOK_REGISTRY, output_dir=cfg.output_dir)
        else:
            hook = build_from_cfg(_hook, HOOK_REGISTRY)
        trainer.register_hook(_hook.type, hook)


class HookBase:

    def before_train(self):
        pass

    def after_train(self):
        pass

    def before_step(self):
        pass

    def after_step(self):
        pass

    def after_extend(self):
        """"""
            Will be called after we increase resolution / model size
        """"""
        pass

    def before_extend(self):
        """"""
            Will be called before we increase resolution / model size
        """"""
        pass

    def load_state_dict(self, state_dict: dict):
        pass

    def state_dict(self):
        return None

    def global_step(self):
        return self.trainer.global_step

    def current_imsize(self):
        return self.trainer.current_imsize()

    def get_transition_value(self):
        return self.trainer.hooks[""progressive_trainer_hook""].transition_value
"
DeepPrivacy,build.py,"from deep_privacy.utils import build_from_cfg, Registry


DETECTOR_REGISTRY = Registry(""DETECTOR_REGISTRY"")


def build_detector(cfg, *args, **kwargs):
    print(cfg)
    return build_from_cfg(cfg, DETECTOR_REGISTRY, *args, **kwargs)
"
DeepPrivacy,__init__.py,"from .detection_api import BaseDetector, RCNNDetector, ImageAnnotation
from .build import build_detector
"
DeepPrivacy,detection_api.py,"import numpy as np
import typing
import torch
import face_detection
import cv2
from deep_privacy import torch_utils
from deep_privacy.box_utils import clip_box, expand_bbox, cut_face
from . import keypoint_rcnn
from .build import DETECTOR_REGISTRY
from .utils import match_bbox_keypoint


def tight_crop(array):
    mask = array == 0
    x0 = np.argmax(mask.any(axis=0))
    x1 = array.shape[1] - np.argmax((np.flip(mask, axis=1)).any(axis=0))
    y0 = np.argmax(mask.any(axis=1))
    y1 = array.shape[0] - np.argmax((np.flip(mask, axis=0).any(axis=1)))
    return np.array([x0, y0, x1, y1])


def generate_rotation_matrix(
        im, landmark, bbox, detector_cls: str, inverse=False):
    """"""
        Creates a rotation matrix to align the two eye landmarks to be horizontal.
    """"""
    if detector_cls == ""BaseDetector"":
        lm0 = landmark[0]
        lm1 = landmark[1]
    else:
        lm0 = landmark[2]
        lm1 = landmark[1]
    l1 = lm1 - lm0
    l2 = np.array((1, 0)).astype(int)
    alpha = np.arctan2(*l2) - np.arctan2(*l1)
    if inverse:
        alpha = - alpha
    center = bbox[:2] + (bbox[2:] - bbox[:2]) / 2
    center = (center[0], center[1])
    matrix = cv2.getRotationMatrix2D(
        center=center, angle=180 * alpha / np.pi, scale=1)

    # Expand bbox to prevent cutting the cheek
    box = bbox.reshape(-1, 2)
    box = np.pad(box, ((0, 0,), (0, 1)), constant_values=1)
    box = box.dot(matrix.T).reshape(-1)
    x0, y0, x1, y1 = box
    new_y_len = max(x1 - x0, y1 - y0)
    cent = bbox[1] + (bbox[3] - bbox[1]) / 2
    orig = bbox
    bbox = bbox.copy()
    bbox[1] = cent - new_y_len / 2
    bbox[3] = cent + new_y_len / 2
    bbox[1] = max(bbox[1], 0)
    bbox[3] = min(bbox[3], im.shape[1])
    return matrix, orig


class ImageAnnotation:

    def __init__(
            self,
            bbox_XYXY: np.ndarray,
            keypoints: np.ndarray,
            im: np.ndarray,
            detector_cls: str,
            simple_expand: bool,
            align_faces: bool,
            resize_background: bool,
            generator_imsize: int):
        self.align_faces = align_faces
        self.resize_background = resize_background
        self.generator_imsize = generator_imsize
        self.bbox_XYXY = bbox_XYXY
        self.keypoints = keypoints[:, :7, :]
        self.im = im
        self.imshape = im.shape
        self.mask = None
        self._detector_cls = detector_cls
        assert keypoints.shape[2] == 2, f""Shape: {keypoints.shape}""
        assert bbox_XYXY.shape[1] == 4
        self.match()
        self.preprocess()
        self.simple_expand = simple_expand

    def preprocess(self):
        if self.align_faces:
            self.rotation_matrices = np.zeros((len(self), 2, 3))
            for face_idx in range(len(self)):
                rot_matrix, new_bbox = generate_rotation_matrix(
                    self.im, self.keypoints[face_idx],
                    self.bbox_XYXY[face_idx], self._detector_cls)
                self.rotation_matrices[face_idx] = rot_matrix
                self.bbox_XYXY[face_idx] = new_bbox

    def match(self):
        self.bbox_XYXY, self.keypoints = match_bbox_keypoint(
            self.bbox_XYXY, self.keypoints
        )
        assert self.bbox_XYXY.shape[0] == self.keypoints.shape[0]

    def get_expanded_bbox(self, face_idx):
        assert face_idx < len(self)
        tight_bbox = self.bbox_XYXY[face_idx]
        expanded_bbox = expand_bbox(
            tight_bbox,
            self.im.shape,
            simple_expand=self.simple_expand,
            default_to_simple=True,
            expansion_factor=0.35
        )
        width = expanded_bbox[2] - expanded_bbox[0]
        height = expanded_bbox[3] - expanded_bbox[1]
        assert width == height
        return expanded_bbox

    def aligned_keypoint(self, face_idx):
        assert face_idx < len(self)
        keypoint = self.keypoints[face_idx].copy().astype(float)
        if self.align_faces:
            matrix = self.rotation_matrices[face_idx]
            keypoint = np.pad(
                keypoint, ((0, 0), (0, 1)), constant_values=1
            )
            keypoint = keypoint.dot(matrix.T)
        expanded_bbox = self.get_expanded_bbox(face_idx)
        keypoint[:, 0] -= expanded_bbox[0]
        keypoint[:, 1] -= expanded_bbox[1]
        w = expanded_bbox[2] - expanded_bbox[0]
        keypoint /= w
        keypoint[keypoint < 0] = 0
        keypoint[keypoint > 1] = 1
        keypoint = torch.from_numpy(keypoint).view(1, -1)
        return keypoint

    def __repr__(self):
        return f""Image Annotation. BBOX_XYXY: {self.bbox_XYXY.shape}"" +\
            f"" Keypoints: {self.keypoints.shape}""

    def __len__(self):
        return self.keypoints.shape[0]

    def get_mask(self, idx):
        mask = np.ones(self.im.shape[:2], dtype=np.bool)
        x0, y0, x1, y1 = self.bbox_XYXY[idx]
        mask[y0:y1, x0:x1] = 0
        return mask

    def get_cut_mask(self, idx, imsize):
        box_exp = self.get_expanded_bbox(idx)
        boxes = self.bbox_XYXY[idx].copy().astype(np.float32)
        boxes[[0, 2]] -= box_exp[0]
        boxes[[1, 3]] -= box_exp[1]
        resize_factor = imsize / (box_exp[2] - box_exp[0])
        boxes *= resize_factor
        boxes = boxes.astype(int)
        mask = np.ones((imsize, imsize), dtype=np.bool)
        x0, y0, x1, y1 = boxes
        mask[y0:y1, x0:x1] = 0
        return mask

    def get_face(self, face_idx: int, imsize):
        assert face_idx < len(self)
        bbox = self.get_expanded_bbox(face_idx)
        im = self.im
        if self.align_faces:
            rot_matrix = self.rotation_matrices[face_idx]
            im = cv2.warpAffine(
                im, M=rot_matrix, dsize=(self.im.shape[1], self.im.shape[0]))
        face = cut_face(im, bbox, simple_expand=self.simple_expand)
        if imsize is not None:
            face = cv2.resize(face, (imsize, imsize),
                              interpolation=cv2.INTER_CUBIC)
        mask = self.get_cut_mask(face_idx, imsize)
        return face, mask

    def paste_face(self, face_idx, face):
        """"""
            Rotates the original image, pastes in the rotated face, then inverse rotate.
        """"""
        im = self.im / 255
        if self.align_faces:
            matrix = self.rotation_matrices[face_idx]
            im = cv2.warpAffine(
                im, M=matrix, dsize=(self.im.shape[1], self.im.shape[0]))

        bbox = self.bbox_XYXY[face_idx].copy()
        exp_bbox = self.get_expanded_bbox(face_idx)
        bbox[[0, 2]] -= exp_bbox[0]
        bbox[[1, 3]] -= exp_bbox[1]

        x0, y0, x1, y1 = self.get_expanded_bbox(face_idx)

        # expanded bbox might go outside of image.
        im[max(0, y0):min(y1, im.shape[0]),
           max(0, x0):min(x1, im.shape[1])] = face[
            max(-y0, 0): min(face.shape[0], face.shape[1] - (y1 - im.shape[0])),
            max(-x0, 0): min(face.shape[0], face.shape[0] - (x1 - im.shape[1]))]

        if self.align_faces:
            matrix, _ = generate_rotation_matrix(
                self.im, self.keypoints[face_idx], self.bbox_XYXY[face_idx],
                self._detector_cls,
                inverse=True
            )
            im = cv2.warpAffine(
                im, M=matrix, dsize=(self.im.shape[1], self.im.shape[0]))
        return im

    def stitch_faces(self, anonymized_faces):
        """"""
            Copies the generated face(s) to the original face
            Make sure that an already anonymized face is not overwritten.
        """"""
        im = self.im.copy()
        mask_not_filled = np.ones_like(im, dtype=bool)
        for face_idx, face in enumerate(anonymized_faces):
            orig_bbox = self.bbox_XYXY[face_idx]
            expanded_bbox = self.get_expanded_bbox(face_idx)
            orig_face_shape = (
                expanded_bbox[2] - expanded_bbox[0],
                expanded_bbox[3] - expanded_bbox[1]
            )
            face = cv2.resize(face, orig_face_shape)
            inpainted_im = self.paste_face(face_idx, face) * 255
            mask_ = cut_face(mask_not_filled, orig_bbox)
            x0, y0, x1, y1 = orig_bbox
            im[y0:y1, x0:x1][mask_] = inpainted_im[y0:y1, x0:x1][mask_]
            mask_not_filled[y0:y1, x0:x1] = 0
            if self.resize_background:
                face = cut_face(im, expanded_bbox, pad_im=False)
                orig_shape = face.shape[:2][::-1]
                face = cv2.resize(face, (self.generator_imsize*2, self.generator_imsize*2))
                
                x0, y0, x1, y1 = clip_box(expanded_bbox, im)
                im[y0:y1, x0:x1] = cv2.resize(face, orig_shape)
        return im


@DETECTOR_REGISTRY.register_module
class BaseDetector:

    def __init__(self, face_detector_cfg: dict, simple_expand: bool,
                 align_faces: bool, resize_background: bool,
                 generator_imsize: int, *args, **kwargs):
        self.face_detector = face_detection.build_detector(
            **face_detector_cfg,
            device=torch_utils.get_device()
        )
        self.simple_expand = simple_expand
        self.align_faces = align_faces
        self.resize_background = resize_background
        self.generator_imsize = generator_imsize
        if self.__class__.__name__ == ""BaseDetector"":
            assert face_detector_cfg.name == ""RetinaNetResNet50""

    def post_process_detections(
            self,
            images: typing.List[np.ndarray],
            im_bboxes: typing.List[np.ndarray],
            keypoints: typing.List[np.ndarray]):
        image_annotations = []
        for im_idx, im in enumerate(images):
            annotation = ImageAnnotation(
                im_bboxes[im_idx], keypoints[im_idx], im,
                self.__class__.__name__,
                self.simple_expand,
                self.align_faces,
                self.resize_background,
                self.generator_imsize
            )
            image_annotations.append(annotation)
        return image_annotations

    def get_detections(self,
                       images: typing.List[np.ndarray],
                       im_bboxes: typing.List[np.ndarray] = None
                       ) -> typing.List[ImageAnnotation]:
        im_bboxes = []
        keypoints = []
        if im_bboxes is None or len(im_bboxes) == 0:
            for im in images:
                boxes, keyps = self.face_detector.batched_detect_with_landmarks(
                    im[None])
                boxes = boxes[0][:, :4]
                im_bboxes.append(boxes.astype(int))
                keypoints.append(keyps[0])
        return self.post_process_detections(images, im_bboxes, keypoints)


@DETECTOR_REGISTRY.register_module
class RCNNDetector(BaseDetector):

    def __init__(self, keypoint_threshold: float, rcnn_batch_size: int, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.keypoint_detector = keypoint_rcnn.RCNNKeypointDetector(
            keypoint_threshold, rcnn_batch_size)
        self.keypoint_threshold = keypoint_threshold

    def detect_faces(self, images, im_bboxes):
        if im_bboxes is None or len(im_bboxes) == 0:
            im_bboxes = []
            for im in images:
                boxes = self.face_detector.batched_detect(im[None])
                boxes = boxes[0][:, :4]
                im_bboxes.append(boxes.astype(int))
        return im_bboxes

    def get_detections(self, images, im_bboxes=None):
        im_bboxes = self.detect_faces(images, im_bboxes)
        keypoints = self.keypoint_detector.batch_detect_keypoints(images)
        return self.post_process_detections(images, im_bboxes, keypoints)
"
DeepPrivacy,utils.py,"import numpy as np


def is_keypoint_within_bbox(x0, y0, x1, y1, keypoint):
    keypoint = keypoint[:3, :]  # only nose + eyes are relevant
    kp_X = keypoint[:, 0]
    kp_Y = keypoint[:, 1]
    within_X = np.all(kp_X >= x0) and np.all(kp_X <= x1)
    within_Y = np.all(kp_Y >= y0) and np.all(kp_Y <= y1)
    return within_X and within_Y


def match_bbox_keypoint(bounding_boxes, keypoints):
    """"""
        bounding_boxes shape: [N, 5]
        keypoints: [N persons, K keypoints, (x, y)]
    """"""
    if len(bounding_boxes) == 0 or len(keypoints) == 0:
        return np.empty((0, 5)), np.empty((0, 7, 2))
    assert bounding_boxes.shape[1] == 4,\
        f""Shape was : {bounding_boxes.shape}""
    assert keypoints.shape[-1] == 2,\
        f""Expected (x,y) in last axis, got: {keypoints.shape}""
    assert keypoints.shape[1] in (5, 7),\
        f""Expeted 5 or 7 keypoints. Keypoint shape was: {keypoints.shape}""

    matches = []
    for bbox_idx, bbox in enumerate(bounding_boxes):
        keypoint = None
        for kp_idx, keypoint in enumerate(keypoints):
            if kp_idx in (x[1] for x in matches):
                continue
            if is_keypoint_within_bbox(*bbox, keypoint):
                matches.append((bbox_idx, kp_idx))
                break
    keypoint_idx = [x[1] for x in matches]
    bbox_idx = [x[0] for x in matches]
    return bounding_boxes[bbox_idx], keypoints[keypoint_idx]
"
DeepPrivacy,keypoint_rcnn.py,"import numpy as np
import torch
from deep_privacy.torch_utils import to_cuda, image_to_torch
from torchvision.models.detection import keypointrcnn_resnet50_fpn


class RCNNKeypointDetector:

    def __init__(self, keypoint_treshold: float, batch_size: int):
        super().__init__()
        model = keypointrcnn_resnet50_fpn(pretrained=True)
        model.eval()
        to_cuda(model)
        self.batch_size = batch_size
        self.keypoint_threshold = keypoint_treshold
        self.model = model

    def detect_keypoints(self, img):
        img = image_to_torch(img, cuda=True)[0]
        with torch.no_grad():
            outputs = self.model([img])

        # Shape: [N persons, K keypoints, (x,y,visibility)]
        keypoints = outputs[0][""keypoints""]
        scores = outputs[0][""scores""]
        assert list(scores) == sorted(list(scores))[::-1]
        mask = scores >= self.keypoint_threshold
        keypoints = keypoints[mask, :, :2]
        return keypoints.cpu().numpy()

    def batch_detect_keypoints(self, images):
        images = [image_to_torch(im, cuda=False)[0] for im in images]
        keypoints = []
        if len(images) > 0:
            num_batches = int(np.ceil(len(images) / self.batch_size))
            with torch.no_grad():
                for i in range(num_batches):
                    images_ = images[i * self.batch_size:(i + 1) * self.batch_size]
                    images_ = [to_cuda(_) for _ in images_]
                    outputs = self.model(images_)
                    images_ = [_.cpu() for _ in images_]
                    kps = [o[""keypoints""].cpu() for o in outputs]
                    score = [o[""scores""].cpu() for o in outputs]
                    masks = [imscore >= self.keypoint_threshold
                             for imscore in score]
                    kps = [kp[mask, :, :2].numpy()
                           for kp, mask in zip(kps, masks)]
                    keypoints += kps
        return keypoints
"
DeepCTR,setup.py,"import sys

import setuptools

with open(""README.md"", ""r"") as fh:
    long_description = fh.read()

REQUIRED_PACKAGES = [
    'requests',
    'h5py==3.7.0; python_version>=""3.9""',
    'h5py==2.10.0; python_version<""3.9""'
]

setuptools.setup(
    name=""deepctr"",
    version=""0.9.3"",
    author=""Weichen Shen"",
    author_email=""weichenswc@163.com"",
    description=""Easy-to-use,Modular and Extendible package of deep learning based CTR(Click Through Rate) prediction models with tensorflow 1.x and 2.x ."",
    long_description=long_description,
    long_description_content_type=""text/markdown"",
    url=""https://github.com/shenweichen/deepctr"",
    download_url='https://github.com/shenweichen/deepctr/tags',
    packages=setuptools.find_packages(
        exclude=[""tests"", ""tests.models"", ""tests.layers""]),
    python_requires="">=2.7,!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*"",  # '>=3.4',  # 3.4.6
    install_requires=REQUIRED_PACKAGES,
    extras_require={
        ""cpu"": [""tensorflow>=1.4.0,!=1.7.*,!=1.8.*""],
        ""gpu"": [""tensorflow-gpu>=1.4.0,!=1.7.*,!=1.8.*""],
    },
    entry_points={
    },
    classifiers=(
        ""License :: OSI Approved :: Apache Software License"",
        ""Operating System :: OS Independent"",
        'Intended Audience :: Developers',
        'Intended Audience :: Education',
        'Intended Audience :: Science/Research',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 2.7',
        'Programming Language :: Python :: 3.6',
        'Programming Language :: Python :: 3.7',
        'Programming Language :: Python :: 3.8',
        'Programming Language :: Python :: 3.9',
        'Programming Language :: Python :: 3.10',
        'Topic :: Scientific/Engineering',
        'Topic :: Scientific/Engineering :: Artificial Intelligence',
        'Topic :: Software Development',
        'Topic :: Software Development :: Libraries',
        'Topic :: Software Development :: Libraries :: Python Modules',
    ),
    license=""Apache-2.0"",
    keywords=['ctr', 'click through rate',
              'deep learning', 'tensorflow', 'tensor', 'keras'],
)
"
DeepCTR,utils_test.py,"from deepctr.utils import check_version


def test_check_version():
    check_version('0.1.0')
    check_version(20191231)
"
DeepCTR,utils_mtl.py,"# test utils for multi task learning

import os

import numpy as np
import tensorflow as tf
from tensorflow.python.keras.models import load_model, save_model

from deepctr.feature_column import SparseFeat, DenseFeat, DEFAULT_GROUP_NAME
from deepctr.layers import custom_objects


def get_mtl_test_data(sample_size=10, embedding_size=4, sparse_feature_num=1,
                      dense_feature_num=1, task_types=('binary', 'binary'),
                      hash_flag=False, prefix='', use_group=False):
    feature_columns = []
    model_input = {}

    for i in range(sparse_feature_num):
        if use_group:
            group_name = str(i % 3)
        else:
            group_name = DEFAULT_GROUP_NAME
        dim = np.random.randint(1, 10)
        feature_columns.append(
            SparseFeat(prefix + 'sparse_feature_' + str(i), dim, embedding_size, use_hash=hash_flag, dtype=tf.int32,
                       group_name=group_name))

    for i in range(dense_feature_num):
        def transform_fn(x): return (x - 0.0) / 1.0

        feature_columns.append(
            DenseFeat(
                prefix + 'dense_feature_' + str(i),
                1,
                dtype=tf.float32,
                transform_fn=transform_fn
            )
        )

    for fc in feature_columns:
        if isinstance(fc, SparseFeat):
            model_input[fc.name] = np.random.randint(0, fc.vocabulary_size, sample_size)
        elif isinstance(fc, DenseFeat):
            model_input[fc.name] = np.random.random(sample_size)
    y_list = []  # multi label
    for task in task_types:
        if task == 'binary':
            y = np.random.randint(0, 2, sample_size)
            y_list.append(y)
        else:
            y = np.random.random(sample_size)
            y_list.append(y)

    return model_input, y_list, feature_columns


def check_mtl_model(model, model_name, x, y_list, task_types, check_model_io=True):
    """"""
    compile model,train and evaluate it,then save/load weight and model file.
    :param model:
    :param model_name:
    :param x:
    :param y_list: mutil label of y
    :param check_model_io: test save/load model file or not
    :return:
    """"""
    loss_list = []
    metric_list = []
    for task_type in task_types:
        if task_type == 'binary':
            loss_list.append('binary_crossentropy')
            # metric_list.append('accuracy')
        elif task_type == 'regression':
            loss_list.append('mean_squared_error')
            # metric_list.append('mae')
    print('loss:', loss_list)
    print('metric:', metric_list)
    model.compile('adam', loss=loss_list, metrics=metric_list)
    model.fit(x, y_list, batch_size=100, epochs=1, validation_split=0.5)

    print(model_name + "" test train valid pass!"")
    model.save_weights(model_name + '_weights.h5')
    model.load_weights(model_name + '_weights.h5')
    os.remove(model_name + '_weights.h5')
    print(model_name + "" test save load weight pass!"")
    if check_model_io:
        save_model(model, model_name + '.h5')
        model = load_model(model_name + '.h5', custom_objects)
        os.remove(model_name + '.h5')
        print(model_name + "" test save load model pass!"")

    print(model_name + "" test pass!"")
"
DeepCTR,feature_test.py,"from deepctr.models import DeepFM
from deepctr.feature_column import SparseFeat, DenseFeat, VarLenSparseFeat, get_feature_names
import numpy as np


def test_long_dense_vector():
    feature_columns = [SparseFeat('user_id', 4, ), SparseFeat('item_id', 5, ), DenseFeat(""pic_vec"", 5)]
    fixlen_feature_names = get_feature_names(feature_columns)

    user_id = np.array([[1], [0], [1]])
    item_id = np.array([[3], [2], [1]])
    pic_vec = np.array([[0.1, 0.5, 0.4, 0.3, 0.2], [0.1, 0.5, 0.4, 0.3, 0.2], [0.1, 0.5, 0.4, 0.3, 0.2]])
    label = np.array([1, 0, 1])

    input_dict = {'user_id': user_id, 'item_id': item_id, 'pic_vec': pic_vec}
    model_input = [input_dict[name] for name in fixlen_feature_names]

    model = DeepFM(feature_columns, feature_columns[:-1])
    model.compile('adagrad', 'binary_crossentropy')
    model.fit(model_input, label)


def test_feature_column_sparsefeat_vocabulary_path():
    vocab_path = ""./dummy_test.csv""
    sf = SparseFeat('user_id', 4, vocabulary_path=vocab_path)
    if sf.vocabulary_path != vocab_path:
        raise ValueError(""sf.vocabulary_path is invalid"")
    vlsf = VarLenSparseFeat(sf, 6)
    if vlsf.vocabulary_path != vocab_path:
        raise ValueError(""vlsf.vocabulary_path is invalid"")
"
DeepCTR,__init__.py,
DeepCTR,utils.py,"from __future__ import absolute_import, division, print_function

import inspect
import os
import sys

import numpy as np
import tensorflow as tf
from numpy.testing import assert_allclose
from packaging import version
from tensorflow.python.keras import backend as K
from tensorflow.python.keras.layers import Input, Masking
from tensorflow.python.keras.models import Model, load_model, save_model

from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, DEFAULT_GROUP_NAME
from deepctr.layers import custom_objects

SAMPLE_SIZE = 8
VOCABULARY_SIZE = 4


def test_estimator_version(tf_version):
    cur_version = version.parse(tf_version)
    tf2_version = version.parse('2.0.0')
    left_version = version.parse('2.2.0')
    right_version = version.parse('2.6.0')
    return cur_version < tf2_version or left_version <= cur_version < right_version


TEST_Estimator = test_estimator_version(tf.__version__)


def gen_sequence(dim, max_len, sample_size):
    return np.array([np.random.randint(0, dim, max_len) for _ in range(sample_size)]), np.random.randint(1, max_len + 1,
                                                                                                         sample_size)


def get_test_data(sample_size=1000, embedding_size=4, sparse_feature_num=1, dense_feature_num=1,
                  sequence_feature=None, classification=True, include_length=False,
                  hash_flag=False, prefix='', use_group=False):
    if sequence_feature is None:
        sequence_feature = ['sum', 'mean', 'max', 'weight']
    feature_columns = []
    model_input = {}

    if 'weight' in sequence_feature:
        feature_columns.append(
            VarLenSparseFeat(SparseFeat(prefix + ""weighted_seq"", vocabulary_size=2, embedding_dim=embedding_size),
                             maxlen=3, length_name=prefix + ""weighted_seq"" + ""_seq_length"",
                             weight_name=prefix + ""weight""))
        s_input, s_len_input = gen_sequence(
            2, 3, sample_size)

        model_input[prefix + ""weighted_seq""] = s_input
        model_input[prefix + 'weight'] = np.random.randn(sample_size, 3, 1)
        model_input[prefix + ""weighted_seq"" + ""_seq_length""] = s_len_input
        sequence_feature.pop(sequence_feature.index('weight'))

    for i in range(sparse_feature_num):
        if use_group:
            group_name = str(i % 3)
        else:
            group_name = DEFAULT_GROUP_NAME
        dim = np.random.randint(1, 10)
        feature_columns.append(
            SparseFeat(prefix + 'sparse_feature_' + str(i), dim, embedding_size, use_hash=hash_flag, dtype=tf.int32,
                       group_name=group_name))

    for i in range(dense_feature_num):
        def transform_fn(x): return (x - 0.0) / 1.0

        feature_columns.append(
            DenseFeat(
                prefix + 'dense_feature_' + str(i),
                1,
                dtype=tf.float32,
                transform_fn=transform_fn
            )
        )
    for i, mode in enumerate(sequence_feature):
        dim = np.random.randint(1, 10)
        maxlen = np.random.randint(1, 10)
        feature_columns.append(
            VarLenSparseFeat(SparseFeat(prefix + 'sequence_' + mode, vocabulary_size=dim, embedding_dim=embedding_size),
                             maxlen=maxlen, combiner=mode))

    for fc in feature_columns:
        if isinstance(fc, SparseFeat):
            model_input[fc.name] = np.random.randint(0, fc.vocabulary_size, sample_size)
        elif isinstance(fc, DenseFeat):
            model_input[fc.name] = np.random.random(sample_size)
        else:
            s_input, s_len_input = gen_sequence(
                fc.vocabulary_size, fc.maxlen, sample_size)
            model_input[fc.name] = s_input
            if include_length:
                fc.length_name = prefix + ""sequence_"" + str(i) + '_seq_length'
                model_input[prefix + ""sequence_"" + str(i) + '_seq_length'] = s_len_input

    if classification:
        y = np.random.randint(0, 2, sample_size)
    else:
        y = np.random.random(sample_size)

    return model_input, y, feature_columns


def layer_test(layer_cls, kwargs=None, input_shape=None, input_dtype=None,

               input_data=None, expected_output=None,

               expected_output_dtype=None, fixed_batch_size=False, supports_masking=False):
    # generate input data

    if kwargs is None:
        kwargs = {}
    if input_data is None:

        if not input_shape:
            raise AssertionError()

        if not input_dtype:
            input_dtype = K.floatx()

        input_data_shape = list(input_shape)

        for i, e in enumerate(input_data_shape):

            if e is None:
                input_data_shape[i] = np.random.randint(1, 4)
        input_mask = []
        if all(isinstance(e, tuple) for e in input_data_shape):
            input_data = []

            for e in input_data_shape:
                input_data.append(
                    (10 * np.random.random(e)).astype(input_dtype))
                if supports_masking:
                    a = np.full(e[:2], False)
                    a[:, :e[1] // 2] = True
                    input_mask.append(a)

        else:

            input_data = (10 * np.random.random(input_data_shape))

            input_data = input_data.astype(input_dtype)
            if supports_masking:
                a = np.full(input_data_shape[:2], False)
                a[:, :input_data_shape[1] // 2] = True

                print(a)
                print(a.shape)
                input_mask.append(a)

    else:

        if input_shape is None:
            input_shape = input_data.shape

        if input_dtype is None:
            input_dtype = input_data.dtype

    if expected_output_dtype is None:
        expected_output_dtype = input_dtype

    # instantiation

    layer = layer_cls(**kwargs)

    # test get_weights , set_weights at layer level

    weights = layer.get_weights()

    layer.set_weights(weights)

    try:
        expected_output_shape = layer.compute_output_shape(input_shape)
    except Exception:
        expected_output_shape = layer._compute_output_shape(input_shape)

    # test in functional API
    if isinstance(input_shape, list):
        if fixed_batch_size:

            x = [Input(batch_shape=e, dtype=input_dtype) for e in input_shape]
            if supports_masking:
                mask = [Input(batch_shape=e[0:2], dtype=bool)
                        for e in input_shape]

        else:

            x = [Input(shape=e[1:], dtype=input_dtype) for e in input_shape]
            if supports_masking:
                mask = [Input(shape=(e[1],), dtype=bool) for e in input_shape]

    else:
        if fixed_batch_size:

            x = Input(batch_shape=input_shape, dtype=input_dtype)
            if supports_masking:
                mask = Input(batch_shape=input_shape[0:2], dtype=bool)

        else:

            x = Input(shape=input_shape[1:], dtype=input_dtype)
            if supports_masking:
                mask = Input(shape=(input_shape[1],), dtype=bool)

    if supports_masking:

        y = layer(Masking()(x), mask=mask)
    else:
        y = layer(x)

    if not (K.dtype(y) == expected_output_dtype):
        raise AssertionError()

    # check with the functional API
    if supports_masking:
        model = Model([x, mask], y)

        actual_output = model.predict([input_data, input_mask[0]])
    else:
        model = Model(x, y)

        actual_output = model.predict(input_data)

    actual_output_shape = actual_output.shape
    for expected_dim, actual_dim in zip(expected_output_shape,

                                        actual_output_shape):

        if expected_dim is not None:

            if not (expected_dim == actual_dim):
                raise AssertionError(""expected_shape"", expected_output_shape, ""actual_shape"", actual_output_shape)

    if expected_output is not None:
        assert_allclose(actual_output, expected_output, rtol=1e-3)

    # test serialization, weight setting at model level

    model_config = model.get_config()

    recovered_model = model.__class__.from_config(model_config)

    if model.weights:
        weights = model.get_weights()

        recovered_model.set_weights(weights)

        _output = recovered_model.predict(input_data)

        assert_allclose(_output, actual_output, rtol=1e-3)

    # test training mode (e.g. useful when the layer has a

    # different behavior at training and testing time).

    if has_arg(layer.call, 'training'):
        model.compile('rmsprop', 'mse')

        model.train_on_batch(input_data, actual_output)

    # test instantiation from layer config

    layer_config = layer.get_config()

    layer_config['batch_input_shape'] = input_shape

    layer = layer.__class__.from_config(layer_config)

    # for further checks in the caller function

    return actual_output


def has_arg(fn, name, accept_all=False):
    """"""Checks if a callable accepts a given keyword argument.



    For Python 2, checks if there is an argument with the given name.



    For Python 3, checks if there is an argument with the given name, and

    also whether this argument can be called with a keyword (i.e. if it is

    not a positional-only argument).



    # Arguments

        fn: Callable to inspect.

        name: Check if `fn` can be called with `name` as a keyword argument.

        accept_all: What to return if there is no parameter called `name`

                    but the function accepts a `**kwargs` argument.



    # Returns

        bool, whether `fn` accepts a `name` keyword argument.

    """"""

    if sys.version_info < (3,):

        arg_spec = inspect.getargspec(fn)

        if accept_all and arg_spec.keywords is not None:
            return True

        return (name in arg_spec.args)

    elif sys.version_info < (3, 3):

        arg_spec = inspect.getfullargspec(fn)

        if accept_all and arg_spec.varkw is not None:
            return True

        return (name in arg_spec.args or

                name in arg_spec.kwonlyargs)

    else:

        signature = inspect.signature(fn)

        parameter = signature.parameters.get(name)

        if parameter is None:

            if accept_all:

                for param in signature.parameters.values():

                    if param.kind == inspect.Parameter.VAR_KEYWORD:
                        return True

            return False

        return (parameter.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD,

                                   inspect.Parameter.KEYWORD_ONLY))


def check_model(model, model_name, x, y, check_model_io=True):
    """"""
    compile model,train and evaluate it,then save/load weight and model file.
    :param model:
    :param model_name:
    :param x:
    :param y:
    :param check_model_io: test save/load model file or not
    :return:
    """"""
    model.compile('adam', 'binary_crossentropy',
                  metrics=['binary_crossentropy'])
    model.fit(x, y, batch_size=100, epochs=1, validation_split=0.5)

    print(model_name + "" test train valid pass!"")
    model.save_weights(model_name + '_weights.h5')
    model.load_weights(model_name + '_weights.h5')
    os.remove(model_name + '_weights.h5')
    print(model_name + "" test save load weight pass!"")
    if check_model_io:
        save_model(model, model_name + '.h5')
        model = load_model(model_name + '.h5', custom_objects)
        os.remove(model_name + '.h5')
        print(model_name + "" test save load model pass!"")

    print(model_name + "" test pass!"")


def get_test_data_estimator(sample_size=1000, embedding_size=4, sparse_feature_num=1, dense_feature_num=1,
                            classification=True):
    x = {}
    dnn_feature_columns = []
    linear_feature_columns = []
    voc_size = 4
    for i in range(sparse_feature_num):
        name = 's_' + str(i)
        x[name] = np.random.randint(0, voc_size, sample_size)
        dnn_feature_columns.append(
            tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_identity(name, voc_size),
                                               embedding_size))
        linear_feature_columns.append(tf.feature_column.categorical_column_with_identity(name, voc_size))

    for i in range(dense_feature_num):
        name = 'd_' + str(i)
        x[name] = np.random.random(sample_size)
        dnn_feature_columns.append(tf.feature_column.numeric_column(name))
        linear_feature_columns.append(tf.feature_column.numeric_column(name))

    if classification:
        y = np.random.randint(0, 2, sample_size)
    else:
        y = np.random.random(sample_size)
    if tf.__version__ >= ""2.0.0"":
        input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(x, y, shuffle=False)
    else:
        input_fn = tf.estimator.inputs.numpy_input_fn(x, y, shuffle=False)

    return linear_feature_columns, dnn_feature_columns, input_fn


def check_estimator(model, input_fn):
    model.train(input_fn)
    model.evaluate(input_fn)
"
DeepCTR,activations_test.py,"from deepctr.layers import activation

try:
    from tensorflow.python.keras.utils.generic_utils import CustomObjectScope
except ImportError:
    from tensorflow.python.keras.utils import CustomObjectScope
from tests.utils import layer_test


def test_dice():
    with CustomObjectScope({'Dice': activation.Dice}):
        layer_test(activation.Dice, kwargs={},
                   input_shape=(2, 3))
"
DeepCTR,sequence_test.py,"import pytest
from packaging import version

try:
    from tensorflow.python.keras.utils.generic_utils import CustomObjectScope
except ImportError:
    from tensorflow.python.keras.utils import CustomObjectScope
import tensorflow as tf
from deepctr.layers import sequence

from tests.utils import layer_test
try:
    tf.keras.backend.set_learning_phase(True)
except ImportError:
    from tensorflow.python.keras.backend import set_learning_phase
    set_learning_phase(True)
BATCH_SIZE = 4
EMBEDDING_SIZE = 8
SEQ_LENGTH = 10


@pytest.mark.parametrize(

    'weight_normalization',

    [True, False
     ]

)
def test_AttentionSequencePoolingLayer(weight_normalization):
    with CustomObjectScope({'AttentionSequencePoolingLayer': sequence.AttentionSequencePoolingLayer}):
        layer_test(sequence.AttentionSequencePoolingLayer, kwargs={'weight_normalization': weight_normalization},
                   input_shape=[(BATCH_SIZE, 1, EMBEDDING_SIZE), (BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE),
                                (BATCH_SIZE, 1)])


@pytest.mark.parametrize(

    'mode,supports_masking,input_shape',

    [('sum', False, [(BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE), (BATCH_SIZE, 1)]),
     ('mean', True, (BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE)), ('max', True, (BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE))
     ]

)
def test_SequencePoolingLayer(mode, supports_masking, input_shape):
    if version.parse(tf.__version__) >= version.parse('1.14.0') and mode != 'sum':  # todo check further version
        return
    with CustomObjectScope({'SequencePoolingLayer': sequence.SequencePoolingLayer}):
        layer_test(sequence.SequencePoolingLayer, kwargs={'mode': mode, 'supports_masking': supports_masking},
                   input_shape=input_shape, supports_masking=supports_masking)


# @pytest.mark.parametrize(
#
#     'supports_masking,input_shape',
#
#     [( False, [(BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE), (BATCH_SIZE, 1),(BATCH_SIZE, 1)]), ( True, [(BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE),(BATCH_SIZE, 1)])
#      ]
#
# )
# def test_WeightedSequenceLayer(supports_masking, input_shape):
#     # if version.parse(tf.__version__) >= version.parse('1.14.0') : #todo check further version
#     #    return
#     with CustomObjectScope({'WeightedSequenceLayer': sequence.WeightedSequenceLayer}):
#         layer_test(sequence.WeightedSequenceLayer, kwargs={'supports_masking': supports_masking},
#                    input_shape=input_shape, supports_masking=supports_masking)
#


@pytest.mark.parametrize(

    'merge_mode',
    ['concat', 'ave', 'fw', 'bw', 'sum', 'mul']

)
def test_BiLSTM(merge_mode):
    with CustomObjectScope({'BiLSTM': sequence.BiLSTM}):
        layer_test(sequence.BiLSTM, kwargs={'merge_mode': merge_mode, 'units': EMBEDDING_SIZE, 'dropout_rate': 0.0},
                   # todo 0.5
                   input_shape=(BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE))


@pytest.mark.parametrize(
    'attention_type',
    ['scaled_dot_product', 'cos', 'ln', 'additive']
)
def test_Transformer(attention_type):
    with CustomObjectScope({'Transformer': sequence.Transformer}):
        layer_test(sequence.Transformer,
                   kwargs={'att_embedding_size': 1, 'head_num': 8, 'use_layer_norm': True, 'supports_masking': False,
                           'attention_type': attention_type, 'dropout_rate': 0.5, 'output_type': 'sum'},
                   input_shape=[(BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE), (BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE),
                                (BATCH_SIZE, 1), (BATCH_SIZE, 1)])


def test_KMaxPooling():
    with CustomObjectScope({'KMaxPooling': sequence.KMaxPooling}):
        layer_test(sequence.KMaxPooling, kwargs={'k': 3, 'axis': 1},
                   input_shape=(BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE, 2))


@pytest.mark.parametrize(

    'pos_embedding_trainable,zero_pad',
    [(True, False), (False, True)
     ]
)
def test_PositionEncoding(pos_embedding_trainable, zero_pad):
    with CustomObjectScope({'PositionEncoding': sequence.PositionEncoding, ""tf"": tf}):
        layer_test(sequence.PositionEncoding,
                   kwargs={'pos_embedding_trainable': pos_embedding_trainable, 'zero_pad': zero_pad},
                   input_shape=(BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE))
"
DeepCTR,utils_test.py,"import numpy as np
import pytest
import tensorflow as tf

from deepctr.layers.utils import Hash, Linear
from tests.layers.interaction_test import BATCH_SIZE, EMBEDDING_SIZE
from tests.utils import layer_test

try:
    from tensorflow.python.keras.utils.generic_utils import CustomObjectScope
except ImportError:
    from tensorflow.python.keras.utils import CustomObjectScope


@pytest.mark.parametrize(
    'num_buckets,mask_zero,vocabulary_path,input_data,expected_output',
    [
        (3 + 1, False, None, ['lakemerson'], None),
        (3 + 1, True, None, ['lakemerson'], None),
        (
                3 + 1, False, ""./tests/layers/vocabulary_example.csv"", [['lake'], ['johnson'], ['lakemerson']],
                [[1], [3], [0]])
    ]
)
def test_Hash(num_buckets, mask_zero, vocabulary_path, input_data, expected_output):
    if not hasattr(tf, 'version') or tf.version.VERSION < '2.0.0':
        return

    with CustomObjectScope({'Hash': Hash}):
        layer_test(Hash,
                   kwargs={'num_buckets': num_buckets, 'mask_zero': mask_zero, 'vocabulary_path': vocabulary_path},
                   input_dtype=tf.string, input_data=np.array(input_data, dtype='str'),
                   expected_output_dtype=tf.int64, expected_output=expected_output)


def test_Linear():
    with CustomObjectScope({'Linear': Linear}):
        layer_test(Linear,
                   kwargs={'mode': 1, 'use_bias': True}, input_shape=(BATCH_SIZE, EMBEDDING_SIZE))
"
DeepCTR,normalization_test.py,"import pytest

try:
    from tensorflow.python.keras.utils.generic_utils import CustomObjectScope
except ImportError:
    from tensorflow.python.keras.utils import CustomObjectScope
from deepctr import layers
from tests.layers.interaction_test import BATCH_SIZE, FIELD_SIZE, EMBEDDING_SIZE
from tests.utils import layer_test


@pytest.mark.parametrize(
    'axis',
    [-1, -2
     ]
)
def test_LayerNormalization(axis):
    with CustomObjectScope({'LayerNormalization': layers.LayerNormalization}):
        layer_test(layers.LayerNormalization, kwargs={""axis"": axis, }, input_shape=(
            BATCH_SIZE, FIELD_SIZE, EMBEDDING_SIZE))
"
DeepCTR,__init__.py,
DeepCTR,interaction_test.py,"import pytest

try:
    from tensorflow.python.keras.utils.generic_utils import CustomObjectScope
except ImportError:
    from tensorflow.python.keras.utils import CustomObjectScope
from deepctr import layers

from tests.utils import layer_test

BATCH_SIZE = 5
FIELD_SIZE = 4
EMBEDDING_SIZE = 3
SEQ_LENGTH = 10


def test_FEFMLayer():
    with CustomObjectScope({'FEFMLayer': layers.FEFMLayer}):
        layer_test(layers.FEFMLayer, kwargs={'regularizer': 0.000001},
                   input_shape=(BATCH_SIZE, FIELD_SIZE, EMBEDDING_SIZE))


@pytest.mark.parametrize(
    'reg_strength',
    [0.000001]
)
def test_FwFM(reg_strength):
    with CustomObjectScope({'FwFMLayer': layers.FwFMLayer}):
        layer_test(layers.FwFMLayer, kwargs={'num_fields': FIELD_SIZE, 'regularizer': reg_strength},
                   input_shape=(BATCH_SIZE, FIELD_SIZE, EMBEDDING_SIZE))


@pytest.mark.parametrize(

    'layer_num',

    [0, 1]

)
def test_CrossNet(layer_num, ):
    with CustomObjectScope({'CrossNet': layers.CrossNet}):
        layer_test(layers.CrossNet, kwargs={
            'layer_num': layer_num, }, input_shape=(2, 3))


# def test_CrossNet_invalid():
#     with pytest.raises(ValueError):
#         with CustomObjectScope({'CrossNet': layers.CrossNet}):
#             layer_test(layers.CrossNet, kwargs={
#                 'layer_num': 1, 'l2_reg': 0}, input_shape=(2, 3, 4))


@pytest.mark.parametrize(
    'reduce_sum',
    [reduce_sum
     for reduce_sum in [True, False]
     ]
)
def test_InnerProductLayer(reduce_sum):
    with CustomObjectScope({'InnerProductLayer': layers.InnerProductLayer}):
        layer_test(layers.InnerProductLayer, kwargs={
            'reduce_sum': reduce_sum}, input_shape=[(BATCH_SIZE, 1, EMBEDDING_SIZE)] * FIELD_SIZE)


@pytest.mark.parametrize(
    'kernel_type',
    [kernel_type
     for kernel_type in ['mat', 'vec', 'num']
     ]
)
def test_OutterProductLayer(kernel_type):
    with CustomObjectScope({'OutterProductLayer': layers.OutterProductLayer}):
        layer_test(layers.OutterProductLayer, kwargs={
            'kernel_type': kernel_type}, input_shape=[(BATCH_SIZE, 1, EMBEDDING_SIZE)] * FIELD_SIZE)


def test_BiInteractionPooling():
    with CustomObjectScope({'BiInteractionPooling': layers.BiInteractionPooling}):
        layer_test(layers.BiInteractionPooling, kwargs={},
                   input_shape=(BATCH_SIZE, FIELD_SIZE, EMBEDDING_SIZE))


def test_FM():
    with CustomObjectScope({'FM': layers.FM}):
        layer_test(layers.FM, kwargs={}, input_shape=(
            BATCH_SIZE, FIELD_SIZE, EMBEDDING_SIZE))


def test_AFMLayer():
    with CustomObjectScope({'AFMLayer': layers.AFMLayer}):
        layer_test(layers.AFMLayer, kwargs={'dropout_rate': 0.5}, input_shape=[(
            BATCH_SIZE, 1, EMBEDDING_SIZE)] * FIELD_SIZE)


@pytest.mark.parametrize(
    'layer_size,split_half',
    [((10,), False), ((10, 8), True)
     ]
)
def test_CIN(layer_size, split_half):
    with CustomObjectScope({'CIN': layers.CIN}):
        layer_test(layers.CIN, kwargs={""layer_size"": layer_size, ""split_half"": split_half}, input_shape=(
            BATCH_SIZE, FIELD_SIZE, EMBEDDING_SIZE))


# @pytest.mark.parametrize(
#     'layer_size',
#     [(), (3, 10)
#      ]
# )
# def test_test_CIN_invalid(layer_size):
#     with pytest.raises(ValueError):
#         with CustomObjectScope({'CIN': layers.CIN}):
#             layer_test(layers.CIN, kwargs={""layer_size"": layer_size}, input_shape=(
#                 BATCH_SIZE, FIELD_SIZE, EMBEDDING_SIZE))


@pytest.mark.parametrize(
    'head_num,use_res',
    [(1, True), (2, False,)]
)
def test_InteractingLayer(head_num, use_res, ):
    with CustomObjectScope({'InteractingLayer': layers.InteractingLayer}):
        layer_test(layers.InteractingLayer, kwargs={""head_num"": head_num, ""use_res"":
            use_res, }, input_shape=(
            BATCH_SIZE, FIELD_SIZE, EMBEDDING_SIZE))


def test_FGCNNLayer():
    with CustomObjectScope({'FGCNNLayer': layers.FGCNNLayer}):
        layer_test(layers.FGCNNLayer, kwargs={'filters': (4, 6,), 'kernel_width': (7, 7,)}, input_shape=(
            BATCH_SIZE, FIELD_SIZE, EMBEDDING_SIZE))


# def test_SENETLayer():
#     with CustomObjectScope({'SENETLayer': layers.SENETLayer}):
#         layer_test(layers.SENETLayer, kwargs={'reduction_ratio':2}, input_shape=[(
#             BATCH_SIZE, 1, EMBEDDING_SIZE)]*FIELD_SIZE)


@pytest.mark.parametrize(
    'bilinear_type',
    ['all', 'each', 'interaction'
     ]
)
def test_BilinearInteraction(bilinear_type):
    with CustomObjectScope({'BilinearInteraction': layers.BilinearInteraction}):
        layer_test(layers.BilinearInteraction, kwargs={'bilinear_type': bilinear_type}, input_shape=[(
            BATCH_SIZE, 1, EMBEDDING_SIZE)] * FIELD_SIZE)
"
DeepCTR,core_test.py,"import pytest
import tensorflow as tf
from tensorflow.python.keras.layers import PReLU

try:
    from tensorflow.python.keras.utils.generic_utils import CustomObjectScope
except ImportError:
    from tensorflow.python.keras.utils import CustomObjectScope
from deepctr import layers
from deepctr.layers import Dice
from tests.layers.interaction_test import BATCH_SIZE, EMBEDDING_SIZE, SEQ_LENGTH
from tests.utils import layer_test


@pytest.mark.parametrize(
    'hidden_units,activation',
    [(hidden_units, activation)
     for hidden_units in [(), (10,)]
     for activation in ['sigmoid', Dice, PReLU]
     ]
)
def test_LocalActivationUnit(hidden_units, activation):
    if tf.__version__ >= '1.13.0' and activation != 'sigmoid':
        return

    with CustomObjectScope({'LocalActivationUnit': layers.LocalActivationUnit}):
        layer_test(layers.LocalActivationUnit,
                   kwargs={'hidden_units': hidden_units, 'activation': activation, 'dropout_rate': 0.5},
                   input_shape=[(BATCH_SIZE, 1, EMBEDDING_SIZE), (BATCH_SIZE, SEQ_LENGTH, EMBEDDING_SIZE)])


@pytest.mark.parametrize(
    'hidden_units,use_bn',
    [(hidden_units, use_bn)
     for hidden_units in [(), (10,)]
     for use_bn in [True, False]
     ]
)
def test_DNN(hidden_units, use_bn):
    with CustomObjectScope({'DNN': layers.DNN}):
        layer_test(layers.DNN, kwargs={'hidden_units': hidden_units, 'use_bn': use_bn, 'dropout_rate': 0.5},
                   input_shape=(
                       BATCH_SIZE, EMBEDDING_SIZE))


@pytest.mark.parametrize(
    'task,use_bias',
    [(task, use_bias)
     for task in ['binary', 'regression']
     for use_bias in [True, False]
     ]
)
def test_PredictionLayer(task, use_bias):
    with CustomObjectScope({'PredictionLayer': layers.PredictionLayer}):
        layer_test(layers.PredictionLayer, kwargs={'task': task, 'use_bias': use_bias
                                                   }, input_shape=(BATCH_SIZE, 1))


@pytest.mark.xfail(reason=""dim size must be 1 except for the batch size dim"")
def test_test_PredictionLayer_invalid():
    # with pytest.raises(ValueError):
    with CustomObjectScope({'PredictionLayer': layers.PredictionLayer}):
        layer_test(layers.PredictionLayer, kwargs={'use_bias': True,
                                                   }, input_shape=(BATCH_SIZE, 2, 1))
"
DeepCTR,FiBiNET_test.py,"import pytest

from deepctr.models import FiBiNET
from ..utils import check_model, SAMPLE_SIZE, get_test_data, get_test_data_estimator, check_estimator, TEST_Estimator


@pytest.mark.parametrize(
    'bilinear_type',
    [""each"",
     ""all"", ""interaction""]
)
def test_FiBiNET(bilinear_type):
    model_name = ""FiBiNET""
    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=2, dense_feature_num=2)

    model = FiBiNET(feature_columns, feature_columns, bilinear_type=bilinear_type, dnn_hidden_units=[4, ],
                    dnn_dropout=0.5, )
    check_model(model, model_name, x, y)


@pytest.mark.parametrize(
    'bilinear_type',
    [""interaction""]
)
def test_FiBiNETEstimator(bilinear_type):
    if not TEST_Estimator:
        return
    from deepctr.estimator import FiBiNETEstimator

    sample_size = SAMPLE_SIZE
    linear_feature_columns, dnn_feature_columns, input_fn = get_test_data_estimator(sample_size, sparse_feature_num=2,
                                                                                    dense_feature_num=2)

    model = FiBiNETEstimator(linear_feature_columns, dnn_feature_columns, bilinear_type=bilinear_type,
                             dnn_hidden_units=[4, ], dnn_dropout=0.5, )

    check_estimator(model, input_fn)


if __name__ == ""__main__"":
    pass
"
DeepCTR,DeepFM_test.py,"import pytest

from deepctr.models import DeepFM
from ..utils import check_model, get_test_data, SAMPLE_SIZE, get_test_data_estimator, check_estimator, TEST_Estimator


@pytest.mark.parametrize(
    'hidden_size,sparse_feature_num',
    [((2,), 1),  #
     ((3,), 2)
     ]  # (True, (32,), 3), (False, (32,), 1)
)
def test_DeepFM(hidden_size, sparse_feature_num):
    model_name = ""DeepFM""
    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=sparse_feature_num)

    model = DeepFM(feature_columns, feature_columns, dnn_hidden_units=hidden_size, dnn_dropout=0.5)

    check_model(model, model_name, x, y)


@pytest.mark.parametrize(
    'hidden_size,sparse_feature_num',
    [
        ((3,), 2)
    ]  # (True, (32,), 3), (False, (32,), 1)
)
def test_DeepFMEstimator(hidden_size, sparse_feature_num):
    if not TEST_Estimator:
        return
    from deepctr.estimator import DeepFMEstimator
    sample_size = SAMPLE_SIZE
    linear_feature_columns, dnn_feature_columns, input_fn = get_test_data_estimator(sample_size,
                                                                                    sparse_feature_num=sparse_feature_num,
                                                                                    dense_feature_num=sparse_feature_num,
                                                                                    classification=False)

    model = DeepFMEstimator(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=hidden_size, dnn_dropout=0.5,
                            task=""regression"")

    check_estimator(model, input_fn)


if __name__ == ""__main__"":
    pass
"
DeepCTR,NFM_test.py,"import pytest

from deepctr.models import NFM
from ..utils import check_model, get_test_data, SAMPLE_SIZE, get_test_data_estimator, check_estimator, TEST_Estimator


@pytest.mark.parametrize(
    'hidden_size,sparse_feature_num',
    [((8,), 1), ((8, 8,), 2)]
)
def test_NFM(hidden_size, sparse_feature_num):
    model_name = ""NFM""

    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=sparse_feature_num)

    model = NFM(feature_columns, feature_columns, dnn_hidden_units=[8, 8], dnn_dropout=0.5)
    check_model(model, model_name, x, y)


@pytest.mark.parametrize(
    'hidden_size,sparse_feature_num',
    [((8,), 1), ((8, 8,), 2)]
)
def test_FNNEstimator(hidden_size, sparse_feature_num):
    if not TEST_Estimator:
        return
    from deepctr.estimator import NFMEstimator

    sample_size = SAMPLE_SIZE
    linear_feature_columns, dnn_feature_columns, input_fn = get_test_data_estimator(sample_size,
                                                                                    sparse_feature_num=sparse_feature_num,
                                                                                    dense_feature_num=sparse_feature_num)

    model = NFMEstimator(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=[8, 8], dnn_dropout=0.5)

    check_estimator(model, input_fn)


if __name__ == ""__main__"":
    pass
"
DeepCTR,FNN_test.py,"import pytest
import tensorflow as tf

from deepctr.models import FNN
from ..utils import check_model, get_test_data, SAMPLE_SIZE, get_test_data_estimator, check_estimator, TEST_Estimator


@pytest.mark.parametrize(
    'sparse_feature_num,dense_feature_num',
    [(1, 1), (3, 3)
     ]
)
def test_FNN(sparse_feature_num, dense_feature_num):
    if tf.__version__ >= ""2.0.0"":
        return
    model_name = ""FNN""

    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=dense_feature_num)

    model = FNN(feature_columns, feature_columns, dnn_hidden_units=[8, 8], dnn_dropout=0.5)
    check_model(model, model_name, x, y)


# @pytest.mark.parametrize(
#     'sparse_feature_num,dense_feature_num',
#     [(0, 1), (1, 0)
#      ]
# )
# def test_FNN_without_seq(sparse_feature_num, dense_feature_num):
#     model_name = ""FNN""
#
#     sample_size = SAMPLE_SIZE
#     x, y, feature_columns = get_test_data(sample_size, sparse_feature_num, dense_feature_num, sequence_feature=())
#
#     model = FNN(feature_columns,feature_columns, dnn_hidden_units=[32, 32], dnn_dropout=0.5)
#     check_model(model, model_name, x, y)

@pytest.mark.parametrize(
    'sparse_feature_num,dense_feature_num',
    [(2, 2),
     ]
)
def test_FNNEstimator(sparse_feature_num, dense_feature_num):
    if not TEST_Estimator:
        return
    from deepctr.estimator import FNNEstimator

    sample_size = SAMPLE_SIZE
    linear_feature_columns, dnn_feature_columns, input_fn = get_test_data_estimator(sample_size,
                                                                                    sparse_feature_num=sparse_feature_num,
                                                                                    dense_feature_num=dense_feature_num)

    model = FNNEstimator(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=[8, 8], dnn_dropout=0.5)

    check_estimator(model, input_fn)


if __name__ == ""__main__"":
    pass
"
DeepCTR,EDCN_test.py,"import pytest

from deepctr.models import EDCN
from ..utils import check_model, get_test_data, SAMPLE_SIZE


@pytest.mark.parametrize(
    'bridge_type, cross_num, cross_parameterization, sparse_feature_num',
    [
        ('pointwise_addition', 2, 'vector', 3),
        ('hadamard_product', 2, 'vector', 4),
        ('concatenation', 1, 'vector', 5),
        ('attention_pooling', 2, 'matrix', 6),
    ]
)
def test_EDCN(bridge_type, cross_num, cross_parameterization, sparse_feature_num):
    model_name = ""EDCN""

    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=0)

    model = EDCN(feature_columns, feature_columns, cross_num, cross_parameterization, bridge_type)
    check_model(model, model_name, x, y)


if __name__ == ""__main__"":
    pass
"
DeepCTR,DIFM_test.py,"import pytest

from deepctr.models import DIFM
from ..utils import check_model, get_test_data, SAMPLE_SIZE


@pytest.mark.parametrize(
    'att_head_num,dnn_hidden_units,sparse_feature_num',
    [(1, (4,), 2), (2, (4, 4,), 2), (1, (4,), 1)]
)
def test_DIFM(att_head_num, dnn_hidden_units, sparse_feature_num):
    model_name = ""DIFM""
    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=sparse_feature_num)

    model = DIFM(feature_columns, feature_columns, dnn_hidden_units=dnn_hidden_units, dnn_dropout=0.5)
    check_model(model, model_name, x, y)


if __name__ == ""__main__"":
    pass
"
DeepCTR,DSIN_test.py,"import numpy as np
import pytest

from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, get_feature_names
from deepctr.models.sequence.dsin import DSIN
from ..utils import check_model


def get_xy_fd(hash_flag=False):
    feature_columns = [SparseFeat('user', 3, use_hash=hash_flag),
                       SparseFeat('gender', 2, use_hash=hash_flag),
                       SparseFeat('item', 3 + 1, use_hash=hash_flag),
                       SparseFeat('item_gender', 2 + 1, use_hash=hash_flag),
                       DenseFeat('score', 1)]
    feature_columns += [
        VarLenSparseFeat(SparseFeat('sess_0_item', 3 + 1, embedding_dim=4, use_hash=hash_flag, embedding_name='item'),
                         maxlen=4), VarLenSparseFeat(
            SparseFeat('sess_0_item_gender', 2 + 1, embedding_dim=4, use_hash=hash_flag, embedding_name='item_gender'),
            maxlen=4)]
    feature_columns += [
        VarLenSparseFeat(SparseFeat('sess_1_item', 3 + 1, embedding_dim=4, use_hash=hash_flag, embedding_name='item'),
                         maxlen=4), VarLenSparseFeat(
            SparseFeat('sess_1_item_gender', 2 + 1, embedding_dim=4, use_hash=hash_flag, embedding_name='item_gender'),
            maxlen=4)]

    behavior_feature_list = [""item"", ""item_gender""]
    uid = np.array([0, 1, 2])
    ugender = np.array([0, 1, 0])
    iid = np.array([1, 2, 3])  # 0 is mask value
    igender = np.array([1, 2, 1])  # 0 is mask value
    score = np.array([0.1, 0.2, 0.3])

    sess1_iid = np.array([[1, 2, 3, 0], [1, 2, 3, 0], [0, 0, 0, 0]])
    sess1_igender = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [0, 0, 0, 0]])

    sess2_iid = np.array([[1, 2, 3, 0], [0, 0, 0, 0], [0, 0, 0, 0]])
    sess2_igender = np.array([[1, 1, 2, 0], [0, 0, 0, 0], [0, 0, 0, 0]])

    sess_number = np.array([2, 1, 0])

    feature_dict = {'user': uid, 'gender': ugender, 'item': iid, 'item_gender': igender,
                    'sess_0_item': sess1_iid, 'sess_0_item_gender': sess1_igender, 'score': score,
                    'sess_1_item': sess2_iid, 'sess_1_item_gender': sess2_igender, }

    x = {name: feature_dict[name] for name in get_feature_names(feature_columns)}
    x[""sess_length""] = sess_number

    y = np.array([1, 0, 1])
    return x, y, feature_columns, behavior_feature_list


@pytest.mark.parametrize(
    'bias_encoding',
    [True, False]
)
def test_DSIN(bias_encoding):
    model_name = ""DSIN""

    x, y, feature_columns, behavior_feature_list = get_xy_fd(True)

    model = DSIN(feature_columns, behavior_feature_list, sess_max_count=2, bias_encoding=bias_encoding,
                 dnn_hidden_units=[4, 4], dnn_dropout=0.5, )
    check_model(model, model_name, x, y)


if __name__ == ""__main__"":
    pass
"
DeepCTR,FwFM_test.py,"import pytest

from deepctr.models import FwFM
from ..utils import check_model, get_test_data, SAMPLE_SIZE, get_test_data_estimator, check_estimator, TEST_Estimator


@pytest.mark.parametrize(
    'hidden_size,sparse_feature_num',
    [((2,), 1),
     ((), 1),
     ]
)
def test_FwFM(hidden_size, sparse_feature_num):
    model_name = ""FwFM""
    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=sparse_feature_num)
    model = FwFM(feature_columns, feature_columns, dnn_hidden_units=hidden_size, dnn_dropout=0.5)

    check_model(model, model_name, x, y)


@pytest.mark.parametrize(
    'hidden_size,sparse_feature_num',
    [((2,), 2),
     ]
)
def test_FwFMEstimator(hidden_size, sparse_feature_num):
    if not TEST_Estimator:
        return
    from deepctr.estimator import FwFMEstimator

    sample_size = SAMPLE_SIZE
    linear_feature_columns, dnn_feature_columns, input_fn = get_test_data_estimator(sample_size,
                                                                                    sparse_feature_num=sparse_feature_num,
                                                                                    dense_feature_num=sparse_feature_num)

    model = FwFMEstimator(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=hidden_size, dnn_dropout=0.5)

    check_estimator(model, input_fn)


if __name__ == ""__main__"":
    pass
"
DeepCTR,AutoInt_test.py,"import pytest
import tensorflow as tf
from packaging import version

from deepctr.models import AutoInt
from ..utils import check_model, get_test_data, SAMPLE_SIZE, get_test_data_estimator, check_estimator, \
    TEST_Estimator


@pytest.mark.parametrize(
    'att_layer_num,dnn_hidden_units,sparse_feature_num',
    [(1, (), 1), (1, (4,), 1)]  # (0, (4,), 2), (2, (4, 4,), 2)
)
def test_AutoInt(att_layer_num, dnn_hidden_units, sparse_feature_num):
    if version.parse(tf.__version__) >= version.parse(""1.14.0"") and len(dnn_hidden_units) == 0:  # todo check version
        return
    model_name = ""AutoInt""
    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=sparse_feature_num)

    model = AutoInt(feature_columns, feature_columns, att_layer_num=att_layer_num,
                    dnn_hidden_units=dnn_hidden_units, dnn_dropout=0.5, )
    check_model(model, model_name, x, y)


@pytest.mark.parametrize(
    'att_layer_num,dnn_hidden_units,sparse_feature_num',
    [(1, (4,), 1)]  # (0, (4,), 2), (2, (4, 4,), 2)
)
def test_AutoIntEstimator(att_layer_num, dnn_hidden_units, sparse_feature_num):
    if not TEST_Estimator:
        return
    from deepctr.estimator import AutoIntEstimator
    sample_size = SAMPLE_SIZE
    linear_feature_columns, dnn_feature_columns, input_fn = get_test_data_estimator(sample_size,
                                                                                    sparse_feature_num=sparse_feature_num,
                                                                                    dense_feature_num=sparse_feature_num)

    model = AutoIntEstimator(linear_feature_columns, dnn_feature_columns, att_layer_num=att_layer_num,
                             dnn_hidden_units=dnn_hidden_units, dnn_dropout=0.5, )
    check_estimator(model, input_fn)


if __name__ == ""__main__"":
    pass
"
DeepCTR,__init__.py,
DeepCTR,FLEN_test.py,"import pytest

from deepctr.models import FLEN
from ..utils import check_model, get_test_data, SAMPLE_SIZE


@pytest.mark.parametrize(
    'hidden_size,sparse_feature_num',
    [
        ((3,), 6)
    ]  # (True, (32,), 3), (False, (32,), 1)
)
def test_FLEN(hidden_size, sparse_feature_num):
    model_name = ""FLEN""
    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, embedding_size=2, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=sparse_feature_num, use_group=True)

    model = FLEN(feature_columns, feature_columns, dnn_hidden_units=hidden_size, dnn_dropout=0.5)

    check_model(model, model_name, x, y)


if __name__ == ""__main__"":
    pass
"
DeepCTR,IFM_test.py,"import pytest

from deepctr.models import IFM
from ..utils import check_model, get_test_data, SAMPLE_SIZE


@pytest.mark.parametrize(
    'hidden_size,sparse_feature_num',
    [((2,), 1),
     ((3,), 2)
     ]
)
def test_IFM(hidden_size, sparse_feature_num):
    model_name = ""IFM""
    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=sparse_feature_num)

    model = IFM(feature_columns, feature_columns, dnn_hidden_units=hidden_size, dnn_dropout=0.5)
    check_model(model, model_name, x, y)


if __name__ == ""__main__"":
    pass
"
DeepCTR,BST_test.py,"from deepctr.models import BST
from ..utils import check_model
from .DIN_test import get_xy_fd


def test_BST():
    model_name = ""BST""

    x, y, feature_columns, behavior_feature_list = get_xy_fd(hash_flag=True)

    model = BST(dnn_feature_columns=feature_columns,
                history_feature_list=behavior_feature_list,
                att_head_num=4)

    check_model(model, model_name, x, y,
                check_model_io=True)


if __name__ == ""__main__"":
    pass
"
DeepCTR,AFM_test.py,"import pytest

from deepctr.models import AFM
from ..utils import check_model, check_estimator, get_test_data, get_test_data_estimator, SAMPLE_SIZE, \
    TEST_Estimator


@pytest.mark.parametrize(
    'use_attention,sparse_feature_num,dense_feature_num',
    [(True, 3, 0),
     ]
)
def test_AFM(use_attention, sparse_feature_num, dense_feature_num):
    model_name = ""AFM""
    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=dense_feature_num)

    model = AFM(feature_columns, feature_columns, use_attention=use_attention, afm_dropout=0.5)

    check_model(model, model_name, x, y)


@pytest.mark.parametrize(
    'use_attention,sparse_feature_num,dense_feature_num',
    [(True, 3, 0),
     ]
)
def test_AFMEstimator(use_attention, sparse_feature_num, dense_feature_num):
    if not TEST_Estimator:
        return
    from deepctr.estimator import AFMEstimator

    sample_size = SAMPLE_SIZE

    linear_feature_columns, dnn_feature_columns, input_fn = get_test_data_estimator(sample_size,
                                                                                    sparse_feature_num=sparse_feature_num,
                                                                                    dense_feature_num=dense_feature_num)
    model = AFMEstimator(linear_feature_columns, dnn_feature_columns, use_attention=use_attention, afm_dropout=0.5)
    check_estimator(model, input_fn)


if __name__ == ""__main__"":
    pass
"
DeepCTR,MLR_test.py,"import pytest

from deepctr.models import MLR
from ..utils import check_model, SAMPLE_SIZE, get_test_data


@pytest.mark.parametrize(

    'region_sparse,region_dense,base_sparse,base_dense,bias_sparse,bias_dense',

    [(0, 2, 0, 2, 0, 1), (0, 2, 0, 1, 0, 2), (0, 2, 0, 0, 1, 0),
     #     (0, 1, 1, 2, 1, 1,), (0, 1, 1, 1, 1, 2), (0, 1, 1, 0, 2, 0),
     #     (1, 0, 2, 2, 2, 1), (2, 0, 2, 1, 2, 2), (2, 0, 2, 0, 0, 0)
     ]

)
def test_MLRs(region_sparse, region_dense, base_sparse, base_dense, bias_sparse, bias_dense):
    model_name = ""MLRs""
    _, y, region_feature_columns = get_test_data(SAMPLE_SIZE, sparse_feature_num=region_sparse,
                                                 dense_feature_num=region_dense, prefix='region')
    base_x, y, base_feature_columns = get_test_data(SAMPLE_SIZE, sparse_feature_num=region_sparse,
                                                    dense_feature_num=region_dense, prefix='base')
    bias_x, y, bias_feature_columns = get_test_data(SAMPLE_SIZE, sparse_feature_num=region_sparse,
                                                    dense_feature_num=region_dense, prefix='bias')

    model = MLR(region_feature_columns, base_feature_columns, bias_feature_columns=bias_feature_columns)
    model.compile('adam', 'binary_crossentropy',
                  metrics=['binary_crossentropy'])
    print(model_name + "" test pass!"")


def test_MLR():
    model_name = ""MLR""
    region_x, y, region_feature_columns = get_test_data(SAMPLE_SIZE, sparse_feature_num=3, dense_feature_num=3,
                                                        prefix='region')
    base_x, y, base_feature_columns = get_test_data(SAMPLE_SIZE, sparse_feature_num=3, dense_feature_num=3,
                                                    prefix='base')
    bias_x, y, bias_feature_columns = get_test_data(SAMPLE_SIZE, sparse_feature_num=3, dense_feature_num=3,
                                                    prefix='bias')

    model = MLR(region_feature_columns)
    model.compile('adam', 'binary_crossentropy',
                  metrics=['binary_crossentropy'])

    check_model(model, model_name, region_x, y)
    print(model_name + "" test pass!"")


if __name__ == ""__main__"":
    pass
"
DeepCTR,MTL_test.py,"import pytest
import tensorflow as tf

from deepctr.models.multitask import SharedBottom, ESMM, MMOE, PLE
from ..utils_mtl import get_mtl_test_data, check_mtl_model


def test_SharedBottom():
    if tf.__version__ == ""1.15.0"":  # slow in tf 1.15
        return
    model_name = ""SharedBottom""
    x, y_list, dnn_feature_columns = get_mtl_test_data()

    model = SharedBottom(dnn_feature_columns, bottom_dnn_hidden_units=(8,), tower_dnn_hidden_units=(8,),
                         task_types=['binary', 'binary'], task_names=['label_income', 'label_marital'])
    check_mtl_model(model, model_name, x, y_list, task_types=['binary', 'binary'])


def test_ESMM():
    if tf.__version__ == ""1.15.0"":  # slow in tf 1.15
        return
    model_name = ""ESMM""
    x, y_list, dnn_feature_columns = get_mtl_test_data()

    model = ESMM(dnn_feature_columns, tower_dnn_hidden_units=(8,), task_types=['binary', 'binary'],
                 task_names=['label_marital', 'label_income'])
    check_mtl_model(model, model_name, x, y_list, task_types=['binary', 'binary'])


def test_MMOE():
    if tf.__version__ == ""1.15.0"":  # slow in tf 1.15
        return
    model_name = ""MMOE""
    x, y_list, dnn_feature_columns = get_mtl_test_data()

    model = MMOE(dnn_feature_columns, num_experts=3, expert_dnn_hidden_units=(8,),
                 tower_dnn_hidden_units=(8,),
                 gate_dnn_hidden_units=(), task_types=['binary', 'binary'],
                 task_names=['income', 'marital'])
    check_mtl_model(model, model_name, x, y_list, task_types=['binary', 'binary'])


@pytest.mark.parametrize(
    'num_levels,gate_dnn_hidden_units',
    [(2, ()),
     (1, (4,))]
)
def test_PLE(num_levels, gate_dnn_hidden_units):
    if tf.__version__ == ""1.15.0"":  # slow in tf 1.15
        return
    model_name = ""PLE""
    x, y_list, dnn_feature_columns = get_mtl_test_data()

    model = PLE(dnn_feature_columns, num_levels=num_levels, expert_dnn_hidden_units=(8,), tower_dnn_hidden_units=(8,),
                gate_dnn_hidden_units=gate_dnn_hidden_units,
                task_types=['binary', 'binary'], task_names=['income', 'marital'])
    check_mtl_model(model, model_name, x, y_list, task_types=['binary', 'binary'])


if __name__ == ""__main__"":
    pass
"
DeepCTR,ONN_test.py,"import pytest
import tensorflow as tf
from packaging import version

from deepctr.models import ONN
from ..utils import check_model, get_test_data, SAMPLE_SIZE


@pytest.mark.parametrize(
    'sparse_feature_num',
    [2]
)
def test_ONN(sparse_feature_num):
    if version.parse(tf.__version__) >= version.parse('1.15.0'):
        return
    model_name = ""ONN""

    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=sparse_feature_num,
                                          sequence_feature=('sum', 'mean', 'max',), hash_flag=True)

    model = ONN(feature_columns, feature_columns,
                dnn_hidden_units=[4, 4], dnn_dropout=0.5)
    check_model(model, model_name, x, y)


if __name__ == ""__main__"":
    pass
"
DeepCTR,DCNMix_test.py,"import pytest

from deepctr.models import DCNMix
from ..utils import check_model, get_test_data, SAMPLE_SIZE


@pytest.mark.parametrize(
    'cross_num,hidden_size,sparse_feature_num',
    [(0, (8,), 2), (1, (), 1), (1, (8,), 3)
     ]
)
def test_DCNMix(cross_num, hidden_size, sparse_feature_num):
    model_name = ""DCNMix""

    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=sparse_feature_num)

    model = DCNMix(feature_columns, feature_columns, cross_num=cross_num, dnn_hidden_units=hidden_size, dnn_dropout=0.5)
    check_model(model, model_name, x, y)


if __name__ == ""__main__"":
    pass
"
DeepCTR,CCPM_test.py,"import pytest
import tensorflow as tf

from deepctr.models import CCPM
from ..utils import check_model, get_test_data, SAMPLE_SIZE, check_estimator, get_test_data_estimator, TEST_Estimator


@pytest.mark.parametrize(
    'sparse_feature_num,dense_feature_num',
    [(3, 0)
     ]
)
def test_CCPM(sparse_feature_num, dense_feature_num):
    if tf.__version__ >= ""2.0.0"":  # todo
        return
    model_name = ""CCPM""

    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=dense_feature_num)

    model = CCPM(feature_columns, feature_columns, conv_kernel_width=(3, 2), conv_filters=(
        2, 1), dnn_hidden_units=[32, ], dnn_dropout=0.5)
    check_model(model, model_name, x, y)


@pytest.mark.parametrize(
    'sparse_feature_num,dense_feature_num',
    [(2, 0),
     ]
)
def test_CCPM_without_seq(sparse_feature_num, dense_feature_num):
    if tf.__version__ >= ""2.0.0"":
        return
    model_name = ""CCPM""

    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=dense_feature_num, sequence_feature=())

    model = CCPM(feature_columns, feature_columns, conv_kernel_width=(3, 2), conv_filters=(
        2, 1), dnn_hidden_units=[32, ], dnn_dropout=0.5)
    check_model(model, model_name, x, y)


@pytest.mark.parametrize(
    'sparse_feature_num,dense_feature_num',
    [(2, 0),
     ]
)
def test_CCPMEstimator_without_seq(sparse_feature_num, dense_feature_num):
    if not TEST_Estimator:
        return
    from deepctr.estimator import CCPMEstimator

    sample_size = SAMPLE_SIZE
    linear_feature_columns, dnn_feature_columns, input_fn = get_test_data_estimator(sample_size,
                                                                                    sparse_feature_num=sparse_feature_num,
                                                                                    dense_feature_num=sparse_feature_num)

    model = CCPMEstimator(linear_feature_columns, dnn_feature_columns, conv_kernel_width=(3, 2), conv_filters=(
        2, 1), dnn_hidden_units=[32, ], dnn_dropout=0.5)
    check_estimator(model, input_fn)


if __name__ == ""__main__"":
    pass
"
DeepCTR,DIEN_test.py,"import numpy as np
import pytest
import tensorflow as tf
from packaging import version

from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, get_feature_names
from deepctr.models import DIEN
from ..utils import check_model


def get_xy_fd(use_neg=False, hash_flag=False):
    feature_columns = [SparseFeat('user', 3, hash_flag),
                       SparseFeat('gender', 2, hash_flag),
                       SparseFeat('item', 3 + 1, hash_flag),
                       SparseFeat('item_gender', 2 + 1, hash_flag),
                       DenseFeat('score', 1)]

    feature_columns += [
        VarLenSparseFeat(SparseFeat('hist_item', vocabulary_size=3 + 1, embedding_dim=8, embedding_name='item'),
                         maxlen=4, length_name=""seq_length""),
        VarLenSparseFeat(SparseFeat('hist_item_gender', 2 + 1, embedding_dim=4, embedding_name='item_gender'),
                         maxlen=4, length_name=""seq_length"")]

    behavior_feature_list = [""item"", ""item_gender""]
    uid = np.array([0, 1, 2])
    ugender = np.array([0, 1, 0])
    iid = np.array([1, 2, 3])  # 0 is mask value
    igender = np.array([1, 2, 1])  # 0 is mask value
    score = np.array([0.1, 0.2, 0.3])

    hist_iid = np.array([[1, 2, 3, 0], [1, 2, 3, 0], [1, 2, 0, 0]])
    hist_igender = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [2, 1, 0, 0]])

    behavior_length = np.array([3, 3, 2])

    feature_dict = {'user': uid, 'gender': ugender, 'item': iid, 'item_gender': igender,
                    'hist_item': hist_iid, 'hist_item_gender': hist_igender,
                    'score': score,""seq_length"":behavior_length}

    if use_neg:
        feature_dict['neg_hist_item'] = np.array([[1, 2, 3, 0], [1, 2, 3, 0], [1, 2, 0, 0]])
        feature_dict['neg_hist_item_gender'] = np.array([[1, 1, 2, 0], [2, 1, 1, 0], [2, 1, 0, 0]])
        feature_columns += [
            VarLenSparseFeat(SparseFeat('neg_hist_item', vocabulary_size=3 + 1, embedding_dim=8, embedding_name='item'),
                             maxlen=4, length_name=""seq_length""),
            VarLenSparseFeat(SparseFeat('neg_hist_item_gender', 2 + 1, embedding_dim=4, embedding_name='item_gender'),
                             maxlen=4, length_name=""seq_length"")]

    feature_names = get_feature_names(feature_columns)
    x = {name: feature_dict[name] for name in feature_names}
    y = np.array([1, 0, 1])
    return x, y, feature_columns, behavior_feature_list


# @pytest.mark.xfail(reason=""There is a bug when save model use Dice"")
# @pytest.mark.skip(reason=""misunderstood the API"")

@pytest.mark.parametrize(
    'gru_type',
    ['GRU', 'AIGRU', 'AGRU'  # ,'AUGRU',
     ]
)
def test_DIEN(gru_type):
    if version.parse(tf.__version__) >= version.parse('2.0.0'):
        tf.compat.v1.disable_eager_execution()  # todo
        return
    model_name = ""DIEN_"" + gru_type

    x, y, feature_columns, behavior_feature_list = get_xy_fd(hash_flag=True)

    model = DIEN(feature_columns, behavior_feature_list,
                 dnn_hidden_units=[4, 4, 4], dnn_dropout=0.5, gru_type=gru_type)

    check_model(model, model_name, x, y,
                check_model_io=(gru_type == ""GRU""))  # TODO:fix bugs when load model in other type


def test_DIEN_neg():
    model_name = ""DIEN_neg""
    if version.parse(tf.__version__) >= version.parse(""1.14.0""):
        return

    x, y, feature_dim_dict, behavior_feature_list = get_xy_fd(use_neg=True)

    model = DIEN(feature_dim_dict, behavior_feature_list,
                 dnn_hidden_units=[4, 4, 4], dnn_dropout=0.5, gru_type=""AUGRU"", use_negsampling=True)
    check_model(model, model_name, x, y)


if __name__ == ""__main__"":
    pass
"
DeepCTR,DeepFEFM_test.py,"import pytest
import tensorflow as tf

from deepctr.models import DeepFEFM
from ..utils import check_model, get_test_data, SAMPLE_SIZE, get_test_data_estimator, check_estimator, TEST_Estimator


@pytest.mark.parametrize(
    'hidden_size,sparse_feature_num,use_fefm,use_linear,use_fefm_embed_in_dnn',
    [((2,), 1, True, True, True),
     ((2,), 1, True, True, False),
     ((2,), 1, True, False, True),
     ((2,), 1, False, True, True),
     ((2,), 1, True, False, False),
     ((2,), 1, False, True, False),
     ((2,), 1, False, False, True),
     ((2,), 1, False, False, False),
     ((), 1, True, True, True)
     ]
)
def test_DeepFEFM(hidden_size, sparse_feature_num, use_fefm, use_linear, use_fefm_embed_in_dnn):
    if tf.__version__ == ""1.15.0"" or tf.__version__ == ""1.4.0"":  # slow in tf 1.15
        return
    model_name = ""DeepFEFM""
    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=sparse_feature_num)
    model = DeepFEFM(feature_columns, feature_columns, dnn_hidden_units=hidden_size, dnn_dropout=0.5,
                     use_linear=use_linear, use_fefm=use_fefm, use_fefm_embed_in_dnn=use_fefm_embed_in_dnn)

    check_model(model, model_name, x, y)


@pytest.mark.parametrize(
    'hidden_size,sparse_feature_num',
    [((2,), 2),
     ((), 2),
     ]
)
def test_DeepFEFMEstimator(hidden_size, sparse_feature_num):
    import tensorflow as tf
    if not TEST_Estimator or tf.__version__ == ""1.4.0"":
        return
    from deepctr.estimator import DeepFEFMEstimator

    sample_size = SAMPLE_SIZE
    linear_feature_columns, dnn_feature_columns, input_fn = get_test_data_estimator(sample_size,
                                                                                    sparse_feature_num=sparse_feature_num,
                                                                                    dense_feature_num=sparse_feature_num)

    model = DeepFEFMEstimator(linear_feature_columns, dnn_feature_columns,
                              dnn_hidden_units=hidden_size, dnn_dropout=0.5)

    check_estimator(model, input_fn)


if __name__ == ""__main__"":
    pass
"
DeepCTR,WDL_test.py,"import pytest
import tensorflow as tf
from packaging import version

from deepctr.models import WDL
from ..utils import check_model, check_estimator, SAMPLE_SIZE, get_test_data, get_test_data_estimator, TEST_Estimator


@pytest.mark.parametrize(
    'sparse_feature_num,dense_feature_num',
    [(2, 0), (0, 2)  # ,(2, 2)
     ]
)
def test_WDL(sparse_feature_num, dense_feature_num):
    if version.parse(tf.__version__) >= version.parse('2.0.0'):
        return
    model_name = ""WDL""
    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=dense_feature_num, hash_flag=True)

    model = WDL(feature_columns, feature_columns,
                dnn_hidden_units=[4, 4], dnn_dropout=0.5)
    check_model(model, model_name, x, y)


@pytest.mark.parametrize(
    'sparse_feature_num,dense_feature_num',
    [(2, 1),  # (0, 2)#,(2, 2)
     ]
)
def test_WDLEstimator(sparse_feature_num, dense_feature_num):
    if not TEST_Estimator:
        return
    from deepctr.estimator import WDLEstimator

    sample_size = SAMPLE_SIZE

    linear_feature_columns, dnn_feature_columns, input_fn = get_test_data_estimator(sample_size, sparse_feature_num,
                                                                                    dense_feature_num)
    model = WDLEstimator(linear_feature_columns, dnn_feature_columns,
                         dnn_hidden_units=[4, 4], dnn_dropout=0.5)
    check_estimator(model, input_fn)


if __name__ == ""__main__"":
    pass
"
DeepCTR,PNN_test.py,"import pytest

from deepctr.models import PNN
from ..utils import check_model, get_test_data, SAMPLE_SIZE, get_test_data_estimator, check_estimator, TEST_Estimator


@pytest.mark.parametrize(
    'use_inner, use_outter,sparse_feature_num',
    [(True, True, 3), (False, False, 1)
     ]
)
def test_PNN(use_inner, use_outter, sparse_feature_num):
    model_name = ""PNN""
    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=sparse_feature_num)
    model = PNN(feature_columns, dnn_hidden_units=[4, 4], dnn_dropout=0.5, use_inner=use_inner, use_outter=use_outter)
    check_model(model, model_name, x, y)


@pytest.mark.parametrize(
    'use_inner, use_outter,sparse_feature_num',
    [(True, True, 2)
     ]
)
def test_PNNEstimator(use_inner, use_outter, sparse_feature_num):
    if not TEST_Estimator:
        return
    from deepctr.estimator import PNNEstimator

    sample_size = SAMPLE_SIZE
    _, dnn_feature_columns, input_fn = get_test_data_estimator(sample_size,
                                                               sparse_feature_num=sparse_feature_num,
                                                               dense_feature_num=sparse_feature_num)

    model = PNNEstimator(dnn_feature_columns, dnn_hidden_units=[4, 4], dnn_dropout=0.5, use_inner=use_inner,
                         use_outter=use_outter)

    check_estimator(model, input_fn)


if __name__ == ""__main__"":
    pass
"
DeepCTR,xDeepFM_test.py,"import pytest

from deepctr.models import xDeepFM
from ..utils import check_model, get_test_data, SAMPLE_SIZE, get_test_data_estimator, check_estimator, TEST_Estimator


@pytest.mark.parametrize(
    'dnn_hidden_units,cin_layer_size,cin_split_half,cin_activation,sparse_feature_num,dense_feature_dim',
    [  # ((), (), True, 'linear', 1, 2),
        ((8,), (), True, 'linear', 1, 1),
        ((), (8,), True, 'linear', 2, 2),
        ((8,), (8,), False, 'relu', 1, 0)
    ]
)
def test_xDeepFM(dnn_hidden_units, cin_layer_size, cin_split_half, cin_activation, sparse_feature_num,
                 dense_feature_dim):
    model_name = ""xDeepFM""

    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=sparse_feature_num)

    model = xDeepFM(feature_columns, feature_columns, dnn_hidden_units=dnn_hidden_units, cin_layer_size=cin_layer_size,
                    cin_split_half=cin_split_half, cin_activation=cin_activation, dnn_dropout=0.5)
    check_model(model, model_name, x, y)


# @pytest.mark.parametrize(
#     'hidden_size,cin_layer_size,',
#     [((8,), (3, 8)),
#      ]
# )
# def test_xDeepFM_invalid(hidden_size, cin_layer_size):
#     feature_dim_dict = {'sparse': {'sparse_1': 2, 'sparse_2': 5,
#                                    'sparse_3': 10}, 'dense': ['dense_1', 'dense_2', 'dense_3']}
#     with pytest.raises(ValueError):
#         _ = xDeepFM(feature_dim_dict, None, dnn_hidden_units=hidden_size, cin_layer_size=cin_layer_size)
@pytest.mark.parametrize(
    'dnn_hidden_units,cin_layer_size,cin_split_half,cin_activation,sparse_feature_num,dense_feature_dim',
    [  # ((), (), True, 'linear', 1, 2),
        ((8,), (8,), False, 'relu', 2, 1)
    ]
)
def test_xDeepFMEstimator(dnn_hidden_units, cin_layer_size, cin_split_half, cin_activation, sparse_feature_num,
                          dense_feature_dim):
    import tensorflow as tf
    if not TEST_Estimator or tf.__version__ == ""1.4.0"":
        return
    from deepctr.estimator import xDeepFMEstimator

    sample_size = SAMPLE_SIZE
    linear_feature_columns, dnn_feature_columns, input_fn = get_test_data_estimator(sample_size,
                                                                                    sparse_feature_num=sparse_feature_num,
                                                                                    dense_feature_num=sparse_feature_num)

    model = xDeepFMEstimator(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=dnn_hidden_units,
                             cin_layer_size=cin_layer_size,
                             cin_split_half=cin_split_half, cin_activation=cin_activation, dnn_dropout=0.5)

    check_estimator(model, input_fn)


if __name__ == ""__main__"":
    pass"
DeepCTR,DCN_test.py,"import pytest

from deepctr.models import DCN
from ..utils import check_model, get_test_data, SAMPLE_SIZE, get_test_data_estimator, check_estimator, TEST_Estimator


@pytest.mark.parametrize(
    'cross_num,hidden_size,sparse_feature_num,cross_parameterization',
    [(0, (8,), 2, 'vector'), (1, (), 1, 'vector'), (1, (8,), 3, 'vector'),
     (0, (8,), 2, 'matrix'), (1, (), 1, 'matrix'), (1, (8,), 3, 'matrix'),
     ]
)
def test_DCN(cross_num, hidden_size, sparse_feature_num, cross_parameterization):
    model_name = ""DCN""

    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=sparse_feature_num)

    model = DCN(feature_columns, feature_columns, cross_num=cross_num, cross_parameterization=cross_parameterization,
                dnn_hidden_units=hidden_size, dnn_dropout=0.5)
    check_model(model, model_name, x, y)


def test_DCN_2():
    model_name = ""DCN""

    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=3,
                                          dense_feature_num=2)

    model = DCN([], feature_columns, cross_num=1, dnn_hidden_units=(8,), dnn_dropout=0.5)
    check_model(model, model_name, x, y)


@pytest.mark.parametrize(
    'cross_num,hidden_size,sparse_feature_num',
    [(1, (8,), 3)
     ]
)
def test_DCNEstimator(cross_num, hidden_size, sparse_feature_num):
    if not TEST_Estimator:
        return
    from deepctr.estimator import DCNEstimator

    sample_size = SAMPLE_SIZE
    linear_feature_columns, dnn_feature_columns, input_fn = get_test_data_estimator(sample_size,
                                                                                    sparse_feature_num=sparse_feature_num,
                                                                                    dense_feature_num=sparse_feature_num)

    model = DCNEstimator(linear_feature_columns, dnn_feature_columns, cross_num=cross_num, dnn_hidden_units=hidden_size,
                         dnn_dropout=0.5)
    check_estimator(model, input_fn)


# def test_DCN_invalid(embedding_size=8, cross_num=0, hidden_size=()):
#     feature_dim_dict = {'sparse': [SparseFeat('sparse_1', 2), SparseFeat('sparse_2', 5), SparseFeat('sparse_3', 10)],
#                         'dense': [SparseFeat('dense_1', 1), SparseFeat('dense_1', 1), SparseFeat('dense_1', 1)]}
#     with pytest.raises(ValueError):
#         _ = DCN(None, embedding_size=embedding_size, cross_num=cross_num, dnn_hidden_units=hidden_size, dnn_dropout=0.5)


if __name__ == ""__main__"":
    pass
"
DeepCTR,FGCNN_test.py,"import pytest

from deepctr.models import FGCNN
from tests.utils import check_model, get_test_data, SAMPLE_SIZE


@pytest.mark.parametrize(
    'sparse_feature_num,dense_feature_num',
    [(1, 1), (3, 3)
     ]
)
def test_FGCNN(sparse_feature_num, dense_feature_num):
    model_name = ""FGCNN""

    sample_size = SAMPLE_SIZE
    x, y, feature_columns = get_test_data(sample_size, embedding_size=8, sparse_feature_num=sparse_feature_num,
                                          dense_feature_num=dense_feature_num)

    model = FGCNN(feature_columns, feature_columns, conv_kernel_width=(3, 2), conv_filters=(2, 1), new_maps=(
        2, 2), pooling_width=(2, 2), dnn_hidden_units=(32,), dnn_dropout=0.5, )
    # TODO: add model_io check
    check_model(model, model_name, x, y, check_model_io=False)


# @pytest.mark.parametrize(
#     'sparse_feature_num,dense_feature_num',
#     [(2, 1),
#      ]
# )
# def test_FGCNN_without_seq(sparse_feature_num, dense_feature_num):
#     model_name = ""FGCNN_noseq""
#
#     sample_size = SAMPLE_SIZE
#     x, y, feature_columns = get_test_data(sample_size, sparse_feature_num=sparse_feature_num,
#                                           dense_feature_num=dense_feature_num, sequence_feature=())
#
#     model = FGCNN(feature_columns, feature_columns, conv_kernel_width=(), conv_filters=(
#     ), new_maps=(), pooling_width=(), dnn_hidden_units=(32,), dnn_dropout=0.5, )
#     # TODO: add model_io check
#     check_model(model, model_name, x, y, check_model_io=False)


if __name__ == ""__main__"":
    pass
"
DeepCTR,DIN_test.py,"import numpy as np
import tensorflow as tf
from packaging import version

from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, get_feature_names
from deepctr.models.sequence.din import DIN
from ..utils import check_model


def get_xy_fd(hash_flag=False):
    feature_columns = [SparseFeat('user', 3, embedding_dim=10), SparseFeat(
        'gender', 2, embedding_dim=4), SparseFeat('item_id', 3 + 1, embedding_dim=8),
                       SparseFeat('cate_id', 2 + 1, embedding_dim=4), DenseFeat('pay_score', 1)]
    feature_columns += [
        VarLenSparseFeat(SparseFeat('hist_item_id', vocabulary_size=3 + 1, embedding_dim=8, embedding_name='item_id'),
                         maxlen=4, length_name=""seq_length""),
        VarLenSparseFeat(SparseFeat('hist_cate_id', 2 + 1, embedding_dim=4, embedding_name='cate_id'), maxlen=4,
                         length_name=""seq_length"")]
    # Notice: History behavior sequence feature name must start with ""hist_"".
    behavior_feature_list = [""item_id"", ""cate_id""]
    uid = np.array([0, 1, 2])
    ugender = np.array([0, 1, 0])
    iid = np.array([1, 2, 3])  # 0 is mask value
    cate_id = np.array([1, 2, 2])  # 0 is mask value
    pay_score = np.array([0.1, 0.2, 0.3])

    hist_iid = np.array([[1, 2, 3, 0], [3, 2, 1, 0], [1, 2, 0, 0]])
    hist_cate_id = np.array([[1, 2, 2, 0], [2, 2, 1, 0], [1, 2, 0, 0]])
    seq_length = np.array([3, 3, 2])  # the actual length of the behavior sequence

    feature_dict = {'user': uid, 'gender': ugender, 'item_id': iid, 'cate_id': cate_id,
                    'hist_item_id': hist_iid, 'hist_cate_id': hist_cate_id,
                    'pay_score': pay_score, 'seq_length': seq_length}
    x = {name: feature_dict[name] for name in get_feature_names(feature_columns)}
    y = np.array([1, 0, 1])
    return x, y, feature_columns, behavior_feature_list


# @pytest.mark.xfail(reason=""There is a bug when save model use Dice"")
# @pytest.mark.skip(reason=""misunderstood the API"")


def test_DIN():
    model_name = ""DIN""

    x, y, feature_columns, behavior_feature_list = get_xy_fd(True)
    cur_version = version.parse(tf.__version__)
    if cur_version >= version.parse('2.8.0'):  # todo:
        att_activation = 'sigmoid'
    else:
        att_activation = 'dice'

    model = DIN(feature_columns, behavior_feature_list, dnn_hidden_units=[4, 4, 4], att_activation=att_activation,
                dnn_dropout=0.5)
    # todo test dice

    check_model(model, model_name, x, y)


if __name__ == ""__main__"":
    pass
"
DeepCTR,conf.py,"# -*- coding: utf-8 -*-
#
# Configuration file for the Sphinx documentation builder.
#
# This file does only contain a selection of the most common options. For a
# full list see the documentation:
# http://www.sphinx-doc.org/en/master/config

# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os
import sys
sys.path.insert(0, os.path.abspath('../../'))


# -- Project information -----------------------------------------------------

project = 'DeepCTR'
copyright = '2017-present, Weichen Shen'
author = 'Weichen Shen'

# The short X.Y version
version = ''
# The full version, including alpha/beta/rc tags
release = '0.9.3'


# -- General configuration ---------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#
# needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.mathjax',
    'sphinx.ext.ifconfig',
    'sphinx.ext.viewcode',
    'sphinx.ext.githubpages',
]

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix(es) of source filenames.
# You can specify multiple suffix as a list of string:
#
source_suffix = ['.rst', '.md']
#source_suffix = '.rst'

# The master toctree document.
master_doc = 'index'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#
# This is also used if you do content translation via gettext catalogs.
# Usually you set ""language"" from the command line for these cases.
language = None

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path .
exclude_patterns = []

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'


# -- Options for HTML output -------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
html_theme = 'alabaster'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#
# html_theme_options = {}

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named ""default.css"" will overwrite the builtin ""default.css"".
html_static_path = ['_static']

# Custom sidebar templates, must be a dictionary that maps document names
# to template names.
#
# The default sidebars (for documents that don't match any pattern) are
# defined by theme itself.  Builtin themes are using these templates by
# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',
# 'searchbox.html']``.
#
# html_sidebars = {}


# -- Options for HTMLHelp output ---------------------------------------------

# Output file base name for HTML help builder.
htmlhelp_basename = 'DeepCTRdoc'


# -- Options for LaTeX output ------------------------------------------------

latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    #
    # 'papersize': 'letterpaper',

    # The font size ('10pt', '11pt' or '12pt').
    #
    # 'pointsize': '10pt',

    # Additional stuff for the LaTeX preamble.
    #
    # 'preamble': '',

    # Latex figure (float) alignment
    #
    # 'figure_align': 'htbp',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
latex_documents = [
    (master_doc, 'DeepCTR.tex', 'DeepCTR Documentation',
     'Weichen Shen', 'manual'),
]


# -- Options for manual page output ------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    (master_doc, 'deepctr', 'DeepCTR Documentation',
     [author], 1)
]


# -- Options for Texinfo output ----------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (master_doc, 'DeepCTR', 'DeepCTR Documentation',
     author, 'DeepCTR', 'One line description of project.',
     'Miscellaneous'),
]


# -- Extension configuration -------------------------------------------------
todo_include_todos = False
html_theme = 'sphinx_rtd_theme'

source_parsers = {
    '.md': 'recommonmark.parser.CommonMarkParser',
}
"
DeepCTR,run_multivalue_movielens.py,"import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from tensorflow.python.keras.preprocessing.sequence import pad_sequences

from deepctr.feature_column import SparseFeat, VarLenSparseFeat,get_feature_names
from deepctr.models import DeepFM


def split(x):
    key_ans = x.split('|')
    for key in key_ans:
        if key not in key2index:
            # Notice : input value 0 is a special ""padding"",so we do not use 0 to encode valid feature for sequence input
            key2index[key] = len(key2index) + 1
    return list(map(lambda x: key2index[x], key_ans))


if __name__ == ""__main__"":
    data = pd.read_csv(""./movielens_sample.txt"")
    sparse_features = [""movie_id"", ""user_id"",
                       ""gender"", ""age"", ""occupation"", ""zip"", ]
    target = ['rating']

    # 1.Label Encoding for sparse features,and process sequence features
    for feat in sparse_features:
        lbe = LabelEncoder()
        data[feat] = lbe.fit_transform(data[feat])
    # preprocess the sequence feature

    key2index = {}
    genres_list = list(map(split, data['genres'].values))
    genres_length = np.array(list(map(len, genres_list)))
    max_len = max(genres_length)
    # Notice : padding=`post`
    genres_list = pad_sequences(genres_list, maxlen=max_len, padding='post', )

    # 2.count #unique features for each sparse field and generate feature config for sequence feature

    fixlen_feature_columns = [SparseFeat(feat, data[feat].max() + 1, embedding_dim=4)
                              for feat in sparse_features]

    use_weighted_sequence = False
    if use_weighted_sequence:
        varlen_feature_columns = [VarLenSparseFeat(SparseFeat('genres', vocabulary_size=len(
            key2index) + 1, embedding_dim=4), maxlen=max_len, combiner='mean',
                                                   weight_name='genres_weight')]  # Notice : value 0 is for padding for sequence input feature
    else:
        varlen_feature_columns = [VarLenSparseFeat(SparseFeat('genres', vocabulary_size=len(
            key2index) + 1, embedding_dim=4), maxlen=max_len, combiner='mean',
                                                   weight_name=None)]  # Notice : value 0 is for padding for sequence input feature

    linear_feature_columns = fixlen_feature_columns + varlen_feature_columns
    dnn_feature_columns = fixlen_feature_columns + varlen_feature_columns

    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)

    # 3.generate input data for model
    model_input = {name: data[name] for name in sparse_features}  #
    model_input[""genres""] = genres_list
    model_input[""genres_weight""] = np.random.randn(data.shape[0], max_len, 1)

    # 4.Define Model,compile and train
    model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression')

    model.compile(""adam"", ""mse"", metrics=['mse'], )
    history = model.fit(model_input, data[target].values,
                        batch_size=256, epochs=10, verbose=2, validation_split=0.2, )
"
DeepCTR,run_classification_criteo.py,"import pandas as pd
from sklearn.metrics import log_loss, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler

from deepctr.models import DeepFM
from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names

if __name__ == ""__main__"":
    data = pd.read_csv('./criteo_sample.txt')

    sparse_features = ['C' + str(i) for i in range(1, 27)]
    dense_features = ['I' + str(i) for i in range(1, 14)]

    data[sparse_features] = data[sparse_features].fillna('-1', )
    data[dense_features] = data[dense_features].fillna(0, )
    target = ['label']

    # 1.Label Encoding for sparse features,and do simple Transformation for dense features
    for feat in sparse_features:
        lbe = LabelEncoder()
        data[feat] = lbe.fit_transform(data[feat])
    mms = MinMaxScaler(feature_range=(0, 1))
    data[dense_features] = mms.fit_transform(data[dense_features])

    # 2.count #unique features for each sparse field,and record dense feature field name

    fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].max() + 1, embedding_dim=4)
                              for i, feat in enumerate(sparse_features)] + [DenseFeat(feat, 1, )
                                                                            for feat in dense_features]

    dnn_feature_columns = fixlen_feature_columns
    linear_feature_columns = fixlen_feature_columns

    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)

    # 3.generate input data for model

    train, test = train_test_split(data, test_size=0.2, random_state=2020)
    train_model_input = {name: train[name] for name in feature_names}
    test_model_input = {name: test[name] for name in feature_names}

    # 4.Define Model,train,predict and evaluate
    model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary')
    model.compile(""adam"", ""binary_crossentropy"",
                  metrics=['binary_crossentropy'], )

    history = model.fit(train_model_input, train[target].values,
                        batch_size=256, epochs=10, verbose=2, validation_split=0.2, )
    pred_ans = model.predict(test_model_input, batch_size=256)
    print(""test LogLoss"", round(log_loss(test[target].values, pred_ans), 4))
    print(""test AUC"", round(roc_auc_score(test[target].values, pred_ans), 4))
"
DeepCTR,run_regression_movielens.py,"import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from deepctr.models import DeepFM
from deepctr.feature_column import SparseFeat,get_feature_names

if __name__ == ""__main__"":

    data = pd.read_csv(""./movielens_sample.txt"")
    sparse_features = [""movie_id"", ""user_id"",
                       ""gender"", ""age"", ""occupation"", ""zip""]
    target = ['rating']

    # 1.Label Encoding for sparse features,and do simple Transformation for dense features
    for feat in sparse_features:
        lbe = LabelEncoder()
        data[feat] = lbe.fit_transform(data[feat])
    # 2.count #unique features for each sparse field
    fixlen_feature_columns = [SparseFeat(feat, data[feat].max() + 1,embedding_dim=4)
                              for feat in sparse_features]
    linear_feature_columns = fixlen_feature_columns
    dnn_feature_columns = fixlen_feature_columns
    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)

    # 3.generate input data for model
    train, test = train_test_split(data, test_size=0.2, random_state=2020)
    train_model_input = {name:train[name].values for name in feature_names}
    test_model_input = {name:test[name].values for name in feature_names}

    # 4.Define Model,train,predict and evaluate
    model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression')
    model.compile(""adam"", ""mse"", metrics=['mse'], )

    history = model.fit(train_model_input, train[target].values,
                        batch_size=256, epochs=10, verbose=2, validation_split=0.2, )
    pred_ans = model.predict(test_model_input, batch_size=256)
    print(""test MSE"", round(mean_squared_error(
        test[target].values, pred_ans), 4))
"
DeepCTR,run_estimator_pandas_classification.py,"import pandas as pd
import tensorflow as tf
from sklearn.metrics import log_loss, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler

from deepctr.estimator import DeepFMEstimator
from deepctr.estimator.inputs import input_fn_pandas

if __name__ == ""__main__"":
    data = pd.read_csv('./criteo_sample.txt')

    sparse_features = ['C' + str(i) for i in range(1, 27)]
    dense_features = ['I' + str(i) for i in range(1, 14)]

    data[sparse_features] = data[sparse_features].fillna('-1', )
    data[dense_features] = data[dense_features].fillna(0, )
    target = ['label']

    # 1.Label Encoding for sparse features,and do simple Transformation for dense features
    for feat in sparse_features:
        lbe = LabelEncoder()
        data[feat] = lbe.fit_transform(data[feat])
    mms = MinMaxScaler(feature_range=(0, 1))
    data[dense_features] = mms.fit_transform(data[dense_features])

    # 2.count #unique features for each sparse field,and record dense feature field name

    dnn_feature_columns = []
    linear_feature_columns = []

    for i, feat in enumerate(sparse_features):
        dnn_feature_columns.append(tf.feature_column.embedding_column(
            tf.feature_column.categorical_column_with_identity(feat, data[feat].max() + 1), 4))
        linear_feature_columns.append(tf.feature_column.categorical_column_with_identity(feat, data[feat].max() + 1))
    for feat in dense_features:
        dnn_feature_columns.append(tf.feature_column.numeric_column(feat))
        linear_feature_columns.append(tf.feature_column.numeric_column(feat))

    # 3.generate input data for model

    train, test = train_test_split(data, test_size=0.2, random_state=2021)

    # Not setting default value for continuous feature. filled with mean.

    train_model_input = input_fn_pandas(train, sparse_features + dense_features, 'label', shuffle=True)
    test_model_input = input_fn_pandas(test, sparse_features + dense_features, None, shuffle=False)

    # 4.Define Model,train,predict and evaluate
    model = DeepFMEstimator(linear_feature_columns, dnn_feature_columns, task='binary',
                            config=tf.estimator.RunConfig(tf_random_seed=2021))

    model.train(train_model_input)
    pred_ans_iter = model.predict(test_model_input)
    pred_ans = list(map(lambda x: x['pred'], pred_ans_iter))
    #
    print(""test LogLoss"", round(log_loss(test[target].values, pred_ans), 4))
    print(""test AUC"", round(roc_auc_score(test[target].values, pred_ans), 4))
"
DeepCTR,run_mtl.py,"import pandas as pd
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler

from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names
from deepctr.models import MMOE

if __name__ == ""__main__"":
    column_names = ['age', 'class_worker', 'det_ind_code', 'det_occ_code', 'education', 'wage_per_hour', 'hs_college',
                    'marital_stat', 'major_ind_code', 'major_occ_code', 'race', 'hisp_origin', 'sex', 'union_member',
                    'unemp_reason', 'full_or_part_emp', 'capital_gains', 'capital_losses', 'stock_dividends',
                    'tax_filer_stat', 'region_prev_res', 'state_prev_res', 'det_hh_fam_stat', 'det_hh_summ',
                    'instance_weight', 'mig_chg_msa', 'mig_chg_reg', 'mig_move_reg', 'mig_same', 'mig_prev_sunbelt',
                    'num_emp', 'fam_under_18', 'country_father', 'country_mother', 'country_self', 'citizenship',
                    'own_or_self', 'vet_question', 'vet_benefits', 'weeks_worked', 'year', 'income_50k']
    data = pd.read_csv('./census-income.sample', header=None, names=column_names)

    data['label_income'] = data['income_50k'].map({' - 50000.': 0, ' 50000+.': 1})
    data['label_marital'] = data['marital_stat'].apply(lambda x: 1 if x == ' Never married' else 0)
    data.drop(labels=['income_50k', 'marital_stat'], axis=1, inplace=True)

    columns = data.columns.values.tolist()
    sparse_features = ['class_worker', 'det_ind_code', 'det_occ_code', 'education', 'hs_college', 'major_ind_code',
                       'major_occ_code', 'race', 'hisp_origin', 'sex', 'union_member', 'unemp_reason',
                       'full_or_part_emp', 'tax_filer_stat', 'region_prev_res', 'state_prev_res', 'det_hh_fam_stat',
                       'det_hh_summ', 'mig_chg_msa', 'mig_chg_reg', 'mig_move_reg', 'mig_same', 'mig_prev_sunbelt',
                       'fam_under_18', 'country_father', 'country_mother', 'country_self', 'citizenship',
                       'vet_question']
    dense_features = [col for col in columns if
                      col not in sparse_features and col not in ['label_income', 'label_marital']]

    data[sparse_features] = data[sparse_features].fillna('-1', )
    data[dense_features] = data[dense_features].fillna(0, )
    mms = MinMaxScaler(feature_range=(0, 1))
    data[dense_features] = mms.fit_transform(data[dense_features])

    for feat in sparse_features:
        lbe = LabelEncoder()
        data[feat] = lbe.fit_transform(data[feat])

    fixlen_feature_columns = [SparseFeat(feat, data[feat].max() + 1, embedding_dim=4) for feat in sparse_features] \
                             + [DenseFeat(feat, 1, ) for feat in dense_features]

    dnn_feature_columns = fixlen_feature_columns
    linear_feature_columns = fixlen_feature_columns

    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)

    # 3.generate input data for model

    train, test = train_test_split(data, test_size=0.2, random_state=2020)
    train_model_input = {name: train[name] for name in feature_names}
    test_model_input = {name: test[name] for name in feature_names}

    # 4.Define Model,train,predict and evaluate
    model = MMOE(dnn_feature_columns, tower_dnn_hidden_units=[], task_types=['binary', 'binary'],
                 task_names=['label_income', 'label_marital'])
    model.compile(""adam"", loss=[""binary_crossentropy"", ""binary_crossentropy""],
                  metrics=['binary_crossentropy'], )

    history = model.fit(train_model_input, [train['label_income'].values, train['label_marital'].values],
                        batch_size=256, epochs=10, verbose=2, validation_split=0.2)
    pred_ans = model.predict(test_model_input, batch_size=256)

    print(""test income AUC"", round(roc_auc_score(test['label_income'], pred_ans[0]), 4))
    print(""test marital AUC"", round(roc_auc_score(test['label_marital'], pred_ans[1]), 4))
"
DeepCTR,run_flen.py,"import pandas as pd
from sklearn.metrics import log_loss, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from deepctr.feature_column import SparseFeat,get_feature_names
from deepctr.models import FLEN

if __name__ == ""__main__"":
    data = pd.read_csv('./avazu_sample.txt')
    data['day'] = data['hour'].apply(lambda x: str(x)[4:6])
    data['hour'] = data['hour'].apply(lambda x: str(x)[6:])

    sparse_features = ['hour', 'C1', 'banner_pos', 'site_id', 'site_domain',
                       'site_category', 'app_id', 'app_domain', 'app_category', 'device_id',
                       'device_model', 'device_type', 'device_conn_type',  # 'device_ip',
                       'C14',
                       'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', ]

    data[sparse_features] = data[sparse_features].fillna('-1', )
    target = ['click']

    # 1.Label Encoding for sparse features,and do simple Transformation for dense features
    for feat in sparse_features:
        lbe = LabelEncoder()
        data[feat] = lbe.fit_transform(data[feat])

    # 2.count #unique features for each sparse field,and record dense feature field name

    field_info = dict(C14='user', C15='user', C16='user', C17='user',
                      C18='user', C19='user', C20='user', C21='user', C1='user',
                      banner_pos='context', site_id='context',
                      site_domain='context', site_category='context',
                      app_id='item', app_domain='item', app_category='item',
                      device_model='user', device_type='user',
                      device_conn_type='context', hour='context',
                      device_id='user'
                      )

    fixlen_feature_columns = [
        SparseFeat(name, vocabulary_size=data[name].max() + 1, embedding_dim=16, use_hash=False, dtype='int32',
                   group_name=field_info[name]) for name in sparse_features]

    dnn_feature_columns = fixlen_feature_columns
    linear_feature_columns = fixlen_feature_columns

    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)

    # 3.generate input data for model

    train, test = train_test_split(data, test_size=0.2, random_state=2020)
    train_model_input = {name: train[name] for name in feature_names}
    test_model_input = {name: test[name] for name in feature_names}

    # 4.Define Model,train,predict and evaluate
    model = FLEN(linear_feature_columns, dnn_feature_columns, task='binary')
    model.compile(""adam"", ""binary_crossentropy"",
                  metrics=['binary_crossentropy'], )

    history = model.fit(train_model_input, train[target].values,
                        batch_size=256, epochs=10, verbose=2, validation_split=0.2, )
    pred_ans = model.predict(test_model_input, batch_size=256)
    print(""test LogLoss"", round(log_loss(test[target].values, pred_ans), 4))
    print(""test AUC"", round(roc_auc_score(test[target].values, pred_ans), 4))
"
DeepCTR,run_multivalue_movielens_vocab_hash.py,"from deepctr.models import DeepFM
from deepctr.feature_column import SparseFeat, VarLenSparseFeat, get_feature_names
import numpy as np
import pandas as pd
from tensorflow.python.keras.preprocessing.sequence import pad_sequences

try:
    import tensorflow.compat.v1 as tf
except ImportError as e:
    import tensorflow as tf

if __name__ == ""__main__"":
    data = pd.read_csv(""./movielens_sample.txt"")
    sparse_features = [""movie_id"", ""user_id"",
                       ""gender"", ""age"", ""occupation"", ""zip"", ]

    data[sparse_features] = data[sparse_features].astype(str)
    target = ['rating']

    # 1.Use hashing encoding on the fly for sparse features,and process sequence features

    genres_list = list(map(lambda x: x.split('|'), data['genres'].values))
    genres_length = np.array(list(map(len, genres_list)))
    max_len = max(genres_length)

    # Notice : padding=`post`
    genres_list = pad_sequences(genres_list, maxlen=max_len, padding='post', dtype=object, value=0).astype(str)
    # 2.set hashing space for each sparse field and generate feature config for sequence feature

    fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique() * 5, embedding_dim=4, use_hash=True,
                                         vocabulary_path='./movielens_age_vocabulary.csv' if feat == 'age' else None,
                                         dtype='string')
                              for feat in sparse_features]
    varlen_feature_columns = [
        VarLenSparseFeat(SparseFeat('genres', vocabulary_size=100, embedding_dim=4,
                                    use_hash=True, dtype=""string""),
                         maxlen=max_len, combiner='mean',
                         )]  # Notice : value 0 is for padding for sequence input feature
    linear_feature_columns = fixlen_feature_columns + varlen_feature_columns
    dnn_feature_columns = fixlen_feature_columns + varlen_feature_columns
    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)

    # 3.generate input data for model
    model_input = {name: data[name] for name in feature_names}
    model_input['genres'] = genres_list

    # 4.Define Model,compile and train
    model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression')
    model.compile(""adam"", ""mse"", metrics=['mse'], )
    if not hasattr(tf, 'version') or tf.version.VERSION < '2.0.0':
        with tf.Session() as sess:
            sess.run(tf.tables_initializer())
            history = model.fit(model_input, data[target].values,
                                batch_size=256, epochs=10, verbose=2, validation_split=0.2, )
    else:
        history = model.fit(model_input, data[target].values,
                            batch_size=256, epochs=10, verbose=2, validation_split=0.2, )
"
DeepCTR,run_classification_criteo_hash.py,"import pandas as pd
from sklearn.metrics import log_loss, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

from deepctr.models import DeepFM
from deepctr.feature_column import SparseFeat, DenseFeat,get_feature_names

if __name__ == ""__main__"":
    data = pd.read_csv('./criteo_sample.txt')

    sparse_features = ['C' + str(i) for i in range(1, 27)]
    dense_features = ['I' + str(i) for i in range(1, 14)]

    data[sparse_features] = data[sparse_features].fillna('-1', )
    data[dense_features] = data[dense_features].fillna(0, )
    target = ['label']

    # 1.do simple Transformation for dense features
    mms = MinMaxScaler(feature_range=(0, 1))
    data[dense_features] = mms.fit_transform(data[dense_features])

    # 2.set hashing space for each sparse field,and record dense feature field name

    fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=1000,embedding_dim=4, use_hash=True, dtype='string')  # since the input is string
                              for feat in sparse_features] + [DenseFeat(feat, 1, )
                          for feat in dense_features]

    linear_feature_columns = fixlen_feature_columns
    dnn_feature_columns = fixlen_feature_columns
    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns, )

    # 3.generate input data for model

    train, test = train_test_split(data, test_size=0.2, random_state=2020)

    train_model_input = {name:train[name] for name in feature_names}
    test_model_input = {name:test[name] for name in feature_names}


    # 4.Define Model,train,predict and evaluate
    model = DeepFM(linear_feature_columns,dnn_feature_columns, task='binary')
    model.compile(""adam"", ""binary_crossentropy"",
                  metrics=['binary_crossentropy'], )

    history = model.fit(train_model_input, train[target].values,
                        batch_size=256, epochs=10, verbose=2, validation_split=0.2, )
    pred_ans = model.predict(test_model_input, batch_size=256)
    print(""test LogLoss"", round(log_loss(test[target].values, pred_ans), 4))
    print(""test AUC"", round(roc_auc_score(test[target].values, pred_ans), 4))
"
DeepCTR,run_estimator_tfrecord_classification.py,"import tensorflow as tf

from tensorflow.python.ops.parsing_ops import FixedLenFeature
from deepctr.estimator import DeepFMEstimator
from deepctr.estimator.inputs import input_fn_tfrecord

if __name__ == ""__main__"":

    # 1.generate feature_column for linear part and dnn part

    sparse_features = ['C' + str(i) for i in range(1, 27)]
    dense_features = ['I' + str(i) for i in range(1, 14)]

    dnn_feature_columns = []
    linear_feature_columns = []

    for i, feat in enumerate(sparse_features):
        dnn_feature_columns.append(tf.feature_column.embedding_column(
            tf.feature_column.categorical_column_with_identity(feat, 1000), 4))
        linear_feature_columns.append(tf.feature_column.categorical_column_with_identity(feat, 1000))
    for feat in dense_features:
        dnn_feature_columns.append(tf.feature_column.numeric_column(feat))
        linear_feature_columns.append(tf.feature_column.numeric_column(feat))

    # 2.generate input data for model

    feature_description = {k: FixedLenFeature(dtype=tf.int64, shape=1) for k in sparse_features}
    feature_description.update(
        {k: FixedLenFeature(dtype=tf.float32, shape=1) for k in dense_features})
    feature_description['label'] = FixedLenFeature(dtype=tf.float32, shape=1)

    train_model_input = input_fn_tfrecord('./criteo_sample.tr.tfrecords', feature_description, 'label', batch_size=256,
                                          num_epochs=1, shuffle_factor=10)
    test_model_input = input_fn_tfrecord('./criteo_sample.te.tfrecords', feature_description, 'label',
                                         batch_size=2 ** 14, num_epochs=1, shuffle_factor=0)

    # 3.Define Model,train,predict and evaluate
    model = DeepFMEstimator(linear_feature_columns, dnn_feature_columns, task='binary',
                            config=tf.estimator.RunConfig(tf_random_seed=2021))

    model.train(train_model_input)
    eval_result = model.evaluate(test_model_input)

    print(eval_result)
"
DeepCTR,gen_tfrecords.py,"import tensorflow as tf

def make_example(line, sparse_feature_name, dense_feature_name, label_name):
    features = {feat: tf.train.Feature(int64_list=tf.train.Int64List(value=[int(line[1][feat])])) for feat in
                sparse_feature_name}
    features.update(
        {feat: tf.train.Feature(float_list=tf.train.FloatList(value=[line[1][feat]])) for feat in dense_feature_name})
    features[label_name] = tf.train.Feature(float_list=tf.train.FloatList(value=[line[1][label_name]]))
    return tf.train.Example(features=tf.train.Features(feature=features))


def write_tfrecord(filename, df, sparse_feature_names, dense_feature_names, label_name):
    writer = tf.python_io.TFRecordWriter(filename)
    for line in df.iterrows():
        ex = make_example(line, sparse_feature_names, dense_feature_names, label_name)
        writer.write(ex.SerializeToString())
    writer.close()

# write_tfrecord('./criteo_sample.tr.tfrecords',train,sparse_features,dense_features,'label')
# write_tfrecord('./criteo_sample.te.tfrecords',test,sparse_features,dense_features,'label')
"
DeepCTR,run_din.py,"import numpy as np

from deepctr.models import DIN
from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, get_feature_names


def get_xy_fd():
    feature_columns = [SparseFeat('user', 3, embedding_dim=10), SparseFeat(
        'gender', 2, embedding_dim=4), SparseFeat('item_id', 3 + 1, embedding_dim=8),
                       SparseFeat('cate_id', 2 + 1, embedding_dim=4), DenseFeat('pay_score', 1)]
    feature_columns += [
        VarLenSparseFeat(SparseFeat('hist_item_id', vocabulary_size=3 + 1, embedding_dim=8, embedding_name='item_id'),
                         maxlen=4, length_name=""seq_length""),
        VarLenSparseFeat(SparseFeat('hist_cate_id', 2 + 1, embedding_dim=4, embedding_name='cate_id'), maxlen=4,
                         length_name=""seq_length"")]
    # Notice: History behavior sequence feature name must start with ""hist_"".
    behavior_feature_list = [""item_id"", ""cate_id""]
    uid = np.array([0, 1, 2])
    ugender = np.array([0, 1, 0])
    iid = np.array([1, 2, 3])  # 0 is mask value
    cate_id = np.array([1, 2, 2])  # 0 is mask value
    pay_score = np.array([0.1, 0.2, 0.3])

    hist_iid = np.array([[1, 2, 3, 0], [3, 2, 1, 0], [1, 2, 0, 0]])
    hist_cate_id = np.array([[1, 2, 2, 0], [2, 2, 1, 0], [1, 2, 0, 0]])
    seq_length = np.array([3, 3, 2])  # the actual length of the behavior sequence

    feature_dict = {'user': uid, 'gender': ugender, 'item_id': iid, 'cate_id': cate_id,
                    'hist_item_id': hist_iid, 'hist_cate_id': hist_cate_id,
                    'pay_score': pay_score, 'seq_length': seq_length}
    x = {name: feature_dict[name] for name in get_feature_names(feature_columns)}
    y = np.array([1, 0, 1])
    return x, y, feature_columns, behavior_feature_list


if __name__ == ""__main__"":
    x, y, feature_columns, behavior_feature_list = get_xy_fd()
    model = DIN(feature_columns, behavior_feature_list)
    # model = BST(feature_columns, behavior_feature_list,att_head_num=4)
    model.compile('adam', 'binary_crossentropy',
                  metrics=['binary_crossentropy'])
    history = model.fit(x, y, verbose=1, epochs=10, validation_split=0.5)
"
DeepCTR,run_classification_criteo_multi_gpu.py,"import pandas as pd
from sklearn.metrics import log_loss, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from tensorflow.python.keras.utils import multi_gpu_model

from deepctr.feature_column import SparseFeat, DenseFeat,get_feature_names
from deepctr.models import DeepFM

if __name__ == ""__main__"":
    data = pd.read_csv('./criteo_sample.txt')

    sparse_features = ['C' + str(i) for i in range(1, 27)]
    dense_features = ['I' + str(i) for i in range(1, 14)]

    data[sparse_features] = data[sparse_features].fillna('-1', )
    data[dense_features] = data[dense_features].fillna(0, )
    target = ['label']

    # 1.Label Encoding for sparse features,and do simple Transformation for dense features
    for feat in sparse_features:
        lbe = LabelEncoder()
        data[feat] = lbe.fit_transform(data[feat])
    mms = MinMaxScaler(feature_range=(0, 1))
    data[dense_features] = mms.fit_transform(data[dense_features])

    # 2.count #unique features for each sparse field,and record dense feature field name

    fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].max() + 1, embedding_dim=4)
                              for feat in sparse_features] + [DenseFeat(feat, 1, )
                                                              for feat in dense_features]

    dnn_feature_columns = fixlen_feature_columns
    linear_feature_columns = fixlen_feature_columns

    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)

    # 3.generate input data for model

    train, test = train_test_split(data, test_size=0.2, random_state=2020)

    train_model_input = {name: train[name] for name in feature_names}
    test_model_input = {name: test[name] for name in feature_names}

    # 4.Define Model,train,predict and evaluate
    model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary')
    model = multi_gpu_model(model, gpus=2)

    model.compile(""adam"", ""binary_crossentropy"",
                  metrics=['binary_crossentropy'], )

    history = model.fit(train_model_input, train[target].values,
                        batch_size=256, epochs=10, verbose=2, validation_split=0.2, )
    pred_ans = model.predict(test_model_input, batch_size=256)
    print(""test LogLoss"", round(log_loss(test[target].values, pred_ans), 4))
    print(""test AUC"", round(roc_auc_score(test[target].values, pred_ans), 4))
"
DeepCTR,run_multivalue_movielens_hash.py,"import numpy as np
import pandas as pd
from tensorflow.python.keras.preprocessing.sequence import pad_sequences

from deepctr.feature_column import SparseFeat, VarLenSparseFeat,get_feature_names
from deepctr.models import DeepFM

if __name__ == ""__main__"":
    data = pd.read_csv(""./movielens_sample.txt"")
    sparse_features = [""movie_id"", ""user_id"",
                       ""gender"", ""age"", ""occupation"", ""zip"", ]

    data[sparse_features] = data[sparse_features].astype(str)
    target = ['rating']

    # 1.Use hashing encoding on the fly for sparse features,and process sequence features

    genres_list = list(map(lambda x: x.split('|'), data['genres'].values))
    genres_length = np.array(list(map(len, genres_list)))
    max_len = max(genres_length)

    # Notice : padding=`post`
    genres_list = pad_sequences(genres_list, maxlen=max_len, padding='post', dtype=object, value=0).astype(str)
    # 2.set hashing space for each sparse field and generate feature config for sequence feature

    fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique() * 5, embedding_dim=4, use_hash=True, dtype='string')
                              for feat in sparse_features]
    varlen_feature_columns = [
        VarLenSparseFeat(SparseFeat('genres', vocabulary_size=100, embedding_dim=4, use_hash=True, dtype=""string""),
                         maxlen=max_len, combiner='mean',
                         )]  # Notice : value 0 is for padding for sequence input feature
    linear_feature_columns = fixlen_feature_columns + varlen_feature_columns
    dnn_feature_columns = fixlen_feature_columns + varlen_feature_columns
    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)

    # 3.generate input data for model
    model_input = {name: data[name] for name in feature_names}
    model_input['genres'] = genres_list

    # 4.Define Model,compile and train
    model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression')

    model.compile(""adam"", ""mse"", metrics=['mse'], )
    history = model.fit(model_input, data[target].values,
                        batch_size=256, epochs=10, verbose=2, validation_split=0.2, )
"
DeepCTR,run_dsin.py,"import numpy as np
import tensorflow as tf

from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat,get_feature_names
from deepctr.models import DSIN


def get_xy_fd(hash_flag=False):
    feature_columns = [SparseFeat('user', 3, embedding_dim=10, use_hash=hash_flag),
                       SparseFeat('gender', 2, embedding_dim=4, use_hash=hash_flag),
                       SparseFeat('item', 3 + 1, embedding_dim=4, use_hash=hash_flag),
                       SparseFeat('cate_id', 2 + 1, embedding_dim=4, use_hash=hash_flag),
                       DenseFeat('pay_score', 1)]
    feature_columns += [
        VarLenSparseFeat(SparseFeat('sess_0_item', 3 + 1, embedding_dim=4, use_hash=hash_flag, embedding_name='item'),
                         maxlen=4), VarLenSparseFeat(
            SparseFeat('sess_0_cate_id', 2 + 1, embedding_dim=4, use_hash=hash_flag, embedding_name='cate_id'),
            maxlen=4)]
    feature_columns += [
        VarLenSparseFeat(SparseFeat('sess_1_item', 3 + 1, embedding_dim=4, use_hash=hash_flag, embedding_name='item'),
                         maxlen=4), VarLenSparseFeat(
            SparseFeat('sess_1_cate_id', 2 + 1, embedding_dim=4, use_hash=hash_flag, embedding_name='cate_id'),
            maxlen=4)]

    behavior_feature_list = [""item"", ""cate_id""]
    uid = np.array([0, 1, 2])
    ugender = np.array([0, 1, 0])
    iid = np.array([1, 2, 3])  # 0 is mask value
    cateid = np.array([1, 2, 2])  # 0 is mask value
    score = np.array([0.1, 0.2, 0.3])

    sess1_iid = np.array([[1, 2, 3, 0], [3, 2, 1, 0], [0, 0, 0, 0]])
    sess1_cate_id = np.array([[1, 2, 2, 0], [2, 2, 1, 0], [0, 0, 0, 0]])

    sess2_iid = np.array([[1, 2, 3, 0], [0, 0, 0, 0], [0, 0, 0, 0]])
    sess2_cate_id = np.array([[1, 2, 2, 0], [0, 0, 0, 0], [0, 0, 0, 0]])

    sess_number = np.array([2, 1, 0])

    feature_dict = {'user': uid, 'gender': ugender, 'item': iid, 'cate_id': cateid,
                    'sess_0_item': sess1_iid, 'sess_0_cate_id': sess1_cate_id, 'pay_score': score,
                    'sess_1_item': sess2_iid, 'sess_1_cate_id': sess2_cate_id, }

    x = {name: feature_dict[name] for name in get_feature_names(feature_columns)}
    x[""sess_length""] = sess_number
    y = np.array([1, 0, 1])
    return x, y, feature_columns, behavior_feature_list


if __name__ == ""__main__"":
    if tf.__version__ >= '2.0.0':
        tf.compat.v1.disable_eager_execution()

    x, y, feature_columns, behavior_feature_list = get_xy_fd(True)

    model = DSIN(feature_columns, behavior_feature_list, sess_max_count=2,
                 dnn_hidden_units=[4, 4, 4], dnn_dropout=0.5, )

    model.compile('adam', 'binary_crossentropy',
                  metrics=['binary_crossentropy'])
    history = model.fit(x, y, verbose=1, epochs=10, validation_split=0.5)
"
DeepCTR,run_dien.py,"import numpy as np
import tensorflow as tf

from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat,get_feature_names
from deepctr.models import DIEN


def get_xy_fd(use_neg=False, hash_flag=False):
    feature_columns = [SparseFeat('user', 3, embedding_dim=10, use_hash=hash_flag),
                       SparseFeat('gender', 2, embedding_dim=4, use_hash=hash_flag),
                       SparseFeat('item_id', 3 + 1, embedding_dim=8, use_hash=hash_flag),
                       SparseFeat('cate_id', 2 + 1, embedding_dim=4, use_hash=hash_flag),
                       DenseFeat('pay_score', 1)]

    feature_columns += [
        VarLenSparseFeat(SparseFeat('hist_item_id', vocabulary_size=3 + 1, embedding_dim=8, embedding_name='item_id'),
                         maxlen=4, length_name=""seq_length""),
        VarLenSparseFeat(SparseFeat('hist_cate_id', 2 + 1, embedding_dim=4, embedding_name='cate_id'), maxlen=4,
                         length_name=""seq_length"")]

    behavior_feature_list = [""item_id"", ""cate_id""]
    uid = np.array([0, 1, 2])
    ugender = np.array([0, 1, 0])
    iid = np.array([1, 2, 3])  # 0 is mask value
    cate_id = np.array([1, 2, 2])  # 0 is mask value
    score = np.array([0.1, 0.2, 0.3])

    hist_iid = np.array([[1, 2, 3, 0], [1, 2, 3, 0], [1, 2, 0, 0]])
    hist_cate_id = np.array([[1, 2, 2, 0], [1, 2, 2, 0], [1, 2, 0, 0]])

    behavior_length = np.array([3, 3, 2])

    feature_dict = {'user': uid, 'gender': ugender, 'item_id': iid, 'cate_id': cate_id,
                    'hist_item_id': hist_iid, 'hist_cate_id': hist_cate_id,
                    'pay_score': score, ""seq_length"": behavior_length}

    if use_neg:
        feature_dict['neg_hist_item_id'] = np.array([[1, 2, 3, 0], [1, 2, 3, 0], [1, 2, 0, 0]])
        feature_dict['neg_hist_cate_id'] = np.array([[1, 2, 2, 0], [1, 2, 2, 0], [1, 2, 0, 0]])
        feature_columns += [
            VarLenSparseFeat(SparseFeat('neg_hist_item_id', vocabulary_size=3 + 1, embedding_dim=8, embedding_name='item_id'),
                             maxlen=4, length_name=""seq_length""),
            VarLenSparseFeat(SparseFeat('neg_hist_cate_id', 2 + 1, embedding_dim=4, embedding_name='cate_id'),
                             maxlen=4, length_name=""seq_length"")]

    x = {name: feature_dict[name] for name in get_feature_names(feature_columns)}
    y = np.array([1, 0, 1])
    return x, y, feature_columns, behavior_feature_list


if __name__ == ""__main__"":
    if tf.__version__ >= '2.0.0':
        tf.compat.v1.disable_eager_execution()
    USE_NEG = True
    x, y, feature_columns, behavior_feature_list = get_xy_fd(use_neg=USE_NEG)

    model = DIEN(feature_columns, behavior_feature_list,
                 dnn_hidden_units=[4, 4, 4], dnn_dropout=0.6, gru_type=""AUGRU"", use_negsampling=USE_NEG)

    model.compile('adam', 'binary_crossentropy',
                  metrics=['binary_crossentropy'])
    history = model.fit(x, y, verbose=1, epochs=10, validation_split=0.5)
"
DeepCTR,feature_column.py,"import tensorflow as tf
from collections import namedtuple, OrderedDict
from copy import copy
from itertools import chain

from tensorflow.python.keras.initializers import RandomNormal, Zeros
from tensorflow.python.keras.layers import Input, Lambda

from .inputs import create_embedding_matrix, embedding_lookup, get_dense_input, varlen_embedding_lookup, \
    get_varlen_pooling_list, mergeDict
from .layers import Linear
from .layers.utils import concat_func

DEFAULT_GROUP_NAME = ""default_group""


class SparseFeat(namedtuple('SparseFeat',
                            ['name', 'vocabulary_size', 'embedding_dim', 'use_hash', 'vocabulary_path', 'dtype', 'embeddings_initializer',
                             'embedding_name',
                             'group_name', 'trainable'])):
    __slots__ = ()

    def __new__(cls, name, vocabulary_size, embedding_dim=4, use_hash=False, vocabulary_path=None, dtype=""int32"", embeddings_initializer=None,
                embedding_name=None,
                group_name=DEFAULT_GROUP_NAME, trainable=True):

        if embedding_dim == ""auto"":
            embedding_dim = 6 * int(pow(vocabulary_size, 0.25))
        if embeddings_initializer is None:
            embeddings_initializer = RandomNormal(mean=0.0, stddev=0.0001, seed=2020)

        if embedding_name is None:
            embedding_name = name

        return super(SparseFeat, cls).__new__(cls, name, vocabulary_size, embedding_dim, use_hash, vocabulary_path, dtype,
                                              embeddings_initializer,
                                              embedding_name, group_name, trainable)

    def __hash__(self):
        return self.name.__hash__()


class VarLenSparseFeat(namedtuple('VarLenSparseFeat',
                                  ['sparsefeat', 'maxlen', 'combiner', 'length_name', 'weight_name', 'weight_norm'])):
    __slots__ = ()

    def __new__(cls, sparsefeat, maxlen, combiner=""mean"", length_name=None, weight_name=None, weight_norm=True):
        return super(VarLenSparseFeat, cls).__new__(cls, sparsefeat, maxlen, combiner, length_name, weight_name,
                                                    weight_norm)

    @property
    def name(self):
        return self.sparsefeat.name

    @property
    def vocabulary_size(self):
        return self.sparsefeat.vocabulary_size

    @property
    def embedding_dim(self):
        return self.sparsefeat.embedding_dim

    @property
    def use_hash(self):
        return self.sparsefeat.use_hash

    @property
    def vocabulary_path(self):
        return self.sparsefeat.vocabulary_path

    @property
    def dtype(self):
        return self.sparsefeat.dtype

    @property
    def embeddings_initializer(self):
        return self.sparsefeat.embeddings_initializer

    @property
    def embedding_name(self):
        return self.sparsefeat.embedding_name

    @property
    def group_name(self):
        return self.sparsefeat.group_name

    @property
    def trainable(self):
        return self.sparsefeat.trainable

    def __hash__(self):
        return self.name.__hash__()


class DenseFeat(namedtuple('DenseFeat', ['name', 'dimension', 'dtype', 'transform_fn'])):
    """""" Dense feature
    Args:
        name: feature name.
        dimension: dimension of the feature, default = 1.
        dtype: dtype of the feature, default=""float32"".
        transform_fn: If not `None` , a function that can be used to transform
        values of the feature.  the function takes the input Tensor as its
        argument, and returns the output Tensor.
        (e.g. lambda x: (x - 3.0) / 4.2).
    """"""
    __slots__ = ()

    def __new__(cls, name, dimension=1, dtype=""float32"", transform_fn=None):
        return super(DenseFeat, cls).__new__(cls, name, dimension, dtype, transform_fn)

    def __hash__(self):
        return self.name.__hash__()

    # def __eq__(self, other):
    #     if self.name == other.name:
    #         return True
    #     return False

    # def __repr__(self):
    #     return 'DenseFeat:'+self.name


def get_feature_names(feature_columns):
    features = build_input_features(feature_columns)
    return list(features.keys())


def build_input_features(feature_columns, prefix=''):
    input_features = OrderedDict()
    for fc in feature_columns:
        if isinstance(fc, SparseFeat):
            input_features[fc.name] = Input(
                shape=(1,), name=prefix + fc.name, dtype=fc.dtype)
        elif isinstance(fc, DenseFeat):
            input_features[fc.name] = Input(
                shape=(fc.dimension,), name=prefix + fc.name, dtype=fc.dtype)
        elif isinstance(fc, VarLenSparseFeat):
            input_features[fc.name] = Input(shape=(fc.maxlen,), name=prefix + fc.name,
                                            dtype=fc.dtype)
            if fc.weight_name is not None:
                input_features[fc.weight_name] = Input(shape=(fc.maxlen, 1), name=prefix + fc.weight_name,
                                                       dtype=""float32"")
            if fc.length_name is not None:
                input_features[fc.length_name] = Input((1,), name=prefix + fc.length_name, dtype='int32')

        else:
            raise TypeError(""Invalid feature column type,got"", type(fc))

    return input_features


def get_linear_logit(features, feature_columns, units=1, use_bias=False, seed=1024, prefix='linear',
                     l2_reg=0, sparse_feat_refine_weight=None):
    linear_feature_columns = copy(feature_columns)
    for i in range(len(linear_feature_columns)):
        if isinstance(linear_feature_columns[i], SparseFeat):
            linear_feature_columns[i] = linear_feature_columns[i]._replace(embedding_dim=1,
                                                                           embeddings_initializer=Zeros())
        if isinstance(linear_feature_columns[i], VarLenSparseFeat):
            linear_feature_columns[i] = linear_feature_columns[i]._replace(
                sparsefeat=linear_feature_columns[i].sparsefeat._replace(embedding_dim=1,
                                                                         embeddings_initializer=Zeros()))

    linear_emb_list = [input_from_feature_columns(features, linear_feature_columns, l2_reg, seed,
                                                  prefix=prefix + str(i))[0] for i in range(units)]
    _, dense_input_list = input_from_feature_columns(features, linear_feature_columns, l2_reg, seed, prefix=prefix)

    linear_logit_list = []
    for i in range(units):

        if len(linear_emb_list[i]) > 0 and len(dense_input_list) > 0:
            sparse_input = concat_func(linear_emb_list[i])
            dense_input = concat_func(dense_input_list)
            if sparse_feat_refine_weight is not None:
                sparse_input = Lambda(lambda x: x[0] * tf.expand_dims(x[1], axis=1))(
                    [sparse_input, sparse_feat_refine_weight])
            linear_logit = Linear(l2_reg, mode=2, use_bias=use_bias, seed=seed)([sparse_input, dense_input])
        elif len(linear_emb_list[i]) > 0:
            sparse_input = concat_func(linear_emb_list[i])
            if sparse_feat_refine_weight is not None:
                sparse_input = Lambda(lambda x: x[0] * tf.expand_dims(x[1], axis=1))(
                    [sparse_input, sparse_feat_refine_weight])
            linear_logit = Linear(l2_reg, mode=0, use_bias=use_bias, seed=seed)(sparse_input)
        elif len(dense_input_list) > 0:
            dense_input = concat_func(dense_input_list)
            linear_logit = Linear(l2_reg, mode=1, use_bias=use_bias, seed=seed)(dense_input)
        else:   #empty feature_columns
            return Lambda(lambda x: tf.constant([[0.0]]))(list(features.values())[0])
        linear_logit_list.append(linear_logit)

    return concat_func(linear_logit_list)


def input_from_feature_columns(features, feature_columns, l2_reg, seed, prefix='', seq_mask_zero=True,
                               support_dense=True, support_group=False):
    sparse_feature_columns = list(
        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if feature_columns else []
    varlen_sparse_feature_columns = list(
        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []

    embedding_matrix_dict = create_embedding_matrix(feature_columns, l2_reg, seed, prefix=prefix,
                                                    seq_mask_zero=seq_mask_zero)
    group_sparse_embedding_dict = embedding_lookup(embedding_matrix_dict, features, sparse_feature_columns)
    dense_value_list = get_dense_input(features, feature_columns)
    if not support_dense and len(dense_value_list) > 0:
        raise ValueError(""DenseFeat is not supported in dnn_feature_columns"")

    sequence_embed_dict = varlen_embedding_lookup(embedding_matrix_dict, features, varlen_sparse_feature_columns)
    group_varlen_sparse_embedding_dict = get_varlen_pooling_list(sequence_embed_dict, features,
                                                                 varlen_sparse_feature_columns)
    group_embedding_dict = mergeDict(group_sparse_embedding_dict, group_varlen_sparse_embedding_dict)
    if not support_group:
        group_embedding_dict = list(chain.from_iterable(group_embedding_dict.values()))
    return group_embedding_dict, dense_value_list
"
DeepCTR,__init__.py,"from .utils import check_version

__version__ = '0.9.3'
check_version(__version__)
"
DeepCTR,inputs.py,"# -*- coding:utf-8 -*-
""""""

Author:
    Weichen Shen,weichenswc@163.com

""""""

from collections import defaultdict
from itertools import chain

from tensorflow.python.keras.layers import Embedding, Lambda
from tensorflow.python.keras.regularizers import l2

from .layers.sequence import SequencePoolingLayer, WeightedSequenceLayer
from .layers.utils import Hash


def get_inputs_list(inputs):
    return list(chain(*list(map(lambda x: x.values(), filter(lambda x: x is not None, inputs)))))


def create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, seed, l2_reg,
                          prefix='sparse_', seq_mask_zero=True):
    sparse_embedding = {}
    for feat in sparse_feature_columns:
        emb = Embedding(feat.vocabulary_size, feat.embedding_dim,
                        embeddings_initializer=feat.embeddings_initializer,
                        embeddings_regularizer=l2(l2_reg),
                        name=prefix + '_emb_' + feat.embedding_name)
        emb.trainable = feat.trainable
        sparse_embedding[feat.embedding_name] = emb

    if varlen_sparse_feature_columns and len(varlen_sparse_feature_columns) > 0:
        for feat in varlen_sparse_feature_columns:
            # if feat.name not in sparse_embedding:
            emb = Embedding(feat.vocabulary_size, feat.embedding_dim,
                            embeddings_initializer=feat.embeddings_initializer,
                            embeddings_regularizer=l2(
                                l2_reg),
                            name=prefix + '_seq_emb_' + feat.name,
                            mask_zero=seq_mask_zero)
            emb.trainable = feat.trainable
            sparse_embedding[feat.embedding_name] = emb
    return sparse_embedding


def get_embedding_vec_list(embedding_dict, input_dict, sparse_feature_columns, return_feat_list=(), mask_feat_list=()):
    embedding_vec_list = []
    for fg in sparse_feature_columns:
        feat_name = fg.name
        if len(return_feat_list) == 0 or feat_name in return_feat_list:
            if fg.use_hash:
                lookup_idx = Hash(fg.vocabulary_size, mask_zero=(feat_name in mask_feat_list), vocabulary_path=fg.vocabulary_path)(input_dict[feat_name])
            else:
                lookup_idx = input_dict[feat_name]

            embedding_vec_list.append(embedding_dict[feat_name](lookup_idx))

    return embedding_vec_list


def create_embedding_matrix(feature_columns, l2_reg, seed, prefix="""", seq_mask_zero=True):
    from . import feature_column as fc_lib

    sparse_feature_columns = list(
        filter(lambda x: isinstance(x, fc_lib.SparseFeat), feature_columns)) if feature_columns else []
    varlen_sparse_feature_columns = list(
        filter(lambda x: isinstance(x, fc_lib.VarLenSparseFeat), feature_columns)) if feature_columns else []
    sparse_emb_dict = create_embedding_dict(sparse_feature_columns, varlen_sparse_feature_columns, seed,
                                            l2_reg, prefix=prefix + 'sparse', seq_mask_zero=seq_mask_zero)
    return sparse_emb_dict


def embedding_lookup(sparse_embedding_dict, sparse_input_dict, sparse_feature_columns, return_feat_list=(),
                     mask_feat_list=(), to_list=False):
    group_embedding_dict = defaultdict(list)
    for fc in sparse_feature_columns:
        feature_name = fc.name
        embedding_name = fc.embedding_name
        if (len(return_feat_list) == 0 or feature_name in return_feat_list):
            if fc.use_hash:
                lookup_idx = Hash(fc.vocabulary_size, mask_zero=(feature_name in mask_feat_list), vocabulary_path=fc.vocabulary_path)(
                    sparse_input_dict[feature_name])
            else:
                lookup_idx = sparse_input_dict[feature_name]

            group_embedding_dict[fc.group_name].append(sparse_embedding_dict[embedding_name](lookup_idx))
    if to_list:
        return list(chain.from_iterable(group_embedding_dict.values()))
    return group_embedding_dict


def varlen_embedding_lookup(embedding_dict, sequence_input_dict, varlen_sparse_feature_columns):
    varlen_embedding_vec_dict = {}
    for fc in varlen_sparse_feature_columns:
        feature_name = fc.name
        embedding_name = fc.embedding_name
        if fc.use_hash:
            lookup_idx = Hash(fc.vocabulary_size, mask_zero=True, vocabulary_path=fc.vocabulary_path)(sequence_input_dict[feature_name])
        else:
            lookup_idx = sequence_input_dict[feature_name]
        varlen_embedding_vec_dict[feature_name] = embedding_dict[embedding_name](lookup_idx)
    return varlen_embedding_vec_dict


def get_varlen_pooling_list(embedding_dict, features, varlen_sparse_feature_columns, to_list=False):
    pooling_vec_list = defaultdict(list)
    for fc in varlen_sparse_feature_columns:
        feature_name = fc.name
        combiner = fc.combiner
        feature_length_name = fc.length_name
        if feature_length_name is not None:
            if fc.weight_name is not None:
                seq_input = WeightedSequenceLayer(weight_normalization=fc.weight_norm)(
                    [embedding_dict[feature_name], features[feature_length_name], features[fc.weight_name]])
            else:
                seq_input = embedding_dict[feature_name]
            vec = SequencePoolingLayer(combiner, supports_masking=False)(
                [seq_input, features[feature_length_name]])
        else:
            if fc.weight_name is not None:
                seq_input = WeightedSequenceLayer(weight_normalization=fc.weight_norm, supports_masking=True)(
                    [embedding_dict[feature_name], features[fc.weight_name]])
            else:
                seq_input = embedding_dict[feature_name]
            vec = SequencePoolingLayer(combiner, supports_masking=True)(
                seq_input)
        pooling_vec_list[fc.group_name].append(vec)
    if to_list:
        return chain.from_iterable(pooling_vec_list.values())
    return pooling_vec_list


def get_dense_input(features, feature_columns):
    from . import feature_column as fc_lib
    dense_feature_columns = list(
        filter(lambda x: isinstance(x, fc_lib.DenseFeat), feature_columns)) if feature_columns else []
    dense_input_list = []
    for fc in dense_feature_columns:
        if fc.transform_fn is None:
            dense_input_list.append(features[fc.name])
        else:
            transform_result = Lambda(fc.transform_fn)(features[fc.name])
            dense_input_list.append(transform_result)
    return dense_input_list


def mergeDict(a, b):
    c = defaultdict(list)
    for k, v in a.items():
        c[k].extend(v)
    for k, v in b.items():
        c[k].extend(v)
    return c
"
DeepCTR,utils.py,"# -*- coding:utf-8 -*-
""""""

Author:
    Weichen Shen,weichenswc@163.com

""""""

import json
import logging
from threading import Thread

import requests

try:
    from packaging.version import parse
except ImportError:
    from pip._vendor.packaging.version import parse


def check_version(version):
    """"""Return version of package on pypi.python.org using json.""""""

    def check(version):
        try:
            url_pattern = 'https://pypi.python.org/pypi/deepctr/json'
            req = requests.get(url_pattern)
            latest_version = parse('0')
            version = parse(version)
            if req.status_code == requests.codes.ok:
                j = json.loads(req.text.encode('utf-8'))
                releases = j.get('releases', [])
                for release in releases:
                    ver = parse(release)
                    if ver.is_prerelease or ver.is_postrelease:
                        continue
                    latest_version = max(latest_version, ver)
                if latest_version > version:
                    logging.warning(
                        '\nDeepCTR version {0} detected. Your version is {1}.\nUse `pip install -U deepctr` to upgrade.Changelog: https://github.com/shenweichen/DeepCTR/releases/tag/v{0}'.format(
                            latest_version, version))
        except:
            print(""Please check the latest version manually on https://pypi.org/project/deepctr/#history"")
            return

    Thread(target=check, args=(version,)).start()
"
DeepCTR,interaction.py,"# -*- coding:utf-8 -*-
""""""

Authors:
    Weichen Shen,weichenswc@163.com,
    Harshit Pande,
    Yi He, heyi_jack@163.com

""""""

import itertools

import tensorflow as tf
from tensorflow.python.keras import backend as K
from tensorflow.python.keras.backend import batch_dot

try:
    from tensorflow.python.ops.init_ops import Zeros, Ones, Constant, TruncatedNormal, \
        glorot_normal_initializer as glorot_normal, \
        glorot_uniform_initializer as glorot_uniform
except ImportError:
    from tensorflow.python.ops.init_ops_v2 import Zeros, Ones, Constant, TruncatedNormal, glorot_normal, glorot_uniform

from tensorflow.python.keras.layers import Layer, MaxPooling2D, Conv2D, Dropout, Lambda, Dense, Flatten
from tensorflow.python.keras.regularizers import l2
from tensorflow.python.layers import utils

from .activation import activation_layer
from .utils import concat_func, reduce_sum, softmax, reduce_mean
from .core import DNN


class AFMLayer(Layer):
    """"""Attentonal Factorization Machine models pairwise (order-2) feature
    interactions without linear term and bias.

      Input shape
        - A list of 3D tensor with shape: ``(batch_size,1,embedding_size)``.

      Output shape
        - 2D tensor with shape: ``(batch_size, 1)``.

      Arguments
        - **attention_factor** : Positive integer, dimensionality of the
         attention network output space.

        - **l2_reg_w** : float between 0 and 1. L2 regularizer strength
         applied to attention network.

        - **dropout_rate** : float between in [0,1). Fraction of the attention net output units to dropout.

        - **seed** : A Python integer to use as random seed.

      References
        - [Attentional Factorization Machines : Learning the Weight of Feature
        Interactions via Attention Networks](https://arxiv.org/pdf/1708.04617.pdf)
    """"""

    def __init__(self, attention_factor=4, l2_reg_w=0, dropout_rate=0, seed=1024, **kwargs):
        self.attention_factor = attention_factor
        self.l2_reg_w = l2_reg_w
        self.dropout_rate = dropout_rate
        self.seed = seed
        super(AFMLayer, self).__init__(**kwargs)

    def build(self, input_shape):

        if not isinstance(input_shape, list) or len(input_shape) < 2:
            # input_shape = input_shape[0]
            # if not isinstance(input_shape, list) or len(input_shape) < 2:
            raise ValueError('A `AttentionalFM` layer should be called '
                             'on a list of at least 2 inputs')

        shape_set = set()
        reduced_input_shape = [shape.as_list() for shape in input_shape]
        for i in range(len(input_shape)):
            shape_set.add(tuple(reduced_input_shape[i]))

        if len(shape_set) > 1:
            raise ValueError('A `AttentionalFM` layer requires '
                             'inputs with same shapes '
                             'Got different shapes: %s' % (shape_set))

        if len(input_shape[0]) != 3 or input_shape[0][1] != 1:
            raise ValueError('A `AttentionalFM` layer requires '
                             'inputs of a list with same shape tensor like\
                             (None, 1, embedding_size)'
                             'Got different shapes: %s' % (input_shape[0]))

        embedding_size = int(input_shape[0][-1])

        self.attention_W = self.add_weight(shape=(embedding_size,
                                                  self.attention_factor), initializer=glorot_normal(seed=self.seed),
                                           regularizer=l2(self.l2_reg_w), name=""attention_W"")
        self.attention_b = self.add_weight(
            shape=(self.attention_factor,), initializer=Zeros(), name=""attention_b"")
        self.projection_h = self.add_weight(shape=(self.attention_factor, 1),
                                            initializer=glorot_normal(seed=self.seed), name=""projection_h"")
        self.projection_p = self.add_weight(shape=(
            embedding_size, 1), initializer=glorot_normal(seed=self.seed), name=""projection_p"")
        self.dropout = Dropout(
            self.dropout_rate, seed=self.seed)

        self.tensordot = Lambda(
            lambda x: tf.tensordot(x[0], x[1], axes=(-1, 0)))

        # Be sure to call this somewhere!
        super(AFMLayer, self).build(input_shape)

    def call(self, inputs, training=None, **kwargs):

        if K.ndim(inputs[0]) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (K.ndim(inputs)))

        embeds_vec_list = inputs
        row = []
        col = []

        for r, c in itertools.combinations(embeds_vec_list, 2):
            row.append(r)
            col.append(c)

        p = tf.concat(row, axis=1)
        q = tf.concat(col, axis=1)
        inner_product = p * q

        bi_interaction = inner_product
        attention_temp = tf.nn.relu(tf.nn.bias_add(tf.tensordot(
            bi_interaction, self.attention_W, axes=(-1, 0)), self.attention_b))
        #  Dense(self.attention_factor,'relu',kernel_regularizer=l2(self.l2_reg_w))(bi_interaction)
        self.normalized_att_score = softmax(tf.tensordot(
            attention_temp, self.projection_h, axes=(-1, 0)), dim=1)
        attention_output = reduce_sum(
            self.normalized_att_score * bi_interaction, axis=1)

        attention_output = self.dropout(attention_output, training=training)  # training

        afm_out = self.tensordot([attention_output, self.projection_p])
        return afm_out

    def compute_output_shape(self, input_shape):

        if not isinstance(input_shape, list):
            raise ValueError('A `AFMLayer` layer should be called '
                             'on a list of inputs.')
        return (None, 1)

    def get_config(self, ):
        config = {'attention_factor': self.attention_factor,
                  'l2_reg_w': self.l2_reg_w, 'dropout_rate': self.dropout_rate, 'seed': self.seed}
        base_config = super(AFMLayer, self).get_config()
        base_config.update(config)
        return base_config


class BiInteractionPooling(Layer):
    """"""Bi-Interaction Layer used in Neural FM,compress the
     pairwise element-wise product of features into one single vector.

      Input shape
        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.

      Output shape
        - 3D tensor with shape: ``(batch_size,1,embedding_size)``.

      References
        - [He X, Chua T S. Neural factorization machines for sparse predictive analytics[C]//Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 2017: 355-364.](http://arxiv.org/abs/1708.05027)
    """"""

    def __init__(self, **kwargs):

        super(BiInteractionPooling, self).__init__(**kwargs)

    def build(self, input_shape):

        if len(input_shape) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (len(input_shape)))

        super(BiInteractionPooling, self).build(
            input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, **kwargs):

        if K.ndim(inputs) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (K.ndim(inputs)))

        concated_embeds_value = inputs
        square_of_sum = tf.square(reduce_sum(
            concated_embeds_value, axis=1, keep_dims=True))
        sum_of_square = reduce_sum(
            concated_embeds_value * concated_embeds_value, axis=1, keep_dims=True)
        cross_term = 0.5 * (square_of_sum - sum_of_square)

        return cross_term

    def compute_output_shape(self, input_shape):
        return (None, 1, input_shape[-1])


class CIN(Layer):
    """"""Compressed Interaction Network used in xDeepFM.This implemention is
    adapted from code that the author of the paper published on https://github.com/Leavingseason/xDeepFM.

      Input shape
        - 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.

      Output shape
        - 2D tensor with shape: ``(batch_size, featuremap_num)`` ``featuremap_num =  sum(self.layer_size[:-1]) // 2 + self.layer_size[-1]`` if ``split_half=True``,else  ``sum(layer_size)`` .

      Arguments
        - **layer_size** : list of int.Feature maps in each layer.

        - **activation** : activation function used on feature maps.

        - **split_half** : bool.if set to False, half of the feature maps in each hidden will connect to output unit.

        - **seed** : A Python integer to use as random seed.

      References
        - [Lian J, Zhou X, Zhang F, et al. xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems[J]. arXiv preprint arXiv:1803.05170, 2018.] (https://arxiv.org/pdf/1803.05170.pdf)
    """"""

    def __init__(self, layer_size=(128, 128), activation='relu', split_half=True, l2_reg=1e-5, seed=1024, **kwargs):
        if len(layer_size) == 0:
            raise ValueError(
                ""layer_size must be a list(tuple) of length greater than 1"")
        self.layer_size = layer_size
        self.split_half = split_half
        self.activation = activation
        self.l2_reg = l2_reg
        self.seed = seed
        super(CIN, self).__init__(**kwargs)

    def build(self, input_shape):
        if len(input_shape) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (len(input_shape)))

        self.field_nums = [int(input_shape[1])]
        self.filters = []
        self.bias = []
        for i, size in enumerate(self.layer_size):

            self.filters.append(self.add_weight(name='filter' + str(i),
                                                shape=[1, self.field_nums[-1]
                                                       * self.field_nums[0], size],
                                                dtype=tf.float32, initializer=glorot_uniform(
                    seed=self.seed + i),
                                                regularizer=l2(self.l2_reg)))

            self.bias.append(self.add_weight(name='bias' + str(i), shape=[size], dtype=tf.float32,
                                             initializer=Zeros()))

            if self.split_half:
                if i != len(self.layer_size) - 1 and size % 2 > 0:
                    raise ValueError(
                        ""layer_size must be even number except for the last layer when split_half=True"")

                self.field_nums.append(size // 2)
            else:
                self.field_nums.append(size)

        self.activation_layers = [activation_layer(
            self.activation) for _ in self.layer_size]

        super(CIN, self).build(input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, **kwargs):

        if K.ndim(inputs) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (K.ndim(inputs)))

        dim = int(inputs.get_shape()[-1])
        hidden_nn_layers = [inputs]
        final_result = []

        split_tensor0 = tf.split(hidden_nn_layers[0], dim * [1], 2)
        for idx, layer_size in enumerate(self.layer_size):
            split_tensor = tf.split(hidden_nn_layers[-1], dim * [1], 2)

            dot_result_m = tf.matmul(
                split_tensor0, split_tensor, transpose_b=True)

            dot_result_o = tf.reshape(
                dot_result_m, shape=[dim, -1, self.field_nums[0] * self.field_nums[idx]])

            dot_result = tf.transpose(dot_result_o, perm=[1, 0, 2])

            curr_out = tf.nn.conv1d(
                dot_result, filters=self.filters[idx], stride=1, padding='VALID')

            curr_out = tf.nn.bias_add(curr_out, self.bias[idx])

            curr_out = self.activation_layers[idx](curr_out)

            curr_out = tf.transpose(curr_out, perm=[0, 2, 1])

            if self.split_half:
                if idx != len(self.layer_size) - 1:
                    next_hidden, direct_connect = tf.split(
                        curr_out, 2 * [layer_size // 2], 1)
                else:
                    direct_connect = curr_out
                    next_hidden = 0
            else:
                direct_connect = curr_out
                next_hidden = curr_out

            final_result.append(direct_connect)
            hidden_nn_layers.append(next_hidden)

        result = tf.concat(final_result, axis=1)
        result = reduce_sum(result, -1, keep_dims=False)

        return result

    def compute_output_shape(self, input_shape):
        if self.split_half:
            featuremap_num = sum(
                self.layer_size[:-1]) // 2 + self.layer_size[-1]
        else:
            featuremap_num = sum(self.layer_size)
        return (None, featuremap_num)

    def get_config(self, ):

        config = {'layer_size': self.layer_size, 'split_half': self.split_half, 'activation': self.activation,
                  'seed': self.seed}
        base_config = super(CIN, self).get_config()
        base_config.update(config)
        return base_config


class CrossNet(Layer):
    """"""The Cross Network part of Deep&Cross Network model,
    which leans both low and high degree cross feature.

      Input shape
        - 2D tensor with shape: ``(batch_size, units)``.

      Output shape
        - 2D tensor with shape: ``(batch_size, units)``.

      Arguments
        - **layer_num**: Positive integer, the cross layer number

        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix

        - **parameterization**: string, ``""vector""``  or ``""matrix""`` ,  way to parameterize the cross network.

        - **seed**: A Python integer to use as random seed.

      References
        - [Wang R, Fu B, Fu G, et al. Deep & cross network for ad click predictions[C]//Proceedings of the ADKDD'17. ACM, 2017: 12.](https://arxiv.org/abs/1708.05123)
    """"""

    def __init__(self, layer_num=2, parameterization='vector', l2_reg=0, seed=1024, **kwargs):
        self.layer_num = layer_num
        self.parameterization = parameterization
        self.l2_reg = l2_reg
        self.seed = seed
        print('CrossNet parameterization:', self.parameterization)
        super(CrossNet, self).__init__(**kwargs)

    def build(self, input_shape):

        if len(input_shape) != 2:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 2 dimensions"" % (len(input_shape),))

        dim = int(input_shape[-1])
        if self.parameterization == 'vector':
            self.kernels = [self.add_weight(name='kernel' + str(i),
                                            shape=(dim, 1),
                                            initializer=glorot_normal(
                                                seed=self.seed),
                                            regularizer=l2(self.l2_reg),
                                            trainable=True) for i in range(self.layer_num)]
        elif self.parameterization == 'matrix':
            self.kernels = [self.add_weight(name='kernel' + str(i),
                                            shape=(dim, dim),
                                            initializer=glorot_normal(
                                                seed=self.seed),
                                            regularizer=l2(self.l2_reg),
                                            trainable=True) for i in range(self.layer_num)]
        else:  # error
            raise ValueError(""parameterization should be 'vector' or 'matrix'"")
        self.bias = [self.add_weight(name='bias' + str(i),
                                     shape=(dim, 1),
                                     initializer=Zeros(),
                                     trainable=True) for i in range(self.layer_num)]
        # Be sure to call this somewhere!
        super(CrossNet, self).build(input_shape)

    def call(self, inputs, **kwargs):
        if K.ndim(inputs) != 2:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 2 dimensions"" % (K.ndim(inputs)))

        x_0 = tf.expand_dims(inputs, axis=2)
        x_l = x_0
        for i in range(self.layer_num):
            if self.parameterization == 'vector':
                xl_w = tf.tensordot(x_l, self.kernels[i], axes=(1, 0))
                dot_ = tf.matmul(x_0, xl_w)
                x_l = dot_ + self.bias[i] + x_l
            elif self.parameterization == 'matrix':
                xl_w = tf.einsum('ij,bjk->bik', self.kernels[i], x_l)  # W * xi  (bs, dim, 1)
                dot_ = xl_w + self.bias[i]  # W * xi + b
                x_l = x_0 * dot_ + x_l  # x0 · (W * xi + b) +xl  Hadamard-product
            else:  # error
                raise ValueError(""parameterization should be 'vector' or 'matrix'"")
        x_l = tf.squeeze(x_l, axis=2)
        return x_l

    def get_config(self, ):

        config = {'layer_num': self.layer_num, 'parameterization': self.parameterization,
                  'l2_reg': self.l2_reg, 'seed': self.seed}
        base_config = super(CrossNet, self).get_config()
        base_config.update(config)
        return base_config

    def compute_output_shape(self, input_shape):
        return input_shape


class CrossNetMix(Layer):
    """"""The Cross Network part of DCN-Mix model, which improves DCN-M by:
      1 add MOE to learn feature interactions in different subspaces
      2 add nonlinear transformations in low-dimensional space

      Input shape
        - 2D tensor with shape: ``(batch_size, units)``.

      Output shape
        - 2D tensor with shape: ``(batch_size, units)``.

      Arguments
        - **low_rank** : Positive integer, dimensionality of low-rank sapce.

        - **num_experts** : Positive integer, number of experts.

        - **layer_num**: Positive integer, the cross layer number

        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix

        - **seed**: A Python integer to use as random seed.

      References
        - [Wang R, Shivanna R, Cheng D Z, et al. DCN-M: Improved Deep & Cross Network for Feature Cross Learning in Web-scale Learning to Rank Systems[J]. 2020.](https://arxiv.org/abs/2008.13535)
    """"""

    def __init__(self, low_rank=32, num_experts=4, layer_num=2, l2_reg=0, seed=1024, **kwargs):
        self.low_rank = low_rank
        self.num_experts = num_experts
        self.layer_num = layer_num
        self.l2_reg = l2_reg
        self.seed = seed
        super(CrossNetMix, self).__init__(**kwargs)

    def build(self, input_shape):

        if len(input_shape) != 2:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 2 dimensions"" % (len(input_shape),))

        dim = int(input_shape[-1])

        # U: (dim, low_rank)
        self.U_list = [self.add_weight(name='U_list' + str(i),
                                       shape=(self.num_experts, dim, self.low_rank),
                                       initializer=glorot_normal(
                                           seed=self.seed),
                                       regularizer=l2(self.l2_reg),
                                       trainable=True) for i in range(self.layer_num)]
        # V: (dim, low_rank)
        self.V_list = [self.add_weight(name='V_list' + str(i),
                                       shape=(self.num_experts, dim, self.low_rank),
                                       initializer=glorot_normal(
                                           seed=self.seed),
                                       regularizer=l2(self.l2_reg),
                                       trainable=True) for i in range(self.layer_num)]
        # C: (low_rank, low_rank)
        self.C_list = [self.add_weight(name='C_list' + str(i),
                                       shape=(self.num_experts, self.low_rank, self.low_rank),
                                       initializer=glorot_normal(
                                           seed=self.seed),
                                       regularizer=l2(self.l2_reg),
                                       trainable=True) for i in range(self.layer_num)]

        self.gating = [Dense(1, use_bias=False) for i in range(self.num_experts)]

        self.bias = [self.add_weight(name='bias' + str(i),
                                     shape=(dim, 1),
                                     initializer=Zeros(),
                                     trainable=True) for i in range(self.layer_num)]
        # Be sure to call this somewhere!
        super(CrossNetMix, self).build(input_shape)

    def call(self, inputs, **kwargs):
        if K.ndim(inputs) != 2:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 2 dimensions"" % (K.ndim(inputs)))

        x_0 = tf.expand_dims(inputs, axis=2)
        x_l = x_0
        for i in range(self.layer_num):
            output_of_experts = []
            gating_score_of_experts = []
            for expert_id in range(self.num_experts):
                # (1) G(x_l)
                # compute the gating score by x_l
                gating_score_of_experts.append(self.gating[expert_id](tf.squeeze(x_l, axis=2)))

                # (2) E(x_l)
                # project the input x_l to $\mathbb{R}^{r}$
                v_x = tf.einsum('ij,bjk->bik', tf.transpose(self.V_list[i][expert_id]), x_l)  # (bs, low_rank, 1)

                # nonlinear activation in low rank space
                v_x = tf.nn.tanh(v_x)
                v_x = tf.einsum('ij,bjk->bik', self.C_list[i][expert_id], v_x)  # (bs, low_rank, 1)
                v_x = tf.nn.tanh(v_x)

                # project back to $\mathbb{R}^{d}$
                uv_x = tf.einsum('ij,bjk->bik', self.U_list[i][expert_id], v_x)  # (bs, dim, 1)

                dot_ = uv_x + self.bias[i]
                dot_ = x_0 * dot_  # Hadamard-product

                output_of_experts.append(tf.squeeze(dot_, axis=2))

            # (3) mixture of low-rank experts
            output_of_experts = tf.stack(output_of_experts, 2)  # (bs, dim, num_experts)
            gating_score_of_experts = tf.stack(gating_score_of_experts, 1)  # (bs, num_experts, 1)
            moe_out = tf.matmul(output_of_experts, tf.nn.softmax(gating_score_of_experts, 1))
            x_l = moe_out + x_l  # (bs, dim, 1)
        x_l = tf.squeeze(x_l, axis=2)
        return x_l

    def get_config(self, ):

        config = {'low_rank': self.low_rank, 'num_experts': self.num_experts, 'layer_num': self.layer_num,
                  'l2_reg': self.l2_reg, 'seed': self.seed}
        base_config = super(CrossNetMix, self).get_config()
        base_config.update(config)
        return base_config

    def compute_output_shape(self, input_shape):
        return input_shape


class FM(Layer):
    """"""Factorization Machine models pairwise (order-2) feature interactions
     without linear term and bias.

      Input shape
        - 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.

      Output shape
        - 2D tensor with shape: ``(batch_size, 1)``.

      References
        - [Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)
    """"""

    def __init__(self, **kwargs):

        super(FM, self).__init__(**kwargs)

    def build(self, input_shape):
        if len(input_shape) != 3:
            raise ValueError(""Unexpected inputs dimensions % d,\
                             expect to be 3 dimensions"" % (len(input_shape)))

        super(FM, self).build(input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, **kwargs):

        if K.ndim(inputs) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions""
                % (K.ndim(inputs)))

        concated_embeds_value = inputs

        square_of_sum = tf.square(reduce_sum(
            concated_embeds_value, axis=1, keep_dims=True))
        sum_of_square = reduce_sum(
            concated_embeds_value * concated_embeds_value, axis=1, keep_dims=True)
        cross_term = square_of_sum - sum_of_square
        cross_term = 0.5 * reduce_sum(cross_term, axis=2, keep_dims=False)

        return cross_term

    def compute_output_shape(self, input_shape):
        return (None, 1)


class InnerProductLayer(Layer):
    """"""InnerProduct Layer used in PNN that compute the element-wise
    product or inner product between feature vectors.

      Input shape
        - a list of 3D tensor with shape: ``(batch_size,1,embedding_size)``.

      Output shape
        - 3D tensor with shape: ``(batch_size, N*(N-1)/2 ,1)`` if use reduce_sum. or 3D tensor with shape: ``(batch_size, N*(N-1)/2, embedding_size )`` if not use reduce_sum.

      Arguments
        - **reduce_sum**: bool. Whether return inner product or element-wise product

      References
            - [Qu Y, Cai H, Ren K, et al. Product-based neural networks for user response prediction[C]//Data Mining (ICDM), 2016 IEEE 16th International Conference on. IEEE, 2016: 1149-1154.](https://arxiv.org/pdf/1611.00144.pdf)
    """"""

    def __init__(self, reduce_sum=True, **kwargs):
        self.reduce_sum = reduce_sum
        super(InnerProductLayer, self).__init__(**kwargs)

    def build(self, input_shape):

        if not isinstance(input_shape, list) or len(input_shape) < 2:
            raise ValueError('A `InnerProductLayer` layer should be called '
                             'on a list of at least 2 inputs')

        reduced_inputs_shapes = [shape.as_list() for shape in input_shape]
        shape_set = set()

        for i in range(len(input_shape)):
            shape_set.add(tuple(reduced_inputs_shapes[i]))

        if len(shape_set) > 1:
            raise ValueError('A `InnerProductLayer` layer requires '
                             'inputs with same shapes '
                             'Got different shapes: %s' % (shape_set))

        if len(input_shape[0]) != 3 or input_shape[0][1] != 1:
            raise ValueError('A `InnerProductLayer` layer requires '
                             'inputs of a list with same shape tensor like (None,1,embedding_size)'
                             'Got different shapes: %s' % (input_shape[0]))
        super(InnerProductLayer, self).build(
            input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, **kwargs):
        if K.ndim(inputs[0]) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (K.ndim(inputs)))

        embed_list = inputs
        row = []
        col = []
        num_inputs = len(embed_list)

        for i in range(num_inputs - 1):
            for j in range(i + 1, num_inputs):
                row.append(i)
                col.append(j)
        p = tf.concat([embed_list[idx]
                       for idx in row], axis=1)  # batch num_pairs k
        q = tf.concat([embed_list[idx]
                       for idx in col], axis=1)

        inner_product = p * q
        if self.reduce_sum:
            inner_product = reduce_sum(
                inner_product, axis=2, keep_dims=True)
        return inner_product

    def compute_output_shape(self, input_shape):
        num_inputs = len(input_shape)
        num_pairs = int(num_inputs * (num_inputs - 1) / 2)
        input_shape = input_shape[0]
        embed_size = input_shape[-1]
        if self.reduce_sum:
            return (input_shape[0], num_pairs, 1)
        else:
            return (input_shape[0], num_pairs, embed_size)

    def get_config(self, ):
        config = {'reduce_sum': self.reduce_sum, }
        base_config = super(InnerProductLayer, self).get_config()
        base_config.update(config)
        return base_config


class InteractingLayer(Layer):
    """"""A Layer used in AutoInt that model the correlations between different feature fields by multi-head self-attention mechanism.

      Input shape
            - A 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.

      Output shape
            - 3D tensor with shape:``(batch_size,field_size,att_embedding_size * head_num)``.


      Arguments
            - **att_embedding_size**: int.The embedding size in multi-head self-attention network.
            - **head_num**: int.The head number in multi-head  self-attention network.
            - **use_res**: bool.Whether or not use standard residual connections before output.
            - **seed**: A Python integer to use as random seed.

      References
            - [Song W, Shi C, Xiao Z, et al. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks[J]. arXiv preprint arXiv:1810.11921, 2018.](https://arxiv.org/abs/1810.11921)
    """"""

    def __init__(self, att_embedding_size=8, head_num=2, use_res=True, scaling=False, seed=1024, **kwargs):
        if head_num <= 0:
            raise ValueError('head_num must be a int > 0')
        self.att_embedding_size = att_embedding_size
        self.head_num = head_num
        self.use_res = use_res
        self.seed = seed
        self.scaling = scaling
        super(InteractingLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        if len(input_shape) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (len(input_shape)))
        embedding_size = int(input_shape[-1])
        self.W_Query = self.add_weight(name='query', shape=[embedding_size, self.att_embedding_size * self.head_num],
                                       dtype=tf.float32,
                                       initializer=TruncatedNormal(seed=self.seed))
        self.W_key = self.add_weight(name='key', shape=[embedding_size, self.att_embedding_size * self.head_num],
                                     dtype=tf.float32,
                                     initializer=TruncatedNormal(seed=self.seed + 1))
        self.W_Value = self.add_weight(name='value', shape=[embedding_size, self.att_embedding_size * self.head_num],
                                       dtype=tf.float32,
                                       initializer=TruncatedNormal(seed=self.seed + 2))
        if self.use_res:
            self.W_Res = self.add_weight(name='res', shape=[embedding_size, self.att_embedding_size * self.head_num],
                                         dtype=tf.float32,
                                         initializer=TruncatedNormal(seed=self.seed))

        # Be sure to call this somewhere!
        super(InteractingLayer, self).build(input_shape)

    def call(self, inputs, **kwargs):
        if K.ndim(inputs) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (K.ndim(inputs)))

        querys = tf.tensordot(inputs, self.W_Query,
                              axes=(-1, 0))  # None F D*head_num
        keys = tf.tensordot(inputs, self.W_key, axes=(-1, 0))
        values = tf.tensordot(inputs, self.W_Value, axes=(-1, 0))

        # head_num None F D
        querys = tf.stack(tf.split(querys, self.head_num, axis=2))
        keys = tf.stack(tf.split(keys, self.head_num, axis=2))
        values = tf.stack(tf.split(values, self.head_num, axis=2))

        inner_product = tf.matmul(
            querys, keys, transpose_b=True)  # head_num None F F
        if self.scaling:
            inner_product /= self.att_embedding_size ** 0.5
        self.normalized_att_scores = softmax(inner_product)

        result = tf.matmul(self.normalized_att_scores,
                           values)  # head_num None F D
        result = tf.concat(tf.split(result, self.head_num, ), axis=-1)
        result = tf.squeeze(result, axis=0)  # None F D*head_num

        if self.use_res:
            result += tf.tensordot(inputs, self.W_Res, axes=(-1, 0))
        result = tf.nn.relu(result)

        return result

    def compute_output_shape(self, input_shape):

        return (None, input_shape[1], self.att_embedding_size * self.head_num)

    def get_config(self, ):
        config = {'att_embedding_size': self.att_embedding_size, 'head_num': self.head_num, 'use_res': self.use_res,
                  'seed': self.seed}
        base_config = super(InteractingLayer, self).get_config()
        base_config.update(config)
        return base_config


class OutterProductLayer(Layer):
    """"""OutterProduct Layer used in PNN.This implemention is
    adapted from code that the author of the paper published on https://github.com/Atomu2014/product-nets.

      Input shape
            - A list of N 3D tensor with shape: ``(batch_size,1,embedding_size)``.

      Output shape
            - 2D tensor with shape:``(batch_size,N*(N-1)/2 )``.

      Arguments
            - **kernel_type**: str. The kernel weight matrix type to use,can be mat,vec or num

            - **seed**: A Python integer to use as random seed.

      References
            - [Qu Y, Cai H, Ren K, et al. Product-based neural networks for user response prediction[C]//Data Mining (ICDM), 2016 IEEE 16th International Conference on. IEEE, 2016: 1149-1154.](https://arxiv.org/pdf/1611.00144.pdf)
    """"""

    def __init__(self, kernel_type='mat', seed=1024, **kwargs):
        if kernel_type not in ['mat', 'vec', 'num']:
            raise ValueError(""kernel_type must be mat,vec or num"")
        self.kernel_type = kernel_type
        self.seed = seed
        super(OutterProductLayer, self).__init__(**kwargs)

    def build(self, input_shape):

        if not isinstance(input_shape, list) or len(input_shape) < 2:
            raise ValueError('A `OutterProductLayer` layer should be called '
                             'on a list of at least 2 inputs')

        reduced_inputs_shapes = [shape.as_list() for shape in input_shape]
        shape_set = set()

        for i in range(len(input_shape)):
            shape_set.add(tuple(reduced_inputs_shapes[i]))

        if len(shape_set) > 1:
            raise ValueError('A `OutterProductLayer` layer requires '
                             'inputs with same shapes '
                             'Got different shapes: %s' % (shape_set))

        if len(input_shape[0]) != 3 or input_shape[0][1] != 1:
            raise ValueError('A `OutterProductLayer` layer requires '
                             'inputs of a list with same shape tensor like (None,1,embedding_size)'
                             'Got different shapes: %s' % (input_shape[0]))
        num_inputs = len(input_shape)
        num_pairs = int(num_inputs * (num_inputs - 1) / 2)
        input_shape = input_shape[0]
        embed_size = int(input_shape[-1])
        if self.kernel_type == 'mat':

            self.kernel = self.add_weight(shape=(embed_size, num_pairs, embed_size),
                                          initializer=glorot_uniform(
                                              seed=self.seed),
                                          name='kernel')
        elif self.kernel_type == 'vec':
            self.kernel = self.add_weight(shape=(num_pairs, embed_size,), initializer=glorot_uniform(self.seed),
                                          name='kernel'
                                          )
        elif self.kernel_type == 'num':
            self.kernel = self.add_weight(
                shape=(num_pairs, 1), initializer=glorot_uniform(self.seed), name='kernel')

        super(OutterProductLayer, self).build(
            input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, **kwargs):

        if K.ndim(inputs[0]) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (K.ndim(inputs)))

        embed_list = inputs
        row = []
        col = []
        num_inputs = len(embed_list)
        for i in range(num_inputs - 1):
            for j in range(i + 1, num_inputs):
                row.append(i)
                col.append(j)
        p = tf.concat([embed_list[idx]
                       for idx in row], axis=1)  # batch num_pairs k
        # Reshape([num_pairs, self.embedding_size])
        q = tf.concat([embed_list[idx] for idx in col], axis=1)

        # -------------------------
        if self.kernel_type == 'mat':
            p = tf.expand_dims(p, 1)
            # k     k* pair* k
            # batch * pair
            kp = reduce_sum(

                # batch * pair * k

                tf.multiply(

                    # batch * pair * k

                    tf.transpose(

                        # batch * k * pair

                        reduce_sum(

                            # batch * k * pair * k

                            tf.multiply(

                                p, self.kernel),

                            -1),

                        [0, 2, 1]),

                    q),

                -1)
        else:
            # 1 * pair * (k or 1)

            k = tf.expand_dims(self.kernel, 0)

            # batch * pair

            kp = reduce_sum(p * q * k, -1)

            # p q # b * p * k

        return kp

    def compute_output_shape(self, input_shape):
        num_inputs = len(input_shape)
        num_pairs = int(num_inputs * (num_inputs - 1) / 2)
        return (None, num_pairs)

    def get_config(self, ):
        config = {'kernel_type': self.kernel_type, 'seed': self.seed}
        base_config = super(OutterProductLayer, self).get_config()
        base_config.update(config)
        return base_config


class FGCNNLayer(Layer):
    """"""Feature Generation Layer used in FGCNN,including Convolution,MaxPooling and Recombination.

      Input shape
        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.

      Output shape
        - 3D tensor with shape: ``(batch_size,new_feture_num,embedding_size)``.

      References
        - [Liu B, Tang R, Chen Y, et al. Feature Generation by Convolutional Neural Network for Click-Through Rate Prediction[J]. arXiv preprint arXiv:1904.04447, 2019.](https://arxiv.org/pdf/1904.04447)

    """"""

    def __init__(self, filters=(14, 16,), kernel_width=(7, 7,), new_maps=(3, 3,), pooling_width=(2, 2),
                 **kwargs):
        if not (len(filters) == len(kernel_width) == len(new_maps) == len(pooling_width)):
            raise ValueError(""length of argument must be equal"")
        self.filters = filters
        self.kernel_width = kernel_width
        self.new_maps = new_maps
        self.pooling_width = pooling_width

        super(FGCNNLayer, self).__init__(**kwargs)

    def build(self, input_shape):

        if len(input_shape) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (len(input_shape)))
        self.conv_layers = []
        self.pooling_layers = []
        self.dense_layers = []
        pooling_shape = input_shape.as_list() + [1, ]
        embedding_size = int(input_shape[-1])
        for i in range(1, len(self.filters) + 1):
            filters = self.filters[i - 1]
            width = self.kernel_width[i - 1]
            new_filters = self.new_maps[i - 1]
            pooling_width = self.pooling_width[i - 1]
            conv_output_shape = self._conv_output_shape(
                pooling_shape, (width, 1))
            pooling_shape = self._pooling_output_shape(
                conv_output_shape, (pooling_width, 1))
            self.conv_layers.append(Conv2D(filters=filters, kernel_size=(width, 1), strides=(1, 1),
                                           padding='same',
                                           activation='tanh', use_bias=True, ))
            self.pooling_layers.append(
                MaxPooling2D(pool_size=(pooling_width, 1)))
            self.dense_layers.append(Dense(pooling_shape[1] * embedding_size * new_filters,
                                           activation='tanh', use_bias=True))

        self.flatten = Flatten()

        super(FGCNNLayer, self).build(
            input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, **kwargs):

        if K.ndim(inputs) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (K.ndim(inputs)))

        embedding_size = int(inputs.shape[-1])
        pooling_result = tf.expand_dims(inputs, axis=3)

        new_feature_list = []

        for i in range(1, len(self.filters) + 1):
            new_filters = self.new_maps[i - 1]

            conv_result = self.conv_layers[i - 1](pooling_result)

            pooling_result = self.pooling_layers[i - 1](conv_result)

            flatten_result = self.flatten(pooling_result)

            new_result = self.dense_layers[i - 1](flatten_result)

            new_feature_list.append(
                tf.reshape(new_result, (-1, int(pooling_result.shape[1]) * new_filters, embedding_size)))

        new_features = concat_func(new_feature_list, axis=1)
        return new_features

    def compute_output_shape(self, input_shape):

        new_features_num = 0
        features_num = input_shape[1]

        for i in range(0, len(self.kernel_width)):
            pooled_features_num = features_num // self.pooling_width[i]
            new_features_num += self.new_maps[i] * pooled_features_num
            features_num = pooled_features_num

        return (None, new_features_num, input_shape[-1])

    def get_config(self, ):
        config = {'kernel_width': self.kernel_width, 'filters': self.filters, 'new_maps': self.new_maps,
                  'pooling_width': self.pooling_width}
        base_config = super(FGCNNLayer, self).get_config()
        base_config.update(config)
        return base_config

    def _conv_output_shape(self, input_shape, kernel_size):
        # channels_last
        space = input_shape[1:-1]
        new_space = []
        for i in range(len(space)):
            new_dim = utils.conv_output_length(
                space[i],
                kernel_size[i],
                padding='same',
                stride=1,
                dilation=1)
            new_space.append(new_dim)
        return ([input_shape[0]] + new_space + [self.filters])

    def _pooling_output_shape(self, input_shape, pool_size):
        # channels_last

        rows = input_shape[1]
        cols = input_shape[2]
        rows = utils.conv_output_length(rows, pool_size[0], 'valid',
                                        pool_size[0])
        cols = utils.conv_output_length(cols, pool_size[1], 'valid',
                                        pool_size[1])
        return [input_shape[0], rows, cols, input_shape[3]]


class SENETLayer(Layer):
    """"""SENETLayer used in FiBiNET.

      Input shape
        - A list of 3D tensor with shape: ``(batch_size,1,embedding_size)``.

      Output shape
        - A list of 3D tensor with shape: ``(batch_size,1,embedding_size)``.

      Arguments
        - **reduction_ratio** : Positive integer, dimensionality of the
         attention network output space.

        - **seed** : A Python integer to use as random seed.

      References
        - [FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction](https://arxiv.org/pdf/1905.09433.pdf)
    """"""

    def __init__(self, reduction_ratio=3, seed=1024, **kwargs):
        self.reduction_ratio = reduction_ratio

        self.seed = seed
        super(SENETLayer, self).__init__(**kwargs)

    def build(self, input_shape):

        if not isinstance(input_shape, list) or len(input_shape) < 2:
            raise ValueError('A `AttentionalFM` layer should be called '
                             'on a list of at least 2 inputs')

        self.filed_size = len(input_shape)
        self.embedding_size = input_shape[0][-1]
        reduction_size = max(1, self.filed_size // self.reduction_ratio)

        self.W_1 = self.add_weight(shape=(
            self.filed_size, reduction_size), initializer=glorot_normal(seed=self.seed), name=""W_1"")
        self.W_2 = self.add_weight(shape=(
            reduction_size, self.filed_size), initializer=glorot_normal(seed=self.seed), name=""W_2"")

        self.tensordot = Lambda(
            lambda x: tf.tensordot(x[0], x[1], axes=(-1, 0)))

        # Be sure to call this somewhere!
        super(SENETLayer, self).build(input_shape)

    def call(self, inputs, training=None, **kwargs):

        if K.ndim(inputs[0]) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (K.ndim(inputs)))

        inputs = concat_func(inputs, axis=1)
        Z = reduce_mean(inputs, axis=-1, )

        A_1 = tf.nn.relu(self.tensordot([Z, self.W_1]))
        A_2 = tf.nn.relu(self.tensordot([A_1, self.W_2]))
        V = tf.multiply(inputs, tf.expand_dims(A_2, axis=2))

        return tf.split(V, self.filed_size, axis=1)

    def compute_output_shape(self, input_shape):

        return input_shape

    def compute_mask(self, inputs, mask=None):
        return [None] * self.filed_size

    def get_config(self, ):
        config = {'reduction_ratio': self.reduction_ratio, 'seed': self.seed}
        base_config = super(SENETLayer, self).get_config()
        base_config.update(config)
        return base_config


class BilinearInteraction(Layer):
    """"""BilinearInteraction Layer used in FiBiNET.

      Input shape
        - A list of 3D tensor with shape: ``(batch_size,1,embedding_size)``. Its length is ``filed_size``.

      Output shape
        - 3D tensor with shape: ``(batch_size,filed_size*(filed_size-1)/2,embedding_size)``.

      Arguments
        - **bilinear_type** : String, types of bilinear functions used in this layer.

        - **seed** : A Python integer to use as random seed.

      References
        - [FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction](https://arxiv.org/pdf/1905.09433.pdf)

    """"""

    def __init__(self, bilinear_type=""interaction"", seed=1024, **kwargs):
        self.bilinear_type = bilinear_type
        self.seed = seed

        super(BilinearInteraction, self).__init__(**kwargs)

    def build(self, input_shape):

        if not isinstance(input_shape, list) or len(input_shape) < 2:
            raise ValueError('A `AttentionalFM` layer should be called '
                             'on a list of at least 2 inputs')
        embedding_size = int(input_shape[0][-1])

        if self.bilinear_type == ""all"":
            self.W = self.add_weight(shape=(embedding_size, embedding_size), initializer=glorot_normal(
                seed=self.seed), name=""bilinear_weight"")
        elif self.bilinear_type == ""each"":
            self.W_list = [self.add_weight(shape=(embedding_size, embedding_size), initializer=glorot_normal(
                seed=self.seed), name=""bilinear_weight"" + str(i)) for i in range(len(input_shape) - 1)]
        elif self.bilinear_type == ""interaction"":
            self.W_list = [self.add_weight(shape=(embedding_size, embedding_size), initializer=glorot_normal(
                seed=self.seed), name=""bilinear_weight"" + str(i) + '_' + str(j)) for i, j in
                           itertools.combinations(range(len(input_shape)), 2)]
        else:
            raise NotImplementedError

        super(BilinearInteraction, self).build(
            input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, **kwargs):

        if K.ndim(inputs[0]) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (K.ndim(inputs)))

        n = len(inputs)
        if self.bilinear_type == ""all"":
            vidots = [tf.tensordot(inputs[i], self.W, axes=(-1, 0)) for i in range(n)]
            p = [tf.multiply(vidots[i], inputs[j]) for i, j in itertools.combinations(range(n), 2)]
        elif self.bilinear_type == ""each"":
            vidots = [tf.tensordot(inputs[i], self.W_list[i], axes=(-1, 0)) for i in range(n - 1)]
            p = [tf.multiply(vidots[i], inputs[j]) for i, j in itertools.combinations(range(n), 2)]
        elif self.bilinear_type == ""interaction"":
            p = [tf.multiply(tf.tensordot(v[0], w, axes=(-1, 0)), v[1])
                 for v, w in zip(itertools.combinations(inputs, 2), self.W_list)]
        else:
            raise NotImplementedError
        output = concat_func(p, axis=1)
        return output

    def compute_output_shape(self, input_shape):
        filed_size = len(input_shape)
        embedding_size = input_shape[0][-1]

        return (None, filed_size * (filed_size - 1) // 2, embedding_size)

    def get_config(self, ):
        config = {'bilinear_type': self.bilinear_type, 'seed': self.seed}
        base_config = super(BilinearInteraction, self).get_config()
        base_config.update(config)
        return base_config


class FieldWiseBiInteraction(Layer):
    """"""Field-Wise Bi-Interaction Layer used in FLEN,compress the
     pairwise element-wise product of features into one single vector.

      Input shape
        - A list of 3D tensor with shape:``(batch_size,field_size,embedding_size)``.

      Output shape
        - 2D tensor with shape: ``(batch_size,embedding_size)``.

      Arguments
        - **use_bias** : Boolean, if use bias.
        - **seed** : A Python integer to use as random seed.

      References
        - [FLEN: Leveraging Field for Scalable CTR Prediction](https://arxiv.org/pdf/1911.04690)

    """"""

    def __init__(self, use_bias=True, seed=1024, **kwargs):
        self.use_bias = use_bias
        self.seed = seed

        super(FieldWiseBiInteraction, self).__init__(**kwargs)

    def build(self, input_shape):

        if not isinstance(input_shape, list) or len(input_shape) < 2:
            raise ValueError(
                'A `Field-Wise Bi-Interaction` layer should be called '
                'on a list of at least 2 inputs')

        self.num_fields = len(input_shape)
        embedding_size = input_shape[0][-1]

        self.kernel_mf = self.add_weight(
            name='kernel_mf',
            shape=(int(self.num_fields * (self.num_fields - 1) / 2), 1),
            initializer=Ones(),
            regularizer=None,
            trainable=True)

        self.kernel_fm = self.add_weight(
            name='kernel_fm',
            shape=(self.num_fields, 1),
            initializer=Constant(value=0.5),
            regularizer=None,
            trainable=True)
        if self.use_bias:
            self.bias_mf = self.add_weight(name='bias_mf',
                                           shape=(embedding_size),
                                           initializer=Zeros())
            self.bias_fm = self.add_weight(name='bias_fm',
                                           shape=(embedding_size),
                                           initializer=Zeros())

        super(FieldWiseBiInteraction,
              self).build(input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, **kwargs):

        if K.ndim(inputs[0]) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" %
                (K.ndim(inputs)))

        field_wise_embeds_list = inputs

        # MF module
        field_wise_vectors = tf.concat([
            reduce_sum(field_i_vectors, axis=1, keep_dims=True)
            for field_i_vectors in field_wise_embeds_list
        ], 1)

        left = []
        right = []

        for i, j in itertools.combinations(list(range(self.num_fields)), 2):
            left.append(i)
            right.append(j)

        embeddings_left = tf.gather(params=field_wise_vectors,
                                    indices=left,
                                    axis=1)
        embeddings_right = tf.gather(params=field_wise_vectors,
                                     indices=right,
                                     axis=1)

        embeddings_prod = embeddings_left * embeddings_right
        field_weighted_embedding = embeddings_prod * self.kernel_mf
        h_mf = reduce_sum(field_weighted_embedding, axis=1)
        if self.use_bias:
            h_mf = tf.nn.bias_add(h_mf, self.bias_mf)

        # FM module
        square_of_sum_list = [
            tf.square(reduce_sum(field_i_vectors, axis=1, keep_dims=True))
            for field_i_vectors in field_wise_embeds_list
        ]
        sum_of_square_list = [
            reduce_sum(field_i_vectors * field_i_vectors,
                       axis=1,
                       keep_dims=True)
            for field_i_vectors in field_wise_embeds_list
        ]

        field_fm = tf.concat([
            square_of_sum - sum_of_square for square_of_sum, sum_of_square in
            zip(square_of_sum_list, sum_of_square_list)
        ], 1)

        h_fm = reduce_sum(field_fm * self.kernel_fm, axis=1)
        if self.use_bias:
            h_fm = tf.nn.bias_add(h_fm, self.bias_fm)

        return h_mf + h_fm

    def compute_output_shape(self, input_shape):
        return (None, input_shape[0][-1])

    def get_config(self, ):
        config = {'use_bias': self.use_bias, 'seed': self.seed}
        base_config = super(FieldWiseBiInteraction, self).get_config()
        base_config.update(config)
        return base_config


class FwFMLayer(Layer):
    """"""Field-weighted Factorization Machines

      Input shape
        - 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.

      Output shape
        - 2D tensor with shape: ``(batch_size, 1)``.

      Arguments
        - **num_fields** : integer for number of fields
        - **regularizer** : L2 regularizer weight for the field strength parameters of FwFM

      References
        - [Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising]
        https://arxiv.org/pdf/1806.03514.pdf
    """"""

    def __init__(self, num_fields=4, regularizer=0.000001, **kwargs):
        self.num_fields = num_fields
        self.regularizer = regularizer
        super(FwFMLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        if len(input_shape) != 3:
            raise ValueError(""Unexpected inputs dimensions % d,\
                             expect to be 3 dimensions"" % (len(input_shape)))

        if input_shape[1] != self.num_fields:
            raise ValueError(""Mismatch in number of fields {} and \
                 concatenated embeddings dims {}"".format(self.num_fields, input_shape[1]))

        self.field_strengths = self.add_weight(name='field_pair_strengths',
                                               shape=(self.num_fields, self.num_fields),
                                               initializer=TruncatedNormal(),
                                               regularizer=l2(self.regularizer),
                                               trainable=True)

        super(FwFMLayer, self).build(input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, **kwargs):
        if K.ndim(inputs) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions""
                % (K.ndim(inputs)))

        if inputs.shape[1] != self.num_fields:
            raise ValueError(""Mismatch in number of fields {} and \
                 concatenated embeddings dims {}"".format(self.num_fields, inputs.shape[1]))

        pairwise_inner_prods = []
        for fi, fj in itertools.combinations(range(self.num_fields), 2):
            # get field strength for pair fi and fj
            r_ij = self.field_strengths[fi, fj]

            # get embeddings for the features of both the fields
            feat_embed_i = tf.squeeze(inputs[0:, fi:fi + 1, 0:], axis=1)
            feat_embed_j = tf.squeeze(inputs[0:, fj:fj + 1, 0:], axis=1)

            f = tf.scalar_mul(r_ij, batch_dot(feat_embed_i, feat_embed_j, axes=1))
            pairwise_inner_prods.append(f)

        sum_ = tf.add_n(pairwise_inner_prods)
        return sum_

    def compute_output_shape(self, input_shape):
        return (None, 1)

    def get_config(self):
        config = super(FwFMLayer, self).get_config().copy()
        config.update({
            'num_fields': self.num_fields,
            'regularizer': self.regularizer
        })
        return config


class FEFMLayer(Layer):
    """"""Field-Embedded Factorization Machines

      Input shape
        - 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.

      Output shape
        - 2D tensor with shape:
            ``(batch_size, (num_fields * (num_fields-1))/2)`` # concatenated FEFM interaction embeddings

      Arguments
        - **regularizer** : L2 regularizer weight for the field pair matrix embeddings parameters of FEFM

      References
        - [Field-Embedded Factorization Machines for Click-through Rate Prediction]
         https://arxiv.org/pdf/2009.09931.pdf
    """"""

    def __init__(self, regularizer, **kwargs):
        self.regularizer = regularizer
        super(FEFMLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        if len(input_shape) != 3:
            raise ValueError(""Unexpected inputs dimensions % d,\
                                expect to be 3 dimensions"" % (len(input_shape)))

        self.num_fields = int(input_shape[1])
        embedding_size = int(input_shape[2])

        self.field_embeddings = {}
        for fi, fj in itertools.combinations(range(self.num_fields), 2):
            field_pair_id = str(fi) + ""-"" + str(fj)
            self.field_embeddings[field_pair_id] = self.add_weight(name='field_embeddings' + field_pair_id,
                                                                   shape=(embedding_size, embedding_size),
                                                                   initializer=TruncatedNormal(),
                                                                   regularizer=l2(self.regularizer),
                                                                   trainable=True)

        super(FEFMLayer, self).build(input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, **kwargs):
        if K.ndim(inputs) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions""
                % (K.ndim(inputs)))

        pairwise_inner_prods = []
        for fi, fj in itertools.combinations(range(self.num_fields), 2):
            field_pair_id = str(fi) + ""-"" + str(fj)
            feat_embed_i = tf.squeeze(inputs[0:, fi:fi + 1, 0:], axis=1)
            feat_embed_j = tf.squeeze(inputs[0:, fj:fj + 1, 0:], axis=1)
            field_pair_embed_ij = self.field_embeddings[field_pair_id]

            feat_embed_i_tr = tf.matmul(feat_embed_i, field_pair_embed_ij + tf.transpose(field_pair_embed_ij))

            f = batch_dot(feat_embed_i_tr, feat_embed_j, axes=1)
            pairwise_inner_prods.append(f)

        concat_vec = tf.concat(pairwise_inner_prods, axis=1)
        return concat_vec

    def compute_output_shape(self, input_shape):
        num_fields = int(input_shape[1])
        return (None, (num_fields * (num_fields - 1)) / 2)

    def get_config(self):
        config = super(FEFMLayer, self).get_config().copy()
        config.update({
            'regularizer': self.regularizer,
        })
        return config


class BridgeModule(Layer):
    """"""Bridge Module used in EDCN

      Input shape
        - A list of two 2D tensor with shape: ``(batch_size, units)``.

      Output shape
        - 2D tensor with shape: ``(batch_size, units)``.

    Arguments
        - **bridge_type**: The type of bridge interaction, one of 'pointwise_addition', 'hadamard_product', 'concatenation', 'attention_pooling'

        - **activation**: Activation function to use.

      References
        - [Enhancing Explicit and Implicit Feature Interactions via Information Sharing for Parallel Deep CTR Models.](https://dlp-kdd.github.io/assets/pdf/DLP-KDD_2021_paper_12.pdf)

    """"""

    def __init__(self, bridge_type='hadamard_product', activation='relu', **kwargs):
        self.bridge_type = bridge_type
        self.activation = activation

        super(BridgeModule, self).__init__(**kwargs)

    def build(self, input_shape):
        if not isinstance(input_shape, list) or len(input_shape) < 2:
            raise ValueError(
                'A `BridgeModule` layer should be called '
                'on a list of 2 inputs')

        self.dnn_dim = int(input_shape[0][-1])
        if self.bridge_type == ""concatenation"":
            self.dense = Dense(self.dnn_dim, self.activation)
        elif self.bridge_type == ""attention_pooling"":
            self.dense_x = DNN([self.dnn_dim, self.dnn_dim], self.activation, output_activation='softmax')
            self.dense_h = DNN([self.dnn_dim, self.dnn_dim], self.activation, output_activation='softmax')

        super(BridgeModule, self).build(input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, **kwargs):
        x, h = inputs
        if self.bridge_type == ""pointwise_addition"":
            return x + h
        elif self.bridge_type == ""hadamard_product"":
            return x * h
        elif self.bridge_type == ""concatenation"":
            return self.dense(tf.concat([x, h], axis=-1))
        elif self.bridge_type == ""attention_pooling"":
            a_x = self.dense_x(x)
            a_h = self.dense_h(h)
            return a_x * x + a_h * h

    def compute_output_shape(self, input_shape):
        return (None, self.dnn_dim)

    def get_config(self):
        base_config = super(BridgeModule, self).get_config().copy()
        config = {
            'bridge_type': self.bridge_type,
            'activation': self.activation
        }
        config.update(base_config)
        return config
"
DeepCTR,sequence.py,"# -*- coding:utf-8 -*-
""""""

Author:
    Weichen Shen,weichenswc@163.com

""""""

import numpy as np
import tensorflow as tf
from tensorflow.python.keras import backend as K

try:
    from tensorflow.python.ops.init_ops import TruncatedNormal, Constant, glorot_uniform_initializer as glorot_uniform
except ImportError:
    from tensorflow.python.ops.init_ops_v2 import TruncatedNormal, Constant, glorot_uniform

from tensorflow.python.keras.layers import LSTM, Lambda, Layer, Dropout

from .core import LocalActivationUnit
from .normalization import LayerNormalization

if tf.__version__ >= '2.0.0':
    from ..contrib.rnn_v2 import dynamic_rnn
else:
    from ..contrib.rnn import dynamic_rnn
from ..contrib.utils import QAAttGRUCell, VecAttGRUCell
from .utils import reduce_sum, reduce_max, div, softmax, reduce_mean


class SequencePoolingLayer(Layer):
    """"""The SequencePoolingLayer is used to apply pooling operation(sum,mean,max) on variable-length sequence feature/multi-value feature.

      Input shape
        - A list of two  tensor [seq_value,seq_len]

        - seq_value is a 3D tensor with shape: ``(batch_size, T, embedding_size)``

        - seq_len is a 2D tensor with shape : ``(batch_size, 1)``,indicate valid length of each sequence.

      Output shape
        - 3D tensor with shape: ``(batch_size, 1, embedding_size)``.

      Arguments
        - **mode**:str.Pooling operation to be used,can be sum,mean or max.

        - **supports_masking**:If True,the input need to support masking.
    """"""

    def __init__(self, mode='mean', supports_masking=False, **kwargs):

        if mode not in ['sum', 'mean', 'max']:
            raise ValueError(""mode must be sum or mean"")
        self.mode = mode
        self.eps = tf.constant(1e-8, tf.float32)
        super(SequencePoolingLayer, self).__init__(**kwargs)

        self.supports_masking = supports_masking

    def build(self, input_shape):
        if not self.supports_masking:
            self.seq_len_max = int(input_shape[0][1])
        super(SequencePoolingLayer, self).build(
            input_shape)  # Be sure to call this somewhere!

    def call(self, seq_value_len_list, mask=None, **kwargs):
        if self.supports_masking:
            if mask is None:
                raise ValueError(
                    ""When supports_masking=True,input must support masking"")
            uiseq_embed_list = seq_value_len_list
            mask = tf.cast(mask, tf.float32)  # tf.to_float(mask)
            user_behavior_length = reduce_sum(mask, axis=-1, keep_dims=True)
            mask = tf.expand_dims(mask, axis=2)
        else:
            uiseq_embed_list, user_behavior_length = seq_value_len_list

            mask = tf.sequence_mask(user_behavior_length,
                                    self.seq_len_max, dtype=tf.float32)
            mask = tf.transpose(mask, (0, 2, 1))

        embedding_size = uiseq_embed_list.shape[-1]

        mask = tf.tile(mask, [1, 1, embedding_size])

        if self.mode == ""max"":
            hist = uiseq_embed_list - (1 - mask) * 1e9
            return reduce_max(hist, 1, keep_dims=True)

        hist = reduce_sum(uiseq_embed_list * mask, 1, keep_dims=False)

        if self.mode == ""mean"":
            hist = div(hist, tf.cast(user_behavior_length, tf.float32) + self.eps)

        hist = tf.expand_dims(hist, axis=1)
        return hist

    def compute_output_shape(self, input_shape):
        if self.supports_masking:
            return (None, 1, input_shape[-1])
        else:
            return (None, 1, input_shape[0][-1])

    def compute_mask(self, inputs, mask):
        return None

    def get_config(self, ):
        config = {'mode': self.mode, 'supports_masking': self.supports_masking}
        base_config = super(SequencePoolingLayer, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class WeightedSequenceLayer(Layer):
    """"""The WeightedSequenceLayer is used to apply weight score on variable-length sequence feature/multi-value feature.

      Input shape
        - A list of two  tensor [seq_value,seq_len,seq_weight]

        - seq_value is a 3D tensor with shape: ``(batch_size, T, embedding_size)``

        - seq_len is a 2D tensor with shape : ``(batch_size, 1)``,indicate valid length of each sequence.

        - seq_weight is a 3D tensor with shape: ``(batch_size, T, 1)``

      Output shape
        - 3D tensor with shape: ``(batch_size, T, embedding_size)``.

      Arguments
        - **weight_normalization**: bool.Whether normalize the weight score before applying to sequence.

        - **supports_masking**:If True,the input need to support masking.
    """"""

    def __init__(self, weight_normalization=True, supports_masking=False, **kwargs):
        super(WeightedSequenceLayer, self).__init__(**kwargs)
        self.weight_normalization = weight_normalization
        self.supports_masking = supports_masking

    def build(self, input_shape):
        if not self.supports_masking:
            self.seq_len_max = int(input_shape[0][1])
        super(WeightedSequenceLayer, self).build(
            input_shape)  # Be sure to call this somewhere!

    def call(self, input_list, mask=None, **kwargs):
        if self.supports_masking:
            if mask is None:
                raise ValueError(
                    ""When supports_masking=True,input must support masking"")
            key_input, value_input = input_list
            mask = tf.expand_dims(mask[0], axis=2)
        else:
            key_input, key_length_input, value_input = input_list
            mask = tf.sequence_mask(key_length_input,
                                    self.seq_len_max, dtype=tf.bool)
            mask = tf.transpose(mask, (0, 2, 1))

        embedding_size = key_input.shape[-1]

        if self.weight_normalization:
            paddings = tf.ones_like(value_input) * (-2 ** 32 + 1)
        else:
            paddings = tf.zeros_like(value_input)
        value_input = tf.where(mask, value_input, paddings)

        if self.weight_normalization:
            value_input = softmax(value_input, dim=1)

        if len(value_input.shape) == 2:
            value_input = tf.expand_dims(value_input, axis=2)
            value_input = tf.tile(value_input, [1, 1, embedding_size])

        return tf.multiply(key_input, value_input)

    def compute_output_shape(self, input_shape):
        return input_shape[0]

    def compute_mask(self, inputs, mask):
        if self.supports_masking:
            return mask[0]
        else:
            return None

    def get_config(self, ):
        config = {'weight_normalization': self.weight_normalization, 'supports_masking': self.supports_masking}
        base_config = super(WeightedSequenceLayer, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class AttentionSequencePoolingLayer(Layer):
    """"""The Attentional sequence pooling operation used in DIN.

      Input shape
        - A list of three tensor: [query,keys,keys_length]

        - query is a 3D tensor with shape:  ``(batch_size, 1, embedding_size)``

        - keys is a 3D tensor with shape:   ``(batch_size, T, embedding_size)``

        - keys_length is a 2D tensor with shape: ``(batch_size, 1)``

      Output shape
        - 3D tensor with shape: ``(batch_size, 1, embedding_size)``.

      Arguments
        - **att_hidden_units**:list of positive integer, the attention net layer number and units in each layer.

        - **att_activation**: Activation function to use in attention net.

        - **weight_normalization**: bool.Whether normalize the attention score of local activation unit.

        - **supports_masking**:If True,the input need to support masking.

      References
        - [Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068.](https://arxiv.org/pdf/1706.06978.pdf)
    """"""

    def __init__(self, att_hidden_units=(80, 40), att_activation='sigmoid', weight_normalization=False,
                 return_score=False,
                 supports_masking=False, **kwargs):

        self.att_hidden_units = att_hidden_units
        self.att_activation = att_activation
        self.weight_normalization = weight_normalization
        self.return_score = return_score
        super(AttentionSequencePoolingLayer, self).__init__(**kwargs)
        self.supports_masking = supports_masking

    def build(self, input_shape):
        if not self.supports_masking:
            if not isinstance(input_shape, list) or len(input_shape) != 3:
                raise ValueError('A `AttentionSequencePoolingLayer` layer should be called '
                                 'on a list of 3 inputs')

            if len(input_shape[0]) != 3 or len(input_shape[1]) != 3 or len(input_shape[2]) != 2:
                raise ValueError(
                    ""Unexpected inputs dimensions,the 3 tensor dimensions are %d,%d and %d , expect to be 3,3 and 2"" % (
                        len(input_shape[0]), len(input_shape[1]), len(input_shape[2])))

            if input_shape[0][-1] != input_shape[1][-1] or input_shape[0][1] != 1 or input_shape[2][1] != 1:
                raise ValueError('A `AttentionSequencePoolingLayer` layer requires '
                                 'inputs of a 3 tensor with shape (None,1,embedding_size),(None,T,embedding_size) and (None,1)'
                                 'Got different shapes: %s' % (input_shape))
        else:
            pass
        self.local_att = LocalActivationUnit(
            self.att_hidden_units, self.att_activation, l2_reg=0, dropout_rate=0, use_bn=False, seed=1024, )
        super(AttentionSequencePoolingLayer, self).build(
            input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, mask=None, training=None, **kwargs):

        if self.supports_masking:
            if mask is None:
                raise ValueError(
                    ""When supports_masking=True,input must support masking"")
            queries, keys = inputs
            key_masks = tf.expand_dims(mask[-1], axis=1)

        else:

            queries, keys, keys_length = inputs
            hist_len = keys.get_shape()[1]
            key_masks = tf.sequence_mask(keys_length, hist_len)

        attention_score = self.local_att([queries, keys], training=training)

        outputs = tf.transpose(attention_score, (0, 2, 1))

        if self.weight_normalization:
            paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)
        else:
            paddings = tf.zeros_like(outputs)

        outputs = tf.where(key_masks, outputs, paddings)

        if self.weight_normalization:
            outputs = softmax(outputs)

        if not self.return_score:
            outputs = tf.matmul(outputs, keys)

        if tf.__version__ < '1.13.0':
            outputs._uses_learning_phase = attention_score._uses_learning_phase
        else:
            outputs._uses_learning_phase = training is not None

        return outputs

    def compute_output_shape(self, input_shape):
        if self.return_score:
            return (None, 1, input_shape[1][1])
        else:
            return (None, 1, input_shape[0][-1])

    def compute_mask(self, inputs, mask):
        return None

    def get_config(self, ):

        config = {'att_hidden_units': self.att_hidden_units, 'att_activation': self.att_activation,
                  'weight_normalization': self.weight_normalization, 'return_score': self.return_score,
                  'supports_masking': self.supports_masking}
        base_config = super(AttentionSequencePoolingLayer, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class BiLSTM(Layer):
    """"""A multiple layer Bidirectional Residual LSTM Layer.

      Input shape
        - 3D tensor with shape ``(batch_size, timesteps, input_dim)``.

      Output shape
        - 3D tensor with shape: ``(batch_size, timesteps, units)``.

      Arguments
        - **units**: Positive integer, dimensionality of the output space.

        - **layers**:Positive integer, number of LSTM layers to stacked.

        - **res_layers**: Positive integer, number of residual connection to used in last ``res_layers``.

        - **dropout_rate**:  Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.

        - **merge_mode**: merge_mode: Mode by which outputs of the forward and backward RNNs will be combined. One of { ``'fw'`` , ``'bw'`` , ``'sum'`` , ``'mul'`` , ``'concat'`` , ``'ave'`` , ``None`` }. If None, the outputs will not be combined, they will be returned as a list.


    """"""

    def __init__(self, units, layers=2, res_layers=0, dropout_rate=0.2, merge_mode='ave', **kwargs):

        if merge_mode not in ['fw', 'bw', 'sum', 'mul', 'ave', 'concat', None]:
            raise ValueError('Invalid merge mode. '
                             'Merge mode should be one of '
                             '{""fw"",""bw"",""sum"", ""mul"", ""ave"", ""concat"", None}')

        self.units = units
        self.layers = layers
        self.res_layers = res_layers
        self.dropout_rate = dropout_rate
        self.merge_mode = merge_mode

        super(BiLSTM, self).__init__(**kwargs)
        self.supports_masking = True

    def build(self, input_shape):

        if len(input_shape) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (len(input_shape)))
        self.fw_lstm = []
        self.bw_lstm = []
        for _ in range(self.layers):
            self.fw_lstm.append(
                LSTM(self.units, dropout=self.dropout_rate, bias_initializer='ones', return_sequences=True,
                     unroll=True))
            self.bw_lstm.append(
                LSTM(self.units, dropout=self.dropout_rate, bias_initializer='ones', return_sequences=True,
                     go_backwards=True, unroll=True))

        super(BiLSTM, self).build(
            input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, mask=None, **kwargs):

        input_fw = inputs
        input_bw = inputs
        for i in range(self.layers):
            output_fw = self.fw_lstm[i](input_fw)
            output_bw = self.bw_lstm[i](input_bw)
            output_bw = Lambda(lambda x: K.reverse(
                x, 1), mask=lambda inputs, mask: mask)(output_bw)

            if i >= self.layers - self.res_layers:
                output_fw += input_fw
                output_bw += input_bw
            input_fw = output_fw
            input_bw = output_bw

        output_fw = input_fw
        output_bw = input_bw

        if self.merge_mode == ""fw"":
            output = output_fw
        elif self.merge_mode == ""bw"":
            output = output_bw
        elif self.merge_mode == 'concat':
            output = tf.concat([output_fw, output_bw], axis=-1)
        elif self.merge_mode == 'sum':
            output = output_fw + output_bw
        elif self.merge_mode == 'ave':
            output = (output_fw + output_bw) / 2
        elif self.merge_mode == 'mul':
            output = output_fw * output_bw
        elif self.merge_mode is None:
            output = [output_fw, output_bw]

        return output

    def compute_output_shape(self, input_shape):
        print(self.merge_mode)
        if self.merge_mode is None:
            return [input_shape, input_shape]
        elif self.merge_mode == 'concat':
            return input_shape[:-1] + (input_shape[-1] * 2,)
        else:
            return input_shape

    def compute_mask(self, inputs, mask):
        return mask

    def get_config(self, ):

        config = {'units': self.units, 'layers': self.layers,
                  'res_layers': self.res_layers, 'dropout_rate': self.dropout_rate, 'merge_mode': self.merge_mode}
        base_config = super(BiLSTM, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class Transformer(Layer):
    """"""  Simplified version of Transformer  proposed in 《Attention is all you need》

      Input shape
        - a list of two 3D tensor with shape ``(batch_size, timesteps, input_dim)`` if ``supports_masking=True`` .
        - a list of two 4 tensors, first two tensors with shape ``(batch_size, timesteps, input_dim)``,last two tensors with shape ``(batch_size, 1)`` if ``supports_masking=False`` .


      Output shape
        - 3D tensor with shape: ``(batch_size, 1, input_dim)``  if ``output_type='mean'`` or ``output_type='sum'`` , else  ``(batch_size, timesteps, input_dim)`` .


      Arguments
            - **att_embedding_size**: int.The embedding size in multi-head self-attention network.
            - **head_num**: int.The head number in multi-head  self-attention network.
            - **dropout_rate**: float between 0 and 1. Fraction of the units to drop.
            - **use_positional_encoding**: bool. Whether or not use positional_encoding
            - **use_res**: bool. Whether or not use standard residual connections before output.
            - **use_feed_forward**: bool. Whether or not use pointwise feed foward network.
            - **use_layer_norm**: bool. Whether or not use Layer Normalization.
            - **blinding**: bool. Whether or not use blinding.
            - **seed**: A Python integer to use as random seed.
            - **supports_masking**:bool. Whether or not support masking.
            - **attention_type**: str, Type of attention, the value must be one of { ``'scaled_dot_product'`` , ``'cos'`` , ``'ln'`` , ``'additive'`` }.
            - **output_type**: ``'mean'`` , ``'sum'`` or `None`. Whether or not use average/sum pooling for output.

      References
            - [Vaswani, Ashish, et al. ""Attention is all you need."" Advances in Neural Information Processing Systems. 2017.](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
    """"""

    def __init__(self, att_embedding_size=1, head_num=8, dropout_rate=0.0, use_positional_encoding=True, use_res=True,
                 use_feed_forward=True, use_layer_norm=False, blinding=True, seed=1024, supports_masking=False,
                 attention_type=""scaled_dot_product"", output_type=""mean"", **kwargs):
        if head_num <= 0:
            raise ValueError('head_num must be a int > 0')
        self.att_embedding_size = att_embedding_size
        self.head_num = head_num
        self.num_units = att_embedding_size * head_num
        self.use_res = use_res
        self.use_feed_forward = use_feed_forward
        self.seed = seed
        self.use_positional_encoding = use_positional_encoding
        self.dropout_rate = dropout_rate
        self.use_layer_norm = use_layer_norm
        self.blinding = blinding
        self.attention_type = attention_type
        self.output_type = output_type
        super(Transformer, self).__init__(**kwargs)
        self.supports_masking = supports_masking

    def build(self, input_shape):
        embedding_size = int(input_shape[0][-1])
        if self.num_units != embedding_size:
            raise ValueError(
                ""att_embedding_size * head_num must equal the last dimension size of inputs,got %d * %d != %d"" % (
                    self.att_embedding_size, self.head_num, embedding_size))
        self.seq_len_max = int(input_shape[0][-2])
        self.W_Query = self.add_weight(name='query', shape=[embedding_size, self.att_embedding_size * self.head_num],
                                       dtype=tf.float32,
                                       initializer=TruncatedNormal(seed=self.seed))
        self.W_key = self.add_weight(name='key', shape=[embedding_size, self.att_embedding_size * self.head_num],
                                     dtype=tf.float32,
                                     initializer=TruncatedNormal(seed=self.seed + 1))
        self.W_Value = self.add_weight(name='value', shape=[embedding_size, self.att_embedding_size * self.head_num],
                                       dtype=tf.float32,
                                       initializer=TruncatedNormal(seed=self.seed + 2))
        if self.attention_type == ""additive"":
            self.b = self.add_weight('b', shape=[self.att_embedding_size], dtype=tf.float32,
                                     initializer=glorot_uniform(seed=self.seed))
            self.v = self.add_weight('v', shape=[self.att_embedding_size], dtype=tf.float32,
                                     initializer=glorot_uniform(seed=self.seed))
        elif self.attention_type == ""ln"":
            self.att_ln_q = LayerNormalization()
            self.att_ln_k = LayerNormalization()
        # if self.use_res:
        #     self.W_Res = self.add_weight(name='res', shape=[embedding_size, self.att_embedding_size * self.head_num], dtype=tf.float32,
        #                                  initializer=TruncatedNormal(seed=self.seed))
        if self.use_feed_forward:
            self.fw1 = self.add_weight('fw1', shape=[self.num_units, 4 * self.num_units], dtype=tf.float32,
                                       initializer=glorot_uniform(seed=self.seed))
            self.fw2 = self.add_weight('fw2', shape=[4 * self.num_units, self.num_units], dtype=tf.float32,
                                       initializer=glorot_uniform(seed=self.seed))

        self.dropout = Dropout(
            self.dropout_rate, seed=self.seed)
        self.ln = LayerNormalization()
        if self.use_positional_encoding:
            self.query_pe = PositionEncoding()
            self.key_pe = PositionEncoding()
        # Be sure to call this somewhere!
        super(Transformer, self).build(input_shape)

    def call(self, inputs, mask=None, training=None, **kwargs):

        if self.supports_masking:
            queries, keys = inputs
            query_masks, key_masks = mask
            query_masks = tf.cast(query_masks, tf.float32)
            key_masks = tf.cast(key_masks, tf.float32)
        else:
            queries, keys, query_masks, key_masks = inputs

            query_masks = tf.sequence_mask(
                query_masks, self.seq_len_max, dtype=tf.float32)
            key_masks = tf.sequence_mask(
                key_masks, self.seq_len_max, dtype=tf.float32)
            query_masks = tf.squeeze(query_masks, axis=1)
            key_masks = tf.squeeze(key_masks, axis=1)

        if self.use_positional_encoding:
            queries = self.query_pe(queries)
            keys = self.key_pe(keys)

        Q = tf.tensordot(queries, self.W_Query,
                         axes=(-1, 0))  # N T_q D*h
        K = tf.tensordot(keys, self.W_key, axes=(-1, 0))
        V = tf.tensordot(keys, self.W_Value, axes=(-1, 0))

        # h*N T_q D
        Q_ = tf.concat(tf.split(Q, self.head_num, axis=2), axis=0)
        K_ = tf.concat(tf.split(K, self.head_num, axis=2), axis=0)
        V_ = tf.concat(tf.split(V, self.head_num, axis=2), axis=0)

        if self.attention_type == ""scaled_dot_product"":
            # h*N T_q T_k
            outputs = tf.matmul(Q_, K_, transpose_b=True)

            outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)
        elif self.attention_type == ""cos"":
            Q_cos = tf.nn.l2_normalize(Q_, dim=-1)
            K_cos = tf.nn.l2_normalize(K_, dim=-1)

            outputs = tf.matmul(Q_cos, K_cos, transpose_b=True)  # h*N T_q T_k

            outputs = outputs * 20  # Scale
        elif self.attention_type == 'ln':
            Q_ = self.att_ln_q(Q_)
            K_ = self.att_ln_k(K_)

            outputs = tf.matmul(Q_, K_, transpose_b=True)  # h*N T_q T_k
            # Scale
            outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)
        elif self.attention_type == ""additive"":
            Q_reshaped = tf.expand_dims(Q_, axis=-2)
            K_reshaped = tf.expand_dims(K_, axis=-3)
            outputs = tf.tanh(tf.nn.bias_add(Q_reshaped + K_reshaped, self.b))
            outputs = tf.squeeze(tf.tensordot(outputs, tf.expand_dims(self.v, axis=-1), axes=[-1, 0]), axis=-1)
        else:
            raise ValueError(""attention_type must be [scaled_dot_product,cos,ln,additive]"")

        key_masks = tf.tile(key_masks, [self.head_num, 1])

        # (h*N, T_q, T_k)
        key_masks = tf.tile(tf.expand_dims(key_masks, 1),
                            [1, tf.shape(queries)[1], 1])

        paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)

        # (h*N, T_q, T_k)

        outputs = tf.where(tf.equal(key_masks, 1), outputs, paddings, )
        if self.blinding:
            try:
                outputs = tf.matrix_set_diag(outputs, tf.ones_like(outputs)[
                                                      :, :, 0] * (-2 ** 32 + 1))
            except AttributeError:
                outputs = tf.compat.v1.matrix_set_diag(outputs, tf.ones_like(outputs)[
                                                                :, :, 0] * (-2 ** 32 + 1))

        outputs -= reduce_max(outputs, axis=-1, keep_dims=True)
        outputs = softmax(outputs)
        query_masks = tf.tile(query_masks, [self.head_num, 1])  # (h*N, T_q)
        # (h*N, T_q, T_k)
        query_masks = tf.tile(tf.expand_dims(
            query_masks, -1), [1, 1, tf.shape(keys)[1]])

        outputs *= query_masks

        outputs = self.dropout(outputs, training=training)
        # Weighted sum
        # ( h*N, T_q, C/h)
        result = tf.matmul(outputs, V_)
        result = tf.concat(tf.split(result, self.head_num, axis=0), axis=2)

        if self.use_res:
            # tf.tensordot(queries, self.W_Res, axes=(-1, 0))
            result += queries
        if self.use_layer_norm:
            result = self.ln(result)

        if self.use_feed_forward:
            fw1 = tf.nn.relu(tf.tensordot(result, self.fw1, axes=[-1, 0]))
            fw1 = self.dropout(fw1, training=training)
            fw2 = tf.tensordot(fw1, self.fw2, axes=[-1, 0])
            if self.use_res:
                result += fw2
            if self.use_layer_norm:
                result = self.ln(result)

        if self.output_type == ""mean"":
            return reduce_mean(result, axis=1, keep_dims=True)
        elif self.output_type == ""sum"":
            return reduce_sum(result, axis=1, keep_dims=True)
        else:
            return result

    def compute_output_shape(self, input_shape):

        return (None, 1, self.att_embedding_size * self.head_num)

    def compute_mask(self, inputs, mask=None):
        return None

    def get_config(self, ):
        config = {'att_embedding_size': self.att_embedding_size, 'head_num': self.head_num,
                  'dropout_rate': self.dropout_rate, 'use_res': self.use_res,
                  'use_positional_encoding': self.use_positional_encoding, 'use_feed_forward': self.use_feed_forward,
                  'use_layer_norm': self.use_layer_norm, 'seed': self.seed, 'supports_masking': self.supports_masking,
                  'blinding': self.blinding, 'attention_type': self.attention_type, 'output_type': self.output_type}
        base_config = super(Transformer, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class PositionEncoding(Layer):
    def __init__(self, pos_embedding_trainable=True,
                 zero_pad=False,
                 scale=True, **kwargs):
        self.pos_embedding_trainable = pos_embedding_trainable
        self.zero_pad = zero_pad
        self.scale = scale
        super(PositionEncoding, self).__init__(**kwargs)

    def build(self, input_shape):
        # Create a trainable weight variable for this layer.
        _, T, num_units = input_shape.as_list()  # inputs.get_shape().as_list()
        # First part of the PE function: sin and cos argument
        position_enc = np.array([
            [pos / np.power(10000, 2. * (i // 2) / num_units) for i in range(num_units)]
            for pos in range(T)])

        # Second part, apply the cosine to even columns and sin to odds.
        position_enc[:, 0::2] = np.sin(position_enc[:, 0::2])  # dim 2i
        position_enc[:, 1::2] = np.cos(position_enc[:, 1::2])  # dim 2i+1
        if self.zero_pad:
            position_enc[0, :] = np.zeros(num_units)
        self.lookup_table = self.add_weight(""lookup_table"", (T, num_units),
                                            initializer=Constant(position_enc),
                                            trainable=self.pos_embedding_trainable)

        # Be sure to call this somewhere!
        super(PositionEncoding, self).build(input_shape)

    def call(self, inputs, mask=None):
        _, T, num_units = inputs.get_shape().as_list()
        position_ind = tf.expand_dims(tf.range(T), 0)
        outputs = tf.nn.embedding_lookup(self.lookup_table, position_ind)
        if self.scale:
            outputs = outputs * num_units ** 0.5
        return outputs + inputs

    def compute_output_shape(self, input_shape):

        return input_shape

    def compute_mask(self, inputs, mask=None):
        return mask

    def get_config(self, ):

        config = {'pos_embedding_trainable': self.pos_embedding_trainable, 'zero_pad': self.zero_pad,
                  'scale': self.scale}
        base_config = super(PositionEncoding, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class BiasEncoding(Layer):
    def __init__(self, sess_max_count, seed=1024, **kwargs):
        self.sess_max_count = sess_max_count
        self.seed = seed
        super(BiasEncoding, self).__init__(**kwargs)

    def build(self, input_shape):
        # Create a trainable weight variable for this layer.

        if self.sess_max_count == 1:
            embed_size = input_shape[2].value
            seq_len_max = input_shape[1].value
        else:
            try:
                embed_size = input_shape[0][2].value
                seq_len_max = input_shape[0][1].value
            except AttributeError:
                embed_size = input_shape[0][2]
                seq_len_max = input_shape[0][1]

        self.sess_bias_embedding = self.add_weight('sess_bias_embedding', shape=(self.sess_max_count, 1, 1),
                                                   initializer=TruncatedNormal(
                                                       mean=0.0, stddev=0.0001, seed=self.seed))
        self.seq_bias_embedding = self.add_weight('seq_bias_embedding', shape=(1, seq_len_max, 1),
                                                  initializer=TruncatedNormal(
                                                      mean=0.0, stddev=0.0001, seed=self.seed))
        self.item_bias_embedding = self.add_weight('item_bias_embedding', shape=(1, 1, embed_size),
                                                   initializer=TruncatedNormal(
                                                       mean=0.0, stddev=0.0001, seed=self.seed))

        # Be sure to call this somewhere!
        super(BiasEncoding, self).build(input_shape)

    def call(self, inputs, mask=None):
        """"""
        :param concated_embeds_value: None * field_size * embedding_size
        :return: None*1
        """"""
        transformer_out = []
        for i in range(self.sess_max_count):
            transformer_out.append(
                inputs[i] + self.item_bias_embedding + self.seq_bias_embedding + self.sess_bias_embedding[i])
        return transformer_out

    def compute_output_shape(self, input_shape):

        return input_shape

    def compute_mask(self, inputs, mask=None):
        return mask

    def get_config(self, ):

        config = {'sess_max_count': self.sess_max_count, 'seed': self.seed, }
        base_config = super(BiasEncoding, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class DynamicGRU(Layer):
    def __init__(self, num_units=None, gru_type='GRU', return_sequence=True, **kwargs):

        self.num_units = num_units
        self.return_sequence = return_sequence
        self.gru_type = gru_type
        super(DynamicGRU, self).__init__(**kwargs)

    def build(self, input_shape):
        # Create a trainable weight variable for this layer.
        input_seq_shape = input_shape[0]
        if self.num_units is None:
            self.num_units = input_seq_shape.as_list()[-1]
        if self.gru_type == ""AGRU"":
            self.gru_cell = QAAttGRUCell(self.num_units)
        elif self.gru_type == ""AUGRU"":
            self.gru_cell = VecAttGRUCell(self.num_units)
        else:
            try:
                self.gru_cell = tf.nn.rnn_cell.GRUCell(self.num_units)  # GRUCell
            except AttributeError:
                self.gru_cell = tf.compat.v1.nn.rnn_cell.GRUCell(self.num_units)

        # Be sure to call this somewhere!
        super(DynamicGRU, self).build(input_shape)

    def call(self, input_list):
        """"""
        :param concated_embeds_value: None * field_size * embedding_size
        :return: None*1
        """"""
        if self.gru_type == ""GRU"" or self.gru_type == ""AIGRU"":
            rnn_input, sequence_length = input_list
            att_score = None
        else:
            rnn_input, sequence_length, att_score = input_list

        rnn_output, hidden_state = dynamic_rnn(self.gru_cell, inputs=rnn_input, att_scores=att_score,
                                               sequence_length=tf.squeeze(sequence_length,
                                                                          ), dtype=tf.float32, scope=self.name)
        if self.return_sequence:
            return rnn_output
        else:
            return tf.expand_dims(hidden_state, axis=1)

    def compute_output_shape(self, input_shape):
        rnn_input_shape = input_shape[0]
        if self.return_sequence:
            return rnn_input_shape
        else:
            return (None, 1, rnn_input_shape[2])

    def get_config(self, ):
        config = {'num_units': self.num_units, 'gru_type': self.gru_type, 'return_sequence': self.return_sequence}
        base_config = super(DynamicGRU, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class KMaxPooling(Layer):
    """"""K Max pooling that selects the k biggest value along the specific axis.

      Input shape
        -  nD tensor with shape: ``(batch_size, ..., input_dim)``.

      Output shape
        - nD tensor with shape: ``(batch_size, ..., output_dim)``.

      Arguments
        - **k**: positive integer, number of top elements to look for along the ``axis`` dimension.

        - **axis**: positive integer, the dimension to look for elements.

     """"""

    def __init__(self, k=1, axis=-1, **kwargs):

        self.k = k
        self.axis = axis
        super(KMaxPooling, self).__init__(**kwargs)

    def build(self, input_shape):

        if self.axis < 1 or self.axis > len(input_shape):
            raise ValueError(""axis must be 1~%d,now is %d"" %
                             (len(input_shape), self.axis))

        if self.k < 1 or self.k > input_shape[self.axis]:
            raise ValueError(""k must be in 1 ~ %d,now k is %d"" %
                             (input_shape[self.axis], self.k))
        self.dims = len(input_shape)
        # Be sure to call this somewhere!
        super(KMaxPooling, self).build(input_shape)

    def call(self, inputs):

        # swap the last and the axis dimensions since top_k will be applied along the last dimension
        perm = list(range(self.dims))
        perm[-1], perm[self.axis] = perm[self.axis], perm[-1]
        shifted_input = tf.transpose(inputs, perm)

        # extract top_k, returns two tensors [values, indices]
        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0]
        output = tf.transpose(top_k, perm)

        return output

    def compute_output_shape(self, input_shape):
        output_shape = list(input_shape)
        output_shape[self.axis] = self.k
        return tuple(output_shape)

    def get_config(self, ):
        config = {'k': self.k, 'axis': self.axis}
        base_config = super(KMaxPooling, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
"
DeepCTR,__init__.py,"import tensorflow as tf

from .activation import Dice
from .core import DNN, LocalActivationUnit, PredictionLayer, RegulationModule
from .interaction import (CIN, FM, AFMLayer, BiInteractionPooling, CrossNet, CrossNetMix,
                          InnerProductLayer, InteractingLayer,
                          OutterProductLayer, FGCNNLayer, SENETLayer, BilinearInteraction,
                          FieldWiseBiInteraction, FwFMLayer, FEFMLayer, BridgeModule)
from .normalization import LayerNormalization
from .sequence import (AttentionSequencePoolingLayer, BiasEncoding, BiLSTM,
                       KMaxPooling, SequencePoolingLayer, WeightedSequenceLayer,
                       Transformer, DynamicGRU, PositionEncoding)
from .utils import NoMask, Hash, Linear, _Add, combined_dnn_input, softmax, reduce_sum, Concat

custom_objects = {'tf': tf,
                  'InnerProductLayer': InnerProductLayer,
                  'OutterProductLayer': OutterProductLayer,
                  'DNN': DNN,
                  'PredictionLayer': PredictionLayer,
                  'FM': FM,
                  'AFMLayer': AFMLayer,
                  'CrossNet': CrossNet,
                  'CrossNetMix': CrossNetMix,
                  'BiInteractionPooling': BiInteractionPooling,
                  'LocalActivationUnit': LocalActivationUnit,
                  'Dice': Dice,
                  'SequencePoolingLayer': SequencePoolingLayer,
                  'AttentionSequencePoolingLayer': AttentionSequencePoolingLayer,
                  'CIN': CIN,
                  'InteractingLayer': InteractingLayer,
                  'LayerNormalization': LayerNormalization,
                  'BiLSTM': BiLSTM,
                  'Transformer': Transformer,
                  'NoMask': NoMask,
                  'BiasEncoding': BiasEncoding,
                  'KMaxPooling': KMaxPooling,
                  'FGCNNLayer': FGCNNLayer,
                  'Hash': Hash,
                  'Linear': Linear,
                  'Concat': Concat,
                  'DynamicGRU': DynamicGRU,
                  'SENETLayer': SENETLayer,
                  'BilinearInteraction': BilinearInteraction,
                  'WeightedSequenceLayer': WeightedSequenceLayer,
                  '_Add': _Add,
                  'FieldWiseBiInteraction': FieldWiseBiInteraction,
                  'FwFMLayer': FwFMLayer,
                  'softmax': softmax,
                  'FEFMLayer': FEFMLayer,
                  'reduce_sum': reduce_sum,
                  'PositionEncoding': PositionEncoding,
                  'RegulationModule': RegulationModule,
                  'BridgeModule': BridgeModule
                  }
"
DeepCTR,core.py,"# -*- coding:utf-8 -*-
""""""

Author:
    Weichen Shen,weichenswc@163.com

""""""

import tensorflow as tf
from tensorflow.python.keras import backend as K

try:
    from tensorflow.python.ops.init_ops_v2 import Zeros, Ones, glorot_normal
except ImportError:
    from tensorflow.python.ops.init_ops import Zeros, Ones, glorot_normal_initializer as glorot_normal

from tensorflow.python.keras.layers import Layer, Dropout

try:
    from tensorflow.python.keras.layers import BatchNormalization
except ImportError:
    BatchNormalization = tf.keras.layers.BatchNormalization
from tensorflow.python.keras.regularizers import l2

from .activation import activation_layer


class LocalActivationUnit(Layer):
    """"""The LocalActivationUnit used in DIN with which the representation of
    user interests varies adaptively given different candidate items.

      Input shape
        - A list of two 3D tensor with shape:  ``(batch_size, 1, embedding_size)`` and ``(batch_size, T, embedding_size)``

      Output shape
        - 3D tensor with shape: ``(batch_size, T, 1)``.

      Arguments
        - **hidden_units**:list of positive integer, the attention net layer number and units in each layer.

        - **activation**: Activation function to use in attention net.

        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix of attention net.

        - **dropout_rate**: float in [0,1). Fraction of the units to dropout in attention net.

        - **use_bn**: bool. Whether use BatchNormalization before activation or not in attention net.

        - **seed**: A Python integer to use as random seed.

      References
        - [Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068.](https://arxiv.org/pdf/1706.06978.pdf)
    """"""

    def __init__(self, hidden_units=(64, 32), activation='sigmoid', l2_reg=0, dropout_rate=0, use_bn=False, seed=1024,
                 **kwargs):
        self.hidden_units = hidden_units
        self.activation = activation
        self.l2_reg = l2_reg
        self.dropout_rate = dropout_rate
        self.use_bn = use_bn
        self.seed = seed
        super(LocalActivationUnit, self).__init__(**kwargs)
        self.supports_masking = True

    def build(self, input_shape):

        if not isinstance(input_shape, list) or len(input_shape) != 2:
            raise ValueError('A `LocalActivationUnit` layer should be called '
                             'on a list of 2 inputs')

        if len(input_shape[0]) != 3 or len(input_shape[1]) != 3:
            raise ValueError(""Unexpected inputs dimensions %d and %d, expect to be 3 dimensions"" % (
                len(input_shape[0]), len(input_shape[1])))

        if input_shape[0][-1] != input_shape[1][-1] or input_shape[0][1] != 1:
            raise ValueError('A `LocalActivationUnit` layer requires '
                             'inputs of a two inputs with shape (None,1,embedding_size) and (None,T,embedding_size)'
                             'Got different shapes: %s,%s' % (input_shape[0], input_shape[1]))
        size = 4 * \
               int(input_shape[0][-1]
                   ) if len(self.hidden_units) == 0 else self.hidden_units[-1]
        self.kernel = self.add_weight(shape=(size, 1),
                                      initializer=glorot_normal(
                                          seed=self.seed),
                                      name=""kernel"")
        self.bias = self.add_weight(
            shape=(1,), initializer=Zeros(), name=""bias"")
        self.dnn = DNN(self.hidden_units, self.activation, self.l2_reg, self.dropout_rate, self.use_bn, seed=self.seed)

        super(LocalActivationUnit, self).build(
            input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, training=None, **kwargs):

        query, keys = inputs

        keys_len = keys.get_shape()[1]
        queries = K.repeat_elements(query, keys_len, 1)

        att_input = tf.concat(
            [queries, keys, queries - keys, queries * keys], axis=-1)

        att_out = self.dnn(att_input, training=training)

        attention_score = tf.nn.bias_add(tf.tensordot(att_out, self.kernel, axes=(-1, 0)), self.bias)

        return attention_score

    def compute_output_shape(self, input_shape):
        return input_shape[1][:2] + (1,)

    def compute_mask(self, inputs, mask):
        return mask

    def get_config(self, ):
        config = {'activation': self.activation, 'hidden_units': self.hidden_units,
                  'l2_reg': self.l2_reg, 'dropout_rate': self.dropout_rate, 'use_bn': self.use_bn, 'seed': self.seed}
        base_config = super(LocalActivationUnit, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class DNN(Layer):
    """"""The Multi Layer Percetron

      Input shape
        - nD tensor with shape: ``(batch_size, ..., input_dim)``. The most common situation would be a 2D input with shape ``(batch_size, input_dim)``.

      Output shape
        - nD tensor with shape: ``(batch_size, ..., hidden_size[-1])``. For instance, for a 2D input with shape ``(batch_size, input_dim)``, the output would have shape ``(batch_size, hidden_size[-1])``.

      Arguments
        - **hidden_units**:list of positive integer, the layer number and units in each layer.

        - **activation**: Activation function to use.

        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix.

        - **dropout_rate**: float in [0,1). Fraction of the units to dropout.

        - **use_bn**: bool. Whether use BatchNormalization before activation or not.

        - **output_activation**: Activation function to use in the last layer.If ``None``,it will be same as ``activation``.

        - **seed**: A Python integer to use as random seed.
    """"""

    def __init__(self, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False, output_activation=None,
                 seed=1024, **kwargs):
        self.hidden_units = hidden_units
        self.activation = activation
        self.l2_reg = l2_reg
        self.dropout_rate = dropout_rate
        self.use_bn = use_bn
        self.output_activation = output_activation
        self.seed = seed

        super(DNN, self).__init__(**kwargs)

    def build(self, input_shape):
        # if len(self.hidden_units) == 0:
        #     raise ValueError(""hidden_units is empty"")
        input_size = input_shape[-1]
        hidden_units = [int(input_size)] + list(self.hidden_units)
        self.kernels = [self.add_weight(name='kernel' + str(i),
                                        shape=(
                                            hidden_units[i], hidden_units[i + 1]),
                                        initializer=glorot_normal(
                                            seed=self.seed),
                                        regularizer=l2(self.l2_reg),
                                        trainable=True) for i in range(len(self.hidden_units))]
        self.bias = [self.add_weight(name='bias' + str(i),
                                     shape=(self.hidden_units[i],),
                                     initializer=Zeros(),
                                     trainable=True) for i in range(len(self.hidden_units))]
        if self.use_bn:
            self.bn_layers = [BatchNormalization() for _ in range(len(self.hidden_units))]

        self.dropout_layers = [Dropout(self.dropout_rate, seed=self.seed + i) for i in
                               range(len(self.hidden_units))]

        self.activation_layers = [activation_layer(self.activation) for _ in range(len(self.hidden_units))]

        if self.output_activation:
            self.activation_layers[-1] = activation_layer(self.output_activation)

        super(DNN, self).build(input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, training=None, **kwargs):

        deep_input = inputs

        for i in range(len(self.hidden_units)):
            fc = tf.nn.bias_add(tf.tensordot(
                deep_input, self.kernels[i], axes=(-1, 0)), self.bias[i])

            if self.use_bn:
                fc = self.bn_layers[i](fc, training=training)
            try:
                fc = self.activation_layers[i](fc, training=training)
            except TypeError as e:  # TypeError: call() got an unexpected keyword argument 'training'
                print(""make sure the activation function use training flag properly"", e)
                fc = self.activation_layers[i](fc)

            fc = self.dropout_layers[i](fc, training=training)
            deep_input = fc

        return deep_input

    def compute_output_shape(self, input_shape):
        if len(self.hidden_units) > 0:
            shape = input_shape[:-1] + (self.hidden_units[-1],)
        else:
            shape = input_shape

        return tuple(shape)

    def get_config(self, ):
        config = {'activation': self.activation, 'hidden_units': self.hidden_units,
                  'l2_reg': self.l2_reg, 'use_bn': self.use_bn, 'dropout_rate': self.dropout_rate,
                  'output_activation': self.output_activation, 'seed': self.seed}
        base_config = super(DNN, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class PredictionLayer(Layer):
    """"""
      Arguments
         - **task**: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss

         - **use_bias**: bool.Whether add bias term or not.
    """"""

    def __init__(self, task='binary', use_bias=True, **kwargs):
        if task not in [""binary"", ""multiclass"", ""regression""]:
            raise ValueError(""task must be binary,multiclass or regression"")
        self.task = task
        self.use_bias = use_bias
        super(PredictionLayer, self).__init__(**kwargs)

    def build(self, input_shape):

        if self.use_bias:
            self.global_bias = self.add_weight(
                shape=(1,), initializer=Zeros(), name=""global_bias"")

        # Be sure to call this somewhere!
        super(PredictionLayer, self).build(input_shape)

    def call(self, inputs, **kwargs):
        x = inputs
        if self.use_bias:
            x = tf.nn.bias_add(x, self.global_bias, data_format='NHWC')
        if self.task == ""binary"":
            x = tf.sigmoid(x)

        output = tf.reshape(x, (-1, 1))

        return output

    def compute_output_shape(self, input_shape):
        return (None, 1)

    def get_config(self, ):
        config = {'task': self.task, 'use_bias': self.use_bias}
        base_config = super(PredictionLayer, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class RegulationModule(Layer):
    """"""Regulation module used in EDCN.

      Input shape
        - 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.

      Output shape
        - 2D tensor with shape: ``(batch_size,field_size * embedding_size)``.

      Arguments
        - **tau** : Positive float, the temperature coefficient to control
        distribution of field-wise gating unit.

      References
        - [Enhancing Explicit and Implicit Feature Interactions via Information Sharing for Parallel Deep CTR Models.](https://dlp-kdd.github.io/assets/pdf/DLP-KDD_2021_paper_12.pdf)
    """"""

    def __init__(self, tau=1.0, **kwargs):
        if tau == 0:
            raise ValueError(""RegulationModule tau can not be zero."")
        self.tau = 1.0 / tau
        super(RegulationModule, self).__init__(**kwargs)

    def build(self, input_shape):
        self.field_size = int(input_shape[1])
        self.embedding_size = int(input_shape[2])
        self.g = self.add_weight(
            shape=(1, self.field_size, 1),
            initializer=Ones(),
            name=self.name + '_field_weight')

        # Be sure to call this somewhere!
        super(RegulationModule, self).build(input_shape)

    def call(self, inputs, **kwargs):

        if K.ndim(inputs) != 3:
            raise ValueError(
                ""Unexpected inputs dimensions %d, expect to be 3 dimensions"" % (K.ndim(inputs)))

        feild_gating_score = tf.nn.softmax(self.g * self.tau, 1)
        E = inputs * feild_gating_score
        return tf.reshape(E, [-1, self.field_size * self.embedding_size])

    def compute_output_shape(self, input_shape):
        return (None, self.field_size * self.embedding_size)

    def get_config(self):
        config = {'tau': self.tau}
        base_config = super(RegulationModule, self).get_config()
        base_config.update(config)
        return base_config
"
DeepCTR,activation.py,"# -*- coding:utf-8 -*-
""""""

Author:
    Weichen Shen,weichenswc@163.com

""""""

import tensorflow as tf

try:
    from tensorflow.python.ops.init_ops import Zeros
except ImportError:
    from tensorflow.python.ops.init_ops_v2 import Zeros
from tensorflow.python.keras.layers import Layer, Activation

try:
    from tensorflow.python.keras.layers import BatchNormalization
except ImportError:
    BatchNormalization = tf.keras.layers.BatchNormalization

try:
    unicode
except NameError:
    unicode = str


class Dice(Layer):
    """"""The Data Adaptive Activation Function in DIN,which can be viewed as a generalization of PReLu and can adaptively adjust the rectified point according to distribution of input data.

      Input shape
        - Arbitrary. Use the keyword argument `input_shape` (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.

      Output shape
        - Same shape as the input.

      Arguments
        - **axis** : Integer, the axis that should be used to compute data distribution (typically the features axis).

        - **epsilon** : Small float added to variance to avoid dividing by zero.

      References
        - [Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068.](https://arxiv.org/pdf/1706.06978.pdf)
    """"""

    def __init__(self, axis=-1, epsilon=1e-9, **kwargs):
        self.axis = axis
        self.epsilon = epsilon
        super(Dice, self).__init__(**kwargs)

    def build(self, input_shape):
        self.bn = BatchNormalization(
            axis=self.axis, epsilon=self.epsilon, center=False, scale=False)
        self.alphas = self.add_weight(shape=(input_shape[-1],), initializer=Zeros(
        ), dtype=tf.float32, name='dice_alpha')  # name='alpha_'+self.name
        super(Dice, self).build(input_shape)  # Be sure to call this somewhere!
        self.uses_learning_phase = True

    def call(self, inputs, training=None, **kwargs):
        inputs_normed = self.bn(inputs, training=training)
        # tf.layers.batch_normalization(
        # inputs, axis=self.axis, epsilon=self.epsilon, center=False, scale=False)
        x_p = tf.sigmoid(inputs_normed)
        return self.alphas * (1.0 - x_p) * inputs + x_p * inputs

    def compute_output_shape(self, input_shape):
        return input_shape

    def get_config(self, ):
        config = {'axis': self.axis, 'epsilon': self.epsilon}
        base_config = super(Dice, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


def activation_layer(activation):
    if activation in (""dice"", ""Dice""):
        act_layer = Dice()
    elif isinstance(activation, (str, unicode)):
        act_layer = Activation(activation)
    elif issubclass(activation, Layer):
        act_layer = activation()
    else:
        raise ValueError(
            ""Invalid activation,found %s.You should use a str or a Activation Layer Class."" % (activation))
    return act_layer
"
DeepCTR,utils.py,"# -*- coding:utf-8 -*-
""""""

Author:
    Weichen Shen,weichenswc@163.com

""""""
import tensorflow as tf
from tensorflow.python.keras import backend as K
from tensorflow.python.keras.layers import Flatten, Layer, Add
from tensorflow.python.ops.lookup_ops import TextFileInitializer

try:
    from tensorflow.python.ops.init_ops import Zeros, glorot_normal_initializer as glorot_normal
except ImportError:
    from tensorflow.python.ops.init_ops_v2 import Zeros, glorot_normal

from tensorflow.python.keras.regularizers import l2

try:
    from tensorflow.python.ops.lookup_ops import StaticHashTable
except ImportError:
    from tensorflow.python.ops.lookup_ops import HashTable as StaticHashTable


class NoMask(Layer):
    def __init__(self, **kwargs):
        super(NoMask, self).__init__(**kwargs)

    def build(self, input_shape):
        # Be sure to call this somewhere!
        super(NoMask, self).build(input_shape)

    def call(self, x, mask=None, **kwargs):
        return x

    def compute_mask(self, inputs, mask):
        return None


class Hash(Layer):
    """"""Looks up keys in a table when setup `vocabulary_path`, which outputs the corresponding values.
    If `vocabulary_path` is not set, `Hash` will hash the input to [0,num_buckets). When `mask_zero` = True,
    input value `0` or `0.0` will be set to `0`, and other value will be set in range [1,num_buckets).

    The following snippet initializes a `Hash` with `vocabulary_path` file with the first column as keys and
    second column as values:

    * `1,emerson`
    * `2,lake`
    * `3,palmer`

    >>> hash = Hash(
    ...   num_buckets=3+1,
    ...   vocabulary_path=filename,
    ...   default_value=0)
    >>> hash(tf.constant('lake')).numpy()
    2
    >>> hash(tf.constant('lakeemerson')).numpy()
    0

    Args:
        num_buckets: An `int` that is >= 1. The number of buckets or the vocabulary size + 1
            when `vocabulary_path` is setup.
        mask_zero: default is False. The `Hash` value will hash input `0` or `0.0` to value `0` when
            the `mask_zero` is `True`. `mask_zero` is not used when `vocabulary_path` is setup.
        vocabulary_path: default `None`. The `CSV` text file path of the vocabulary hash, which contains
            two columns seperated by delimiter `comma`, the first column is the value and the second is
            the key. The key data type is `string`, the value data type is `int`. The path must
            be accessible from wherever `Hash` is initialized.
        default_value: default '0'. The default value if a key is missing in the table.
        **kwargs: Additional keyword arguments.
    """"""

    def __init__(self, num_buckets, mask_zero=False, vocabulary_path=None, default_value=0, **kwargs):
        self.num_buckets = num_buckets
        self.mask_zero = mask_zero
        self.vocabulary_path = vocabulary_path
        self.default_value = default_value
        if self.vocabulary_path:
            initializer = TextFileInitializer(vocabulary_path, 'string', 1, 'int64', 0, delimiter=',')
            self.hash_table = StaticHashTable(initializer, default_value=self.default_value)
        super(Hash, self).__init__(**kwargs)

    def build(self, input_shape):
        # Be sure to call this somewhere!
        super(Hash, self).build(input_shape)

    def call(self, x, mask=None, **kwargs):

        if x.dtype != tf.string:
            zero = tf.as_string(tf.zeros([1], dtype=x.dtype))
            x = tf.as_string(x, )
        else:
            zero = tf.as_string(tf.zeros([1], dtype='int32'))

        if self.vocabulary_path:
            hash_x = self.hash_table.lookup(x)
            return hash_x

        num_buckets = self.num_buckets if not self.mask_zero else self.num_buckets - 1
        try:
            hash_x = tf.string_to_hash_bucket_fast(x, num_buckets,
                                                   name=None)  # weak hash
        except AttributeError:
            hash_x = tf.strings.to_hash_bucket_fast(x, num_buckets,
                                                    name=None)  # weak hash
        if self.mask_zero:
            mask = tf.cast(tf.not_equal(x, zero), dtype='int64')
            hash_x = (hash_x + 1) * mask

        return hash_x

    def compute_output_shape(self, input_shape):
        return input_shape

    def get_config(self, ):
        config = {'num_buckets': self.num_buckets, 'mask_zero': self.mask_zero, 'vocabulary_path': self.vocabulary_path,
                  'default_value': self.default_value}
        base_config = super(Hash, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class Linear(Layer):

    def __init__(self, l2_reg=0.0, mode=0, use_bias=False, seed=1024, **kwargs):

        self.l2_reg = l2_reg
        # self.l2_reg = tf.contrib.layers.l2_regularizer(float(l2_reg_linear))
        if mode not in [0, 1, 2]:
            raise ValueError(""mode must be 0,1 or 2"")
        self.mode = mode
        self.use_bias = use_bias
        self.seed = seed
        super(Linear, self).__init__(**kwargs)

    def build(self, input_shape):
        if self.use_bias:
            self.bias = self.add_weight(name='linear_bias',
                                        shape=(1,),
                                        initializer=Zeros(),
                                        trainable=True)
        if self.mode == 1:
            self.kernel = self.add_weight(
                'linear_kernel',
                shape=[int(input_shape[-1]), 1],
                initializer=glorot_normal(self.seed),
                regularizer=l2(self.l2_reg),
                trainable=True)
        elif self.mode == 2:
            self.kernel = self.add_weight(
                'linear_kernel',
                shape=[int(input_shape[1][-1]), 1],
                initializer=glorot_normal(self.seed),
                regularizer=l2(self.l2_reg),
                trainable=True)

        super(Linear, self).build(input_shape)  # Be sure to call this somewhere!

    def call(self, inputs, **kwargs):
        if self.mode == 0:
            sparse_input = inputs
            linear_logit = reduce_sum(sparse_input, axis=-1, keep_dims=True)
        elif self.mode == 1:
            dense_input = inputs
            fc = tf.tensordot(dense_input, self.kernel, axes=(-1, 0))
            linear_logit = fc
        else:
            sparse_input, dense_input = inputs
            fc = tf.tensordot(dense_input, self.kernel, axes=(-1, 0))
            linear_logit = reduce_sum(sparse_input, axis=-1, keep_dims=False) + fc
        if self.use_bias:
            linear_logit += self.bias

        return linear_logit

    def compute_output_shape(self, input_shape):
        return (None, 1)

    def compute_mask(self, inputs, mask):
        return None

    def get_config(self, ):
        config = {'mode': self.mode, 'l2_reg': self.l2_reg, 'use_bias': self.use_bias, 'seed': self.seed}
        base_config = super(Linear, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class Concat(Layer):
    def __init__(self, axis, supports_masking=True, **kwargs):
        super(Concat, self).__init__(**kwargs)
        self.axis = axis
        self.supports_masking = supports_masking

    def call(self, inputs):
        return tf.concat(inputs, axis=self.axis)

    def compute_mask(self, inputs, mask=None):
        if not self.supports_masking:
            return None
        if mask is None:
            mask = [inputs_i._keras_mask if hasattr(inputs_i, ""_keras_mask"") else None for inputs_i in inputs]
        if mask is None:
            return None
        if not isinstance(mask, list):
            raise ValueError('`mask` should be a list.')
        if not isinstance(inputs, list):
            raise ValueError('`inputs` should be a list.')
        if len(mask) != len(inputs):
            raise ValueError('The lists `inputs` and `mask` '
                             'should have the same length.')
        if all([m is None for m in mask]):
            return None
        # Make a list of masks while making sure
        # the dimensionality of each mask
        # is the same as the corresponding input.
        masks = []
        for input_i, mask_i in zip(inputs, mask):
            if mask_i is None:
                # Input is unmasked. Append all 1s to masks,
                masks.append(tf.ones_like(input_i, dtype='bool'))
            elif K.ndim(mask_i) < K.ndim(input_i):
                # Mask is smaller than the input, expand it
                masks.append(tf.expand_dims(mask_i, axis=-1))
            else:
                masks.append(mask_i)
        concatenated = K.concatenate(masks, axis=self.axis)
        return K.all(concatenated, axis=-1, keepdims=False)

    def get_config(self, ):
        config = {'axis': self.axis, 'supports_masking': self.supports_masking}
        base_config = super(Concat, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


def concat_func(inputs, axis=-1, mask=False):
    if len(inputs) == 1:
        input = inputs[0]
        if not mask:
            input = NoMask()(input)
        return input
    return Concat(axis, supports_masking=mask)(inputs)


def reduce_mean(input_tensor,
                axis=None,
                keep_dims=False,
                name=None,
                reduction_indices=None):
    try:
        return tf.reduce_mean(input_tensor,
                              axis=axis,
                              keep_dims=keep_dims,
                              name=name,
                              reduction_indices=reduction_indices)
    except TypeError:
        return tf.reduce_mean(input_tensor,
                              axis=axis,
                              keepdims=keep_dims,
                              name=name)


def reduce_sum(input_tensor,
               axis=None,
               keep_dims=False,
               name=None,
               reduction_indices=None):
    try:
        return tf.reduce_sum(input_tensor,
                             axis=axis,
                             keep_dims=keep_dims,
                             name=name,
                             reduction_indices=reduction_indices)
    except TypeError:
        return tf.reduce_sum(input_tensor,
                             axis=axis,
                             keepdims=keep_dims,
                             name=name)


def reduce_max(input_tensor,
               axis=None,
               keep_dims=False,
               name=None,
               reduction_indices=None):
    try:
        return tf.reduce_max(input_tensor,
                             axis=axis,
                             keep_dims=keep_dims,
                             name=name,
                             reduction_indices=reduction_indices)
    except TypeError:
        return tf.reduce_max(input_tensor,
                             axis=axis,
                             keepdims=keep_dims,
                             name=name)


def div(x, y, name=None):
    try:
        return tf.div(x, y, name=name)
    except AttributeError:
        return tf.divide(x, y, name=name)


def softmax(logits, dim=-1, name=None):
    try:
        return tf.nn.softmax(logits, dim=dim, name=name)
    except TypeError:
        return tf.nn.softmax(logits, axis=dim, name=name)


class _Add(Layer):
    def __init__(self, **kwargs):
        super(_Add, self).__init__(**kwargs)

    def build(self, input_shape):
        # Be sure to call this somewhere!
        super(_Add, self).build(input_shape)

    def call(self, inputs, **kwargs):
        if len(inputs) == 0:
            return tf.constant([[0.0]])

        return Add()(inputs)


def add_func(inputs):
    if not isinstance(inputs, list):
        return inputs
    if len(inputs) == 1:
        return inputs[0]
    return _Add()(inputs)


def combined_dnn_input(sparse_embedding_list, dense_value_list):
    if len(sparse_embedding_list) > 0 and len(dense_value_list) > 0:
        sparse_dnn_input = Flatten()(concat_func(sparse_embedding_list))
        dense_dnn_input = Flatten()(concat_func(dense_value_list))
        return concat_func([sparse_dnn_input, dense_dnn_input])
    elif len(sparse_embedding_list) > 0:
        return Flatten()(concat_func(sparse_embedding_list))
    elif len(dense_value_list) > 0:
        return Flatten()(concat_func(dense_value_list))
    else:
        raise NotImplementedError(""dnn_feature_columns can not be empty list"")
"
DeepCTR,normalization.py,"# -*- coding:utf-8 -*-
""""""

Author:
    Weichen Shen,weichenswc@163.com

""""""

from tensorflow.python.keras import backend as K
from tensorflow.python.keras.layers import Layer

try:
    from tensorflow.python.ops.init_ops import Zeros, Ones
except ImportError:
    from tensorflow.python.ops.init_ops_v2 import Zeros, Ones


class LayerNormalization(Layer):
    def __init__(self, axis=-1, eps=1e-9, center=True,
                 scale=True, **kwargs):
        self.axis = axis
        self.eps = eps
        self.center = center
        self.scale = scale
        super(LayerNormalization, self).__init__(**kwargs)

    def build(self, input_shape):
        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],
                                     initializer=Ones(), trainable=True)
        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],
                                    initializer=Zeros(), trainable=True)
        super(LayerNormalization, self).build(input_shape)

    def call(self, inputs):
        mean = K.mean(inputs, axis=self.axis, keepdims=True)
        variance = K.mean(K.square(inputs - mean), axis=-1, keepdims=True)
        std = K.sqrt(variance + self.eps)
        outputs = (inputs - mean) / std
        if self.scale:
            outputs *= self.gamma
        if self.center:
            outputs += self.beta
        return outputs

    def compute_output_shape(self, input_shape):
        return input_shape

    def get_config(self, ):
        config = {'axis': self.axis, 'eps': self.eps, 'center': self.center, 'scale': self.scale}
        base_config = super(LayerNormalization, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
"
DeepCTR,feature_column.py,"import tensorflow as tf
from tensorflow.python.feature_column.feature_column import _EmbeddingColumn

from .utils import LINEAR_SCOPE_NAME, variable_scope, get_collection, get_GraphKeys, input_layer, get_losses


def linear_model(features, linear_feature_columns):
    if tf.__version__ >= '2.0.0':
        linear_logits = tf.compat.v1.feature_column.linear_model(features, linear_feature_columns)
    else:
        linear_logits = tf.feature_column.linear_model(features, linear_feature_columns)
    return linear_logits


def get_linear_logit(features, linear_feature_columns, l2_reg_linear=0):
    with variable_scope(LINEAR_SCOPE_NAME):
        if not linear_feature_columns:
            linear_logits = tf.Variable([[0.0]], name='bias_weights')
        else:

            linear_logits = linear_model(features, linear_feature_columns)

            if l2_reg_linear > 0:
                for var in get_collection(get_GraphKeys().TRAINABLE_VARIABLES, LINEAR_SCOPE_NAME)[:-1]:
                    get_losses().add_loss(l2_reg_linear * tf.nn.l2_loss(var, name=var.name.split("":"")[0] + ""_l2loss""),
                                          get_GraphKeys().REGULARIZATION_LOSSES)
    return linear_logits


def input_from_feature_columns(features, feature_columns, l2_reg_embedding=0.0):
    dense_value_list = []
    sparse_emb_list = []
    for feat in feature_columns:
        if is_embedding(feat):
            sparse_emb = tf.expand_dims(input_layer(features, [feat]), axis=1)
            sparse_emb_list.append(sparse_emb)
            if l2_reg_embedding > 0:
                get_losses().add_loss(l2_reg_embedding * tf.nn.l2_loss(sparse_emb, name=feat.name + ""_l2loss""),
                                      get_GraphKeys().REGULARIZATION_LOSSES)

        else:
            dense_value_list.append(input_layer(features, [feat]))

    return sparse_emb_list, dense_value_list


def is_embedding(feature_column):
    try:
        from tensorflow.python.feature_column.feature_column_v2 import EmbeddingColumn
    except ImportError:
        EmbeddingColumn = _EmbeddingColumn
    return isinstance(feature_column, (_EmbeddingColumn, EmbeddingColumn))
"
DeepCTR,__init__.py,from .models import *
DeepCTR,inputs.py,"import tensorflow as tf


def input_fn_pandas(df, features, label=None, batch_size=256, num_epochs=1, shuffle=False, queue_capacity_factor=10,
                    num_threads=1):
    if label is not None:
        y = df[label]
    else:
        y = None
    if tf.__version__ >= ""2.0.0"":
        return tf.compat.v1.estimator.inputs.pandas_input_fn(df[features], y, batch_size=batch_size,
                                                             num_epochs=num_epochs,
                                                             shuffle=shuffle,
                                                             queue_capacity=batch_size * queue_capacity_factor,
                                                             num_threads=num_threads)

    return tf.estimator.inputs.pandas_input_fn(df[features], y, batch_size=batch_size, num_epochs=num_epochs,
                                               shuffle=shuffle, queue_capacity=batch_size * queue_capacity_factor,
                                               num_threads=num_threads)


def input_fn_tfrecord(filenames, feature_description, label=None, batch_size=256, num_epochs=1, num_parallel_calls=8,
                      shuffle_factor=10, prefetch_factor=1,
                      ):
    def _parse_examples(serial_exmp):
        try:
            features = tf.parse_single_example(serial_exmp, features=feature_description)
        except AttributeError:
            features = tf.io.parse_single_example(serial_exmp, features=feature_description)
        if label is not None:
            labels = features.pop(label)
            return features, labels
        return features

    def input_fn():
        dataset = tf.data.TFRecordDataset(filenames)
        dataset = dataset.map(_parse_examples, num_parallel_calls=num_parallel_calls)
        if shuffle_factor > 0:
            dataset = dataset.shuffle(buffer_size=batch_size * shuffle_factor)

        dataset = dataset.repeat(num_epochs).batch(batch_size)

        if prefetch_factor > 0:
            dataset = dataset.prefetch(buffer_size=batch_size * prefetch_factor)
        try:
            iterator = dataset.make_one_shot_iterator()
        except AttributeError:
            iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)

        return iterator.get_next()

    return input_fn
"
DeepCTR,utils.py,"import tensorflow as tf
from tensorflow.python.estimator.canned.head import _Head
from tensorflow.python.estimator.canned.optimizers import get_optimizer_instance

LINEAR_SCOPE_NAME = 'linear'
DNN_SCOPE_NAME = 'dnn'


def _summary_key(head_name, val):
    return '%s/%s' % (val, head_name) if head_name else val


class Head(_Head):

    def __init__(self, task,
                 name=None):
        self._task = task
        self._name = name

    @property
    def name(self):
        return self._name

    @property
    def logits_dimension(self):
        return 1

    def _eval_metric_ops(self,
                         labels,
                         logits,
                         predictions,
                         unweighted_loss,
                         weights=None):

        labels = to_float(labels)
        predictions = to_float(predictions)

        # with name_scope(None, 'metrics', (labels, logits, predictions,
        # unweighted_loss, weights)):
        metrics = get_metrics()
        losses = get_losses()

        metric_ops = {
            _summary_key(self._name, ""prediction/mean""): metrics.mean(predictions, weights=weights),
            _summary_key(self._name, ""label/mean""): metrics.mean(labels, weights=weights),
        }

        summary_scalar(""prediction/mean"", metric_ops[_summary_key(self._name, ""prediction/mean"")][1])
        summary_scalar(""label/mean"", metric_ops[_summary_key(self._name, ""label/mean"")][1])


        mean_loss = losses.compute_weighted_loss(
            unweighted_loss, weights=1.0, reduction=losses.Reduction.MEAN)

        if self._task == ""binary"":
            metric_ops[_summary_key(self._name, ""LogLoss"")] = metrics.mean(mean_loss, weights=weights, )
            summary_scalar(""LogLoss"", mean_loss)

            metric_ops[_summary_key(self._name, ""AUC"")] = metrics.auc(labels, predictions, weights=weights)
            summary_scalar(""AUC"", metric_ops[_summary_key(self._name, ""AUC"")][1])
        else:

            metric_ops[_summary_key(self._name, ""MSE"")] = metrics.mean_squared_error(labels, predictions,
                                                                                     weights=weights)
            summary_scalar(""MSE"", mean_loss)

            metric_ops[_summary_key(self._name, ""MAE"")] = metrics.mean_absolute_error(labels, predictions,
                                                                                      weights=weights)
            summary_scalar(""MAE"", metric_ops[_summary_key(self._name, ""MAE"")][1])

        return metric_ops

    def create_loss(self, features, mode, logits, labels):
        del mode, features  # Unused for this head.
        losses = get_losses()
        if self._task == ""binary"":
            loss = losses.sigmoid_cross_entropy(labels, logits, reduction=losses.Reduction.NONE)
        else:
            loss = losses.mean_squared_error(labels, logits, reduction=losses.Reduction.NONE)
        return loss

    def create_estimator_spec(
            self, features, mode, logits, labels=None, train_op_fn=None, training_chief_hooks=None):
        # with name_scope('head'):
        logits = tf.reshape(logits, [-1, 1])
        if self._task == 'binary':
            pred = tf.sigmoid(logits)
        else:
            pred = logits

        predictions = {""pred"": pred, ""logits"": logits}
        export_outputs = {""predict"": tf.estimator.export.PredictOutput(predictions)}
        if mode == tf.estimator.ModeKeys.PREDICT:
            return tf.estimator.EstimatorSpec(
                mode=mode,
                predictions=predictions,
                export_outputs=export_outputs)

        labels = tf.reshape(labels, [-1, 1])

        unweighted_loss = self.create_loss(features, mode, logits, labels)

        losses = get_losses()
        loss = losses.compute_weighted_loss(
            unweighted_loss, weights=1.0, reduction=losses.Reduction.SUM)
        reg_loss = losses.get_regularization_loss()

        training_loss = loss + reg_loss

        eval_metric_ops = self._eval_metric_ops(labels, logits, pred, unweighted_loss)

        return tf.estimator.EstimatorSpec(
            mode=mode,
            predictions=predictions,
            loss=training_loss,
            train_op=train_op_fn(training_loss),
            eval_metric_ops=eval_metric_ops,
            training_chief_hooks=training_chief_hooks)


def deepctr_model_fn(features, mode, logits, labels, task, linear_optimizer, dnn_optimizer, training_chief_hooks):
    linear_optimizer = get_optimizer_instance(linear_optimizer, 0.005)
    dnn_optimizer = get_optimizer_instance(dnn_optimizer, 0.01)
    train_op_fn = get_train_op_fn(linear_optimizer, dnn_optimizer)

    head = Head(task)
    return head.create_estimator_spec(features=features,
                                      mode=mode,
                                      labels=labels,
                                      train_op_fn=train_op_fn,
                                      logits=logits, training_chief_hooks=training_chief_hooks)


def get_train_op_fn(linear_optimizer, dnn_optimizer):
    def _train_op_fn(loss):
        train_ops = []
        try:
            global_step = tf.train.get_global_step()
        except AttributeError:
            global_step = tf.compat.v1.train.get_global_step()
        linear_var_list = get_collection(get_GraphKeys().TRAINABLE_VARIABLES, LINEAR_SCOPE_NAME)
        dnn_var_list = get_collection(get_GraphKeys().TRAINABLE_VARIABLES, DNN_SCOPE_NAME)

        if len(dnn_var_list) > 0:
            train_ops.append(
                dnn_optimizer.minimize(
                    loss,
                    var_list=dnn_var_list))
        if len(linear_var_list) > 0:
            train_ops.append(
                linear_optimizer.minimize(
                    loss,
                    var_list=linear_var_list))

        train_op = tf.group(*train_ops)
        with tf.control_dependencies([train_op]):
            try:
                return tf.assign_add(global_step, 1).op
            except AttributeError:
                return tf.compat.v1.assign_add(global_step, 1).op

    return _train_op_fn


def variable_scope(name_or_scope):
    try:
        return tf.variable_scope(name_or_scope)
    except AttributeError:
        return tf.compat.v1.variable_scope(name_or_scope)

def get_collection(key, scope=None):
    try:
        return tf.get_collection(key, scope=scope)
    except AttributeError:
        return tf.compat.v1.get_collection(key, scope=scope)


def get_GraphKeys():
    try:
        return tf.GraphKeys
    except AttributeError:
        return tf.compat.v1.GraphKeys


def get_losses():
    try:
        return tf.compat.v1.losses
    except AttributeError:
        return tf.losses


def input_layer(features, feature_columns):
    try:
        return tf.feature_column.input_layer(features, feature_columns)
    except AttributeError:
        return tf.compat.v1.feature_column.input_layer(features, feature_columns)


def get_metrics():
    try:
        return tf.compat.v1.metrics
    except AttributeError:
        return tf.metrics


def to_float(x, name=""ToFloat""):
    try:
        return tf.to_float(x, name)
    except AttributeError:
        return tf.compat.v1.to_float(x, name)


def summary_scalar(name, data):
    try:
        tf.summary.scalar(name, data)
    except AttributeError:  # tf version 2.5.0+:AttributeError: module 'tensorflow._api.v2.summary' has no attribute 'scalar'
        tf.compat.v1.summary.scalar(name, data)"
DeepCTR,pnn.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Qu Y, Cai H, Ren K, et al. Product-based neural networks for user response prediction[C]//Data Mining (ICDM), 2016 IEEE 16th International Conference on. IEEE, 2016: 1149-1154.(https://arxiv.org/pdf/1611.00144.pdf)
""""""

import tensorflow as tf

from ..feature_column import get_linear_logit, input_from_feature_columns
from ..utils import deepctr_model_fn, DNN_SCOPE_NAME, variable_scope
from ...layers.core import DNN
from ...layers.interaction import InnerProductLayer, OutterProductLayer
from ...layers.utils import concat_func, combined_dnn_input


def PNNEstimator(dnn_feature_columns, dnn_hidden_units=(256, 128, 64), l2_reg_embedding=1e-5, l2_reg_dnn=0,
                 seed=1024, dnn_dropout=0, dnn_activation='relu', use_inner=True, use_outter=False, kernel_type='mat',
                 task='binary', model_dir=None, config=None,
                 linear_optimizer='Ftrl',
                 dnn_optimizer='Adagrad', training_chief_hooks=None):
    """"""Instantiates the Product-based Neural Network architecture.

    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net
    :param l2_reg_embedding: float . L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param use_inner: bool,whether use inner-product or not.
    :param use_outter: bool,whether use outter-product or not.
    :param kernel_type: str,kernel_type used in outter-product,can be ``'mat'`` , ``'vec'`` or ``'num'``
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :param model_dir: Directory to save model parameters, graph and etc. This can
        also be used to load checkpoints from the directory into a estimator
        to continue training a previously saved model.
    :param config: tf.RunConfig object to configure the runtime settings.
    :param linear_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the linear part of the model. Defaults to FTRL optimizer.
    :param dnn_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the deep part of the model. Defaults to Adagrad optimizer.
    :param training_chief_hooks: Iterable of `tf.train.SessionRunHook` objects to
        run on the chief worker during training.
    :return: A Tensorflow Estimator  instance.

    """"""

    if kernel_type not in ['mat', 'vec', 'num']:
        raise ValueError(""kernel_type must be mat,vec or num"")

    def _model_fn(features, labels, mode, config):
        train_flag = (mode == tf.estimator.ModeKeys.TRAIN)

        linear_logits = get_linear_logit(features, [], l2_reg_linear=0)

        with variable_scope(DNN_SCOPE_NAME):
            sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                                 l2_reg_embedding=l2_reg_embedding)

            inner_product = tf.keras.layers.Flatten()(
                InnerProductLayer()(sparse_embedding_list))
            outter_product = OutterProductLayer(kernel_type)(sparse_embedding_list)

            # ipnn deep input
            linear_signal = tf.keras.layers.Reshape(
                [sum(map(lambda x: int(x.shape[-1]), sparse_embedding_list))])(concat_func(sparse_embedding_list))

            if use_inner and use_outter:
                deep_input = tf.keras.layers.Concatenate()(
                    [linear_signal, inner_product, outter_product])
            elif use_inner:
                deep_input = tf.keras.layers.Concatenate()(
                    [linear_signal, inner_product])
            elif use_outter:
                deep_input = tf.keras.layers.Concatenate()(
                    [linear_signal, outter_product])
            else:
                deep_input = linear_signal

            dnn_input = combined_dnn_input([deep_input], dense_value_list)
            dnn_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed=seed)(dnn_input, training=train_flag)
            dnn_logit = tf.keras.layers.Dense(
                1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(dnn_out)

        logits = linear_logits + dnn_logit

        return deepctr_model_fn(features, mode, logits, labels, task, linear_optimizer, dnn_optimizer,
                                training_chief_hooks=training_chief_hooks)

    return tf.estimator.Estimator(_model_fn, model_dir=model_dir, config=config)
"
DeepCTR,deepfm.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Guo H, Tang R, Ye Y, et al. Deepfm: a factorization-machine based neural network for ctr prediction[J]. arXiv preprint arXiv:1703.04247, 2017.(https://arxiv.org/abs/1703.04247)

""""""

import tensorflow as tf

from ..feature_column import get_linear_logit, input_from_feature_columns
from ..utils import deepctr_model_fn, DNN_SCOPE_NAME, variable_scope
from ...layers.core import DNN
from ...layers.interaction import FM
from ...layers.utils import concat_func, combined_dnn_input


def DeepFMEstimator(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64),
                    l2_reg_linear=0.00001, l2_reg_embedding=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0,
                    dnn_activation='relu', dnn_use_bn=False, task='binary', model_dir=None, config=None,
                    linear_optimizer='Ftrl',
                    dnn_optimizer='Adagrad', training_chief_hooks=None):
    """"""Instantiates the DeepFM Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param fm_group: list, group_name of features that will be used to do feature interactions.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :param model_dir: Directory to save model parameters, graph and etc. This can
        also be used to load checkpoints from the directory into a estimator
        to continue training a previously saved model.
    :param config: tf.RunConfig object to configure the runtime settings.
    :param linear_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the linear part of the model. Defaults to FTRL optimizer.
    :param dnn_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the deep part of the model. Defaults to Adagrad optimizer.
    :param training_chief_hooks: Iterable of `tf.train.SessionRunHook` objects to
        run on the chief worker during training.
    :return: A Tensorflow Estimator  instance.

    """"""

    def _model_fn(features, labels, mode, config):
        train_flag = (mode == tf.estimator.ModeKeys.TRAIN)

        linear_logits = get_linear_logit(features, linear_feature_columns, l2_reg_linear=l2_reg_linear)

        with variable_scope(DNN_SCOPE_NAME):
            sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                                 l2_reg_embedding=l2_reg_embedding)

            dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)

            fm_logit = FM()(concat_func(sparse_embedding_list, axis=1))

            dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input, training=train_flag)
            dnn_logit = tf.keras.layers.Dense(
                1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed=seed))(dnn_output)

        logits = linear_logits + fm_logit + dnn_logit

        return deepctr_model_fn(features, mode, logits, labels, task, linear_optimizer, dnn_optimizer,
                                training_chief_hooks
                                =training_chief_hooks)

    return tf.estimator.Estimator(_model_fn, model_dir=model_dir, config=config)
"
DeepCTR,wdl.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Cheng H T, Koc L, Harmsen J, et al. Wide & deep learning for recommender systems[C]//Proceedings of the 1st Workshop on Deep Learning for Recommender Systems. ACM, 2016: 7-10.(https://arxiv.org/pdf/1606.07792.pdf)
""""""

import tensorflow as tf
from tensorflow.python.keras.layers import Dense

from ..feature_column import get_linear_logit, input_from_feature_columns
from ..utils import deepctr_model_fn, DNN_SCOPE_NAME, variable_scope
from ...layers import DNN, combined_dnn_input


def WDLEstimator(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64), l2_reg_linear=1e-5,
                 l2_reg_embedding=1e-5, l2_reg_dnn=0, seed=1024, dnn_dropout=0, dnn_activation='relu',
                 task='binary', model_dir=None, config=None, linear_optimizer='Ftrl',
                 dnn_optimizer='Adagrad', training_chief_hooks=None):
    """"""Instantiates the Wide&Deep Learning architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to wide part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :param model_dir: Directory to save model parameters, graph and etc. This can
        also be used to load checkpoints from the directory into a estimator
        to continue training a previously saved model.
    :param config: tf.RunConfig object to configure the runtime settings.
    :param linear_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the linear part of the model. Defaults to FTRL optimizer.
    :param dnn_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the deep part of the model. Defaults to Adagrad optimizer.
    :param training_chief_hooks: Iterable of `tf.train.SessionRunHook` objects to
        run on the chief worker during training.
    :return: A Tensorflow Estimator  instance.

    """"""

    def _model_fn(features, labels, mode, config):
        train_flag = (mode == tf.estimator.ModeKeys.TRAIN)

        linear_logits = get_linear_logit(features, linear_feature_columns, l2_reg_linear=l2_reg_linear)

        with variable_scope(DNN_SCOPE_NAME):
            sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                                 l2_reg_embedding=l2_reg_embedding)
            dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)
            dnn_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed=seed)(dnn_input, training=train_flag)
            dnn_logits = Dense(
                1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(dnn_out)

        logits = linear_logits + dnn_logits

        return deepctr_model_fn(features, mode, logits, labels, task, linear_optimizer, dnn_optimizer,
                                training_chief_hooks=training_chief_hooks)

    return tf.estimator.Estimator(_model_fn, model_dir=model_dir, config=config)
"
DeepCTR,__init__.py,"from .afm import AFMEstimator
from .autoint import AutoIntEstimator
from .ccpm import CCPMEstimator
from .dcn import DCNEstimator
from .deepfm import DeepFMEstimator
from .fwfm import FwFMEstimator
from .fibinet import FiBiNETEstimator
from .fnn import FNNEstimator
from .nfm import NFMEstimator
from .pnn import PNNEstimator
from .wdl import WDLEstimator
from .xdeepfm import xDeepFMEstimator
from .deepfefm import DeepFEFMEstimator
"
DeepCTR,fibinet.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Huang T, Zhang Z, Zhang J. FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction[J]. arXiv preprint arXiv:1905.09433, 2019.
""""""

import tensorflow as tf
from tensorflow.python.keras.layers import Dense, Flatten

from ..feature_column import get_linear_logit, input_from_feature_columns
from ..utils import deepctr_model_fn, DNN_SCOPE_NAME, variable_scope
from ...layers.core import DNN
from ...layers.interaction import SENETLayer, BilinearInteraction
from ...layers.utils import concat_func, combined_dnn_input


def FiBiNETEstimator(linear_feature_columns, dnn_feature_columns, bilinear_type='interaction', reduction_ratio=3,
                     dnn_hidden_units=(256, 128, 64), l2_reg_linear=1e-5,
                     l2_reg_embedding=1e-5, l2_reg_dnn=0, seed=1024, dnn_dropout=0, dnn_activation='relu',
                     task='binary', model_dir=None, config=None, linear_optimizer='Ftrl',
                     dnn_optimizer='Adagrad', training_chief_hooks=None):
    """"""Instantiates the Feature Importance and Bilinear feature Interaction NETwork architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param bilinear_type: str,bilinear function type used in Bilinear Interaction Layer,can be ``'all'`` , ``'each'`` or ``'interaction'``
    :param reduction_ratio: integer in [1,inf), reduction ratio used in SENET Layer
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to wide part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :param model_dir: Directory to save model parameters, graph and etc. This can
        also be used to load checkpoints from the directory into a estimator
        to continue training a previously saved model.
    :param config: tf.RunConfig object to configure the runtime settings.
    :param linear_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the linear part of the model. Defaults to FTRL optimizer.
    :param dnn_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the deep part of the model. Defaults to Adagrad optimizer.
    :param training_chief_hooks: Iterable of `tf.train.SessionRunHook` objects to
        run on the chief worker during training.
    :return: A Tensorflow Estimator  instance.
    """"""

    def _model_fn(features, labels, mode, config):
        train_flag = (mode == tf.estimator.ModeKeys.TRAIN)

        linear_logits = get_linear_logit(features, linear_feature_columns, l2_reg_linear=l2_reg_linear)

        with variable_scope(DNN_SCOPE_NAME):
            sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                                 l2_reg_embedding=l2_reg_embedding)

            senet_embedding_list = SENETLayer(
                reduction_ratio, seed)(sparse_embedding_list)

            senet_bilinear_out = BilinearInteraction(
                bilinear_type=bilinear_type, seed=seed)(senet_embedding_list)
            bilinear_out = BilinearInteraction(
                bilinear_type=bilinear_type, seed=seed)(sparse_embedding_list)

            dnn_input = combined_dnn_input(
                [Flatten()(concat_func([senet_bilinear_out, bilinear_out]))], dense_value_list)
            dnn_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed=seed)(dnn_input, training=train_flag)
            dnn_logit = Dense(
                1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(dnn_out)

        logits = linear_logits + dnn_logit

        return deepctr_model_fn(features, mode, logits, labels, task, linear_optimizer, dnn_optimizer,
                                training_chief_hooks=training_chief_hooks)

    return tf.estimator.Estimator(_model_fn, model_dir=model_dir, config=config)
"
DeepCTR,nfm.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] He X, Chua T S. Neural factorization machines for sparse predictive analytics[C]//Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 2017: 355-364. (https://arxiv.org/abs/1708.05027)
""""""
import tensorflow as tf

from ..feature_column import get_linear_logit, input_from_feature_columns
from ..utils import deepctr_model_fn, DNN_SCOPE_NAME, variable_scope
from ...layers.core import DNN
from ...layers.interaction import BiInteractionPooling
from ...layers.utils import concat_func, combined_dnn_input


def NFMEstimator(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64),
                 l2_reg_embedding=1e-5, l2_reg_linear=1e-5, l2_reg_dnn=0, seed=1024, bi_dropout=0,
                 dnn_dropout=0, dnn_activation='relu', task='binary', model_dir=None, config=None,
                 linear_optimizer='Ftrl',
                 dnn_optimizer='Adagrad', training_chief_hooks=None):
    """"""Instantiates the Neural Factorization Machine architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part.
    :param l2_reg_dnn: float . L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param biout_dropout: When not ``None``, the probability we will drop out the output of BiInteractionPooling Layer.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in deep net
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :param model_dir: Directory to save model parameters, graph and etc. This can
        also be used to load checkpoints from the directory into a estimator
        to continue training a previously saved model.
    :param config: tf.RunConfig object to configure the runtime settings.
    :param linear_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the linear part of the model. Defaults to FTRL optimizer.
    :param dnn_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the deep part of the model. Defaults to Adagrad optimizer.
    :param training_chief_hooks: Iterable of `tf.train.SessionRunHook` objects to
        run on the chief worker during training.
    :return: A Tensorflow Estimator  instance.

    """"""

    def _model_fn(features, labels, mode, config):
        train_flag = (mode == tf.estimator.ModeKeys.TRAIN)

        linear_logits = get_linear_logit(features, linear_feature_columns, l2_reg_linear=l2_reg_linear)

        with variable_scope(DNN_SCOPE_NAME):
            sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                                 l2_reg_embedding=l2_reg_embedding)

            fm_input = concat_func(sparse_embedding_list, axis=1)
            bi_out = BiInteractionPooling()(fm_input)
            if bi_dropout:
                bi_out = tf.keras.layers.Dropout(bi_dropout)(bi_out, training=None)
            dnn_input = combined_dnn_input([bi_out], dense_value_list)
            dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed=seed)(dnn_input, training=train_flag)
            dnn_logit = tf.keras.layers.Dense(
                1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(dnn_output)

        logits = linear_logits + dnn_logit

        return deepctr_model_fn(features, mode, logits, labels, task, linear_optimizer, dnn_optimizer,
                                training_chief_hooks=training_chief_hooks)

    return tf.estimator.Estimator(_model_fn, model_dir=model_dir, config=config)
"
DeepCTR,afm.py,"# -*- coding:utf-8 -*-
""""""

Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Xiao J, Ye H, He X, et al. Attentional factorization machines: Learning the weight of feature interactions via attention networks[J]. arXiv preprint arXiv:1708.04617, 2017.
    (https://arxiv.org/abs/1708.04617)

""""""
import tensorflow as tf

from ..feature_column import get_linear_logit, input_from_feature_columns
from ..utils import deepctr_model_fn, DNN_SCOPE_NAME, variable_scope
from ...layers.interaction import AFMLayer, FM
from ...layers.utils import concat_func


def AFMEstimator(linear_feature_columns, dnn_feature_columns, use_attention=True, attention_factor=8,
                 l2_reg_linear=1e-5, l2_reg_embedding=1e-5, l2_reg_att=1e-5, afm_dropout=0, seed=1024,
                 task='binary', model_dir=None, config=None, linear_optimizer='Ftrl',
                 dnn_optimizer='Adagrad', training_chief_hooks=None):
    """"""Instantiates the Attentional Factorization Machine architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param use_attention: bool,whether use attention or not,if set to ``False``.it is the same as **standard Factorization Machine**
    :param attention_factor: positive integer,units in attention net
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_att: float. L2 regularizer strength applied to attention net
    :param afm_dropout: float in [0,1), Fraction of the attention net output units to dropout.
    :param seed: integer ,to use as random seed.
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :param model_dir: Directory to save model parameters, graph and etc. This can
        also be used to load checkpoints from the directory into a estimator
        to continue training a previously saved model.
    :param config: tf.RunConfig object to configure the runtime settings.
    :param linear_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the linear part of the model. Defaults to FTRL optimizer.
    :param dnn_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the deep part of the model. Defaults to Adagrad optimizer.
    :param training_chief_hooks: Iterable of `tf.train.SessionRunHook` objects to
        run on the chief worker during training.
    :return: A Tensorflow Estimator  instance.

    """"""

    def _model_fn(features, labels, mode, config):
        train_flag = (mode == tf.estimator.ModeKeys.TRAIN)

        linear_logits = get_linear_logit(features, linear_feature_columns, l2_reg_linear=l2_reg_linear)

        with variable_scope(DNN_SCOPE_NAME):
            sparse_embedding_list, _ = input_from_feature_columns(features, dnn_feature_columns,
                                                                                 l2_reg_embedding=l2_reg_embedding)
            if use_attention:

                fm_logit = AFMLayer(attention_factor, l2_reg_att, afm_dropout,
                                    seed)(sparse_embedding_list, training=train_flag)
            else:
                fm_logit = FM()(concat_func(sparse_embedding_list, axis=1))

        logits = linear_logits + fm_logit

        return deepctr_model_fn(features, mode, logits, labels, task, linear_optimizer, dnn_optimizer,
                                training_chief_hooks=training_chief_hooks)

    return tf.estimator.Estimator(_model_fn, model_dir=model_dir, config=config)
"
DeepCTR,deepfefm.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Harshit Pande

Reference:
    [1] Field-Embedded Factorization Machines for Click-through Rate Prediction]
    (https://arxiv.org/abs/2009.09931)

""""""

import tensorflow as tf

from ..feature_column import get_linear_logit, input_from_feature_columns
from ..utils import DNN_SCOPE_NAME, deepctr_model_fn, variable_scope
from ...layers.core import DNN
from ...layers.interaction import FEFMLayer
from ...layers.utils import concat_func, add_func, combined_dnn_input, reduce_sum


def DeepFEFMEstimator(linear_feature_columns, dnn_feature_columns,
                      dnn_hidden_units=(256, 128, 64), l2_reg_linear=0.00001, l2_reg_embedding_feat=0.00001,
                      l2_reg_embedding_field=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0.0,
                      dnn_activation='relu', dnn_use_bn=False, task='binary', model_dir=None,
                      config=None, linear_optimizer='Ftrl', dnn_optimizer='Adagrad', training_chief_hooks=None):
    """"""Instantiates the DeepFEFM Network architecture or the shallow FEFM architecture (Ablation support not provided
    as estimator is meant for production, Ablation support provided in DeepFEFM implementation in models

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding_feat: float. L2 regularizer strength applied to embedding vector of features
    :param l2_reg_embedding_field: float, L2 regularizer to field embeddings
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :param model_dir: Directory to save model parameters, graph and etc. This can
        also be used to load checkpoints from the directory into a estimator
        to continue training a previously saved model.
    :param config: tf.RunConfig object to configure the runtime settings.
    :param linear_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the linear part of the model. Defaults to FTRL optimizer.
    :param dnn_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the deep part of the model. Defaults to Adagrad optimizer.
    :param training_chief_hooks: Iterable of `tf.train.SessionRunHook` objects to
        run on the chief worker during training.
    :return: A Tensorflow Estimator  instance.
    """"""

    def _model_fn(features, labels, mode, config):
        train_flag = (mode == tf.estimator.ModeKeys.TRAIN)

        linear_logits = get_linear_logit(features, linear_feature_columns, l2_reg_linear=l2_reg_linear)
        final_logit_components = [linear_logits]

        with variable_scope(DNN_SCOPE_NAME):
            sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                                 l2_reg_embedding=l2_reg_embedding_feat)

            fefm_interaction_embedding = FEFMLayer(
                regularizer=l2_reg_embedding_field)(concat_func(sparse_embedding_list, axis=1))

            fefm_logit = tf.keras.layers.Lambda(lambda x: reduce_sum(x, axis=1, keep_dims=True))(
                fefm_interaction_embedding)

            final_logit_components.append(fefm_logit)

            if dnn_hidden_units:
                dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)
                dnn_input = concat_func([dnn_input, fefm_interaction_embedding], axis=1)

                dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(
                    dnn_input, training=train_flag)

                dnn_logit = tf.keras.layers.Dense(
                    1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(dnn_output)

                final_logit_components.append(dnn_logit)

        logits = add_func(final_logit_components)

        return deepctr_model_fn(features, mode, logits, labels, task, linear_optimizer, dnn_optimizer,
                                training_chief_hooks=training_chief_hooks)

    return tf.estimator.Estimator(_model_fn, model_dir=model_dir, config=config)
"
DeepCTR,fwfm.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com
    Harshit Pande

Reference:
    [1] Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising
    (https://arxiv.org/pdf/1806.03514.pdf)

""""""

import tensorflow as tf

from ..feature_column import get_linear_logit, input_from_feature_columns
from ..utils import DNN_SCOPE_NAME, deepctr_model_fn, variable_scope
from ...layers.core import DNN
from ...layers.interaction import FwFMLayer
from ...layers.utils import concat_func, add_func, combined_dnn_input


def FwFMEstimator(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64),
                  l2_reg_linear=0.00001, l2_reg_embedding=0.00001, l2_reg_field_strength=0.00001, l2_reg_dnn=0,
                  seed=1024, dnn_dropout=0, dnn_activation='relu', dnn_use_bn=False, task='binary', model_dir=None,
                  config=None, linear_optimizer='Ftrl',
                  dnn_optimizer='Adagrad', training_chief_hooks=None):
    """"""Instantiates the DeepFwFM Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param fm_group: list, group_name of features that will be used to do feature interactions.
    :param dnn_hidden_units: list,list of positive integer or empty list if do not want DNN, the layer number and units
    in each layer of DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_field_strength: float. L2 regularizer strength applied to the field pair strength parameters
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :param model_dir: Directory to save model parameters, graph and etc. This can
        also be used to load checkpoints from the directory into a estimator
        to continue training a previously saved model.
    :param config: tf.RunConfig object to configure the runtime settings.
    :param linear_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the linear part of the model. Defaults to FTRL optimizer.
    :param dnn_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the deep part of the model. Defaults to Adagrad optimizer.
    :param training_chief_hooks: Iterable of `tf.train.SessionRunHook` objects to
        run on the chief worker during training.
    :return: A Tensorflow Estimator  instance.

    """"""

    def _model_fn(features, labels, mode, config):
        train_flag = (mode == tf.estimator.ModeKeys.TRAIN)

        linear_logits = get_linear_logit(features, linear_feature_columns, l2_reg_linear=l2_reg_linear)
        final_logit_components = [linear_logits]
        with variable_scope(DNN_SCOPE_NAME):
            sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                                 l2_reg_embedding=l2_reg_embedding)

            fwfm_logit = FwFMLayer(num_fields=len(sparse_embedding_list), regularizer=l2_reg_field_strength)(
                concat_func(sparse_embedding_list, axis=1))

            final_logit_components.append(fwfm_logit)

            if dnn_hidden_units:
                dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)

                dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input, training=train_flag)
                dnn_logit = tf.keras.layers.Dense(
                    1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(dnn_output)
                final_logit_components.append(dnn_logit)

        logits = add_func(final_logit_components)

        return deepctr_model_fn(features, mode, logits, labels, task, linear_optimizer, dnn_optimizer,
                                training_chief_hooks=training_chief_hooks)

    return tf.estimator.Estimator(_model_fn, model_dir=model_dir, config=config)
"
DeepCTR,autoint.py,"# -*- coding:utf-8 -*-
""""""

Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Song W, Shi C, Xiao Z, et al. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks[J]. arXiv preprint arXiv:1810.11921, 2018.(https://arxiv.org/abs/1810.11921)

""""""

import tensorflow as tf

from ..feature_column import get_linear_logit, input_from_feature_columns
from ..utils import deepctr_model_fn, DNN_SCOPE_NAME, variable_scope
from ...layers.core import DNN
from ...layers.interaction import InteractingLayer
from ...layers.utils import concat_func, combined_dnn_input


def AutoIntEstimator(linear_feature_columns, dnn_feature_columns, att_layer_num=3, att_embedding_size=8, att_head_num=2,
                     att_res=True,
                     dnn_hidden_units=(256, 128, 64), dnn_activation='relu', l2_reg_linear=1e-5,
                     l2_reg_embedding=1e-5, l2_reg_dnn=0, dnn_use_bn=False, dnn_dropout=0, seed=1024,
                     task='binary', model_dir=None, config=None, linear_optimizer='Ftrl',
                     dnn_optimizer='Adagrad', training_chief_hooks=None):
    """"""Instantiates the AutoInt Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param att_layer_num: int.The InteractingLayer number to be used.
    :param att_embedding_size: int.The embedding size in multi-head self-attention network.
    :param att_head_num: int.The head number in multi-head  self-attention network.
    :param att_res: bool.Whether or not use standard residual connections before output.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param dnn_activation: Activation function to use in DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param dnn_use_bn:  bool. Whether use BatchNormalization before activation or not in DNN
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param seed: integer ,to use as random seed.
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :param model_dir: Directory to save model parameters, graph and etc. This can
        also be used to load checkpoints from the directory into a estimator
        to continue training a previously saved model.
    :param config: tf.RunConfig object to configure the runtime settings.
    :param linear_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the linear part of the model. Defaults to FTRL optimizer.
    :param dnn_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the deep part of the model. Defaults to Adagrad optimizer.
    :param training_chief_hooks: Iterable of `tf.train.SessionRunHook` objects to
        run on the chief worker during training.
    :return: A Tensorflow Estimator  instance.

    """"""

    def _model_fn(features, labels, mode, config):
        train_flag = (mode == tf.estimator.ModeKeys.TRAIN)

        linear_logits = get_linear_logit(features, linear_feature_columns, l2_reg_linear=l2_reg_linear)

        with variable_scope(DNN_SCOPE_NAME):
            sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                                 l2_reg_embedding=l2_reg_embedding)
            att_input = concat_func(sparse_embedding_list, axis=1)

            for _ in range(att_layer_num):
                att_input = InteractingLayer(
                    att_embedding_size, att_head_num, att_res)(att_input)
            att_output = tf.keras.layers.Flatten()(att_input)

            dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)

            if len(dnn_hidden_units) > 0 and att_layer_num > 0:  # Deep & Interacting Layer
                deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input, training=train_flag)
                stack_out = tf.keras.layers.Concatenate()([att_output, deep_out])
                final_logit = tf.keras.layers.Dense(
                    1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(stack_out)
            elif len(dnn_hidden_units) > 0:  # Only Deep
                deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input, training=train_flag)
                final_logit = tf.keras.layers.Dense(
                    1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(deep_out)
            elif att_layer_num > 0:  # Only Interacting Layer
                final_logit = tf.keras.layers.Dense(
                    1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(att_output)
            else:  # Error
                raise NotImplementedError

        logits = linear_logits + final_logit

        return deepctr_model_fn(features, mode, logits, labels, task, linear_optimizer, dnn_optimizer,
                                training_chief_hooks=training_chief_hooks)

    return tf.estimator.Estimator(_model_fn, model_dir=model_dir, config=config)
"
DeepCTR,fnn.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Zhang W, Du T, Wang J. Deep learning over multi-field categorical data[C]//European conference on information retrieval. Springer, Cham, 2016: 45-57.(https://arxiv.org/pdf/1601.02376.pdf)
""""""
import tensorflow as tf

from ..feature_column import get_linear_logit, input_from_feature_columns
from ..utils import deepctr_model_fn, DNN_SCOPE_NAME, variable_scope
from ...layers.core import DNN
from ...layers.utils import combined_dnn_input


def FNNEstimator(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64),
                 l2_reg_embedding=1e-5, l2_reg_linear=1e-5, l2_reg_dnn=0, seed=1024, dnn_dropout=0,
                 dnn_activation='relu', task='binary', model_dir=None, config=None, linear_optimizer='Ftrl',
                 dnn_optimizer='Adagrad', training_chief_hooks=None):
    """"""Instantiates the Factorization-supported Neural Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_linear: float. L2 regularizer strength applied to linear weight
    :param l2_reg_dnn: float . L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :param model_dir: Directory to save model parameters, graph and etc. This can
        also be used to load checkpoints from the directory into a estimator
        to continue training a previously saved model.
    :param config: tf.RunConfig object to configure the runtime settings.
    :param linear_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the linear part of the model. Defaults to FTRL optimizer.
    :param dnn_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the deep part of the model. Defaults to Adagrad optimizer.
    :param training_chief_hooks: Iterable of `tf.train.SessionRunHook` objects to
        run on the chief worker during training.
    :return: A Tensorflow Estimator  instance.

    """"""

    def _model_fn(features, labels, mode, config):
        train_flag = (mode == tf.estimator.ModeKeys.TRAIN)

        linear_logits = get_linear_logit(features, linear_feature_columns, l2_reg_linear=l2_reg_linear)

        with variable_scope(DNN_SCOPE_NAME):
            sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                                 l2_reg_embedding=l2_reg_embedding)
            dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)
            deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed=seed)(dnn_input, training=train_flag)
            dnn_logit = tf.keras.layers.Dense(
                1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(deep_out)

        logits = linear_logits + dnn_logit

        return deepctr_model_fn(features, mode, logits, labels, task, linear_optimizer, dnn_optimizer,
                                training_chief_hooks=training_chief_hooks)

    return tf.estimator.Estimator(_model_fn, model_dir=model_dir, config=config)
"
DeepCTR,dcn.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Wang R, Fu B, Fu G, et al. Deep & cross network for ad click predictions[C]//Proceedings of the ADKDD'17. ACM, 2017: 12. (https://arxiv.org/abs/1708.05123)
""""""
import tensorflow as tf

from ..feature_column import get_linear_logit, input_from_feature_columns
from ..utils import deepctr_model_fn, DNN_SCOPE_NAME, variable_scope
from ...layers.core import DNN
from ...layers.interaction import CrossNet
from ...layers.utils import combined_dnn_input


def DCNEstimator(linear_feature_columns, dnn_feature_columns, cross_num=2, dnn_hidden_units=(256, 128, 64),
                 l2_reg_linear=1e-5,
                 l2_reg_embedding=1e-5,
                 l2_reg_cross=1e-5, l2_reg_dnn=0, seed=1024, dnn_dropout=0, dnn_use_bn=False,
                 dnn_activation='relu', task='binary', model_dir=None, config=None, linear_optimizer='Ftrl',
                 dnn_optimizer='Adagrad', training_chief_hooks=None):
    """"""Instantiates the Deep&Cross Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param cross_num: positive integet,cross layer number
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_cross: float. L2 regularizer strength applied to cross net
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not DNN
    :param dnn_activation: Activation function to use in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :param model_dir: Directory to save model parameters, graph and etc. This can
        also be used to load checkpoints from the directory into a estimator
        to continue training a previously saved model.
    :param config: tf.RunConfig object to configure the runtime settings.
    :param linear_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the linear part of the model. Defaults to FTRL optimizer.
    :param dnn_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the deep part of the model. Defaults to Adagrad optimizer.
    :param training_chief_hooks: Iterable of `tf.train.SessionRunHook` objects to
        run on the chief worker during training.
    :return: A Tensorflow Estimator  instance.

    """"""
    if len(dnn_hidden_units) == 0 and cross_num == 0:
        raise ValueError(""Either hidden_layer or cross layer must > 0"")

    def _model_fn(features, labels, mode, config):
        train_flag = (mode == tf.estimator.ModeKeys.TRAIN)

        linear_logits = get_linear_logit(features, linear_feature_columns, l2_reg_linear=l2_reg_linear)

        with variable_scope(DNN_SCOPE_NAME):
            sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                                 l2_reg_embedding=l2_reg_embedding)

            dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)

            if len(dnn_hidden_units) > 0 and cross_num > 0:  # Deep & Cross
                deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input, training=train_flag)
                cross_out = CrossNet(cross_num, l2_reg=l2_reg_cross)(dnn_input)
                stack_out = tf.keras.layers.Concatenate()([cross_out, deep_out])
                final_logit = tf.keras.layers.Dense(
                    1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(stack_out)
            elif len(dnn_hidden_units) > 0:  # Only Deep
                deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input, training=train_flag)
                final_logit = tf.keras.layers.Dense(
                    1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(deep_out)
            elif cross_num > 0:  # Only Cross
                cross_out = CrossNet(cross_num, l2_reg=l2_reg_cross)(dnn_input)
                final_logit = tf.keras.layers.Dense(
                    1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(cross_out)
            else:  # Error
                raise NotImplementedError

        logits = linear_logits + final_logit

        return deepctr_model_fn(features, mode, logits, labels, task, linear_optimizer, dnn_optimizer,
                                training_chief_hooks=training_chief_hooks)

    return tf.estimator.Estimator(_model_fn, model_dir=model_dir, config=config)
"
DeepCTR,xdeepfm.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Lian J, Zhou X, Zhang F, et al. xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems[J]. arXiv preprint arXiv:1803.05170, 2018.(https://arxiv.org/pdf/1803.05170.pdf)
""""""
import tensorflow as tf

from ..feature_column import get_linear_logit, input_from_feature_columns
from ..utils import deepctr_model_fn, DNN_SCOPE_NAME, variable_scope
from ...layers.core import DNN
from ...layers.interaction import CIN
from ...layers.utils import concat_func, add_func, combined_dnn_input


def xDeepFMEstimator(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64),
                     cin_layer_size=(128, 128,), cin_split_half=True, cin_activation='relu', l2_reg_linear=0.00001,
                     l2_reg_embedding=0.00001, l2_reg_dnn=0, l2_reg_cin=0, seed=1024, dnn_dropout=0,
                     dnn_activation='relu', dnn_use_bn=False, task='binary', model_dir=None, config=None,
                     linear_optimizer='Ftrl',
                     dnn_optimizer='Adagrad', training_chief_hooks=None):
    """"""Instantiates the xDeepFM architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net
    :param cin_layer_size: list,list of positive integer or empty list, the feature maps  in each hidden layer of Compressed Interaction Network
    :param cin_split_half: bool.if set to True, half of the feature maps in each hidden will connect to output unit
    :param cin_activation: activation function used on feature maps
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: L2 regularizer strength applied to deep net
    :param l2_reg_cin: L2 regularizer strength applied to CIN.
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :param model_dir: Directory to save model parameters, graph and etc. This can
        also be used to load checkpoints from the directory into a estimator
        to continue training a previously saved model.
    :param config: tf.RunConfig object to configure the runtime settings.
    :param linear_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the linear part of the model. Defaults to FTRL optimizer.
    :param dnn_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the deep part of the model. Defaults to Adagrad optimizer.
    :param training_chief_hooks: Iterable of `tf.train.SessionRunHook` objects to
        run on the chief worker during training.
    :return: A Tensorflow Estimator  instance.

    """"""

    def _model_fn(features, labels, mode, config):
        train_flag = (mode == tf.estimator.ModeKeys.TRAIN)

        linear_logits = get_linear_logit(features, linear_feature_columns, l2_reg_linear=l2_reg_linear)
        logits_list = [linear_logits]

        with variable_scope(DNN_SCOPE_NAME):
            sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                                 l2_reg_embedding=l2_reg_embedding)
            fm_input = concat_func(sparse_embedding_list, axis=1)

            dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)
            dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input, training=train_flag)
            dnn_logit = tf.keras.layers.Dense(
                1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(dnn_output)

            logits_list.append(dnn_logit)

            if len(cin_layer_size) > 0:
                exFM_out = CIN(cin_layer_size, cin_activation,
                               cin_split_half, l2_reg_cin, seed)(fm_input, training=train_flag)
                exFM_logit = tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.glorot_normal(seed) )(exFM_out)
                logits_list.append(exFM_logit)

        logits = add_func(logits_list)

        return deepctr_model_fn(features, mode, logits, labels, task, linear_optimizer, dnn_optimizer,
                                training_chief_hooks=training_chief_hooks)

    return tf.estimator.Estimator(_model_fn, model_dir=model_dir, config=config)
"
DeepCTR,ccpm.py,"# -*- coding:utf-8 -*-
""""""

Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Liu Q, Yu F, Wu S, et al. A convolutional click prediction model[C]//Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 2015: 1743-1746.
    (http://ir.ia.ac.cn/bitstream/173211/12337/1/A%20Convolutional%20Click%20Prediction%20Model.pdf)

""""""
import tensorflow as tf

from ..feature_column import get_linear_logit, input_from_feature_columns
from ..utils import deepctr_model_fn, DNN_SCOPE_NAME, variable_scope
from ...layers.core import DNN
from ...layers.sequence import KMaxPooling
from ...layers.utils import concat_func


def CCPMEstimator(linear_feature_columns, dnn_feature_columns, conv_kernel_width=(6, 5), conv_filters=(4, 4),
                  dnn_hidden_units=(128, 64), l2_reg_linear=1e-5, l2_reg_embedding=1e-5, l2_reg_dnn=0, dnn_dropout=0,
                  seed=1024, task='binary', model_dir=None, config=None, linear_optimizer='Ftrl',
                  dnn_optimizer='Adagrad', training_chief_hooks=None):
    """"""Instantiates the Convolutional Click Prediction Model architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param conv_kernel_width: list,list of positive integer or empty list,the width of filter in each conv layer.
    :param conv_filters: list,list of positive integer or empty list,the number of filters in each conv layer.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN.
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param init_std: float,to use as the initialize std of embedding vector
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :param model_dir: Directory to save model parameters, graph and etc. This can
        also be used to load checkpoints from the directory into a estimator
        to continue training a previously saved model.
    :param config: tf.RunConfig object to configure the runtime settings.
    :param linear_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the linear part of the model. Defaults to FTRL optimizer.
    :param dnn_optimizer: An instance of `tf.Optimizer` used to apply gradients to
        the deep part of the model. Defaults to Adagrad optimizer.
    :param training_chief_hooks: Iterable of `tf.train.SessionRunHook` objects to
        run on the chief worker during training.
    :return: A Tensorflow Estimator  instance.

    """"""

    if len(conv_kernel_width) != len(conv_filters):
        raise ValueError(
            ""conv_kernel_width must have same element with conv_filters"")

    def _model_fn(features, labels, mode, config):
        train_flag = (mode == tf.estimator.ModeKeys.TRAIN)

        linear_logits = get_linear_logit(features, linear_feature_columns, l2_reg_linear=l2_reg_linear)

        with variable_scope(DNN_SCOPE_NAME):
            sparse_embedding_list, _ = input_from_feature_columns(features, dnn_feature_columns,
                                                                                 l2_reg_embedding=l2_reg_embedding)
            n = len(sparse_embedding_list)
            l = len(conv_filters)

            conv_input = concat_func(sparse_embedding_list, axis=1)
            pooling_result = tf.keras.layers.Lambda(
                lambda x: tf.expand_dims(x, axis=3))(conv_input)

            for i in range(1, l + 1):
                filters = conv_filters[i - 1]
                width = conv_kernel_width[i - 1]
                k = max(1, int((1 - pow(i / l, l - i)) * n)) if i < l else 3

                conv_result = tf.keras.layers.Conv2D(filters=filters, kernel_size=(width, 1), strides=(1, 1),
                                                     padding='same',
                                                     activation='tanh', use_bias=True, )(pooling_result)
                pooling_result = KMaxPooling(
                    k=min(k, int(conv_result.shape[1])), axis=1)(conv_result)

            flatten_result = tf.keras.layers.Flatten()(pooling_result)
            dnn_out = DNN(dnn_hidden_units, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout, seed=seed)(flatten_result, training=train_flag)
            dnn_logit = tf.keras.layers.Dense(1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(dnn_out)

        logits = linear_logits + dnn_logit

        return deepctr_model_fn(features, mode, logits, labels, task, linear_optimizer, dnn_optimizer,
                                training_chief_hooks=training_chief_hooks
                                )

    return tf.estimator.Estimator(_model_fn, model_dir=model_dir, config=config)
"
DeepCTR,ifm.py,"# -*- coding:utf-8 -*-
""""""
Author:
    zanshuxun, zanshuxun@aliyun.com
Reference:
    [1] Yu Y, Wang Z, Yuan B. An Input-aware Factorization Machine for Sparse Prediction[C]//IJCAI. 2019: 1466-1472.
    (https://www.ijcai.org/Proceedings/2019/0203.pdf)
""""""

import tensorflow as tf
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense, Lambda

from ..feature_column import build_input_features, get_linear_logit, input_from_feature_columns, SparseFeat, \
    VarLenSparseFeat
from ..layers.core import PredictionLayer, DNN
from ..layers.interaction import FM
from ..layers.utils import concat_func, add_func, combined_dnn_input, softmax


def IFM(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64),
        l2_reg_linear=0.00001, l2_reg_embedding=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0,
        dnn_activation='relu', dnn_use_bn=False, task='binary'):
    """"""Instantiates the IFM Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    if not len(dnn_hidden_units) > 0:
        raise ValueError(""dnn_hidden_units is null!"")

    features = build_input_features(
        linear_feature_columns + dnn_feature_columns)

    sparse_feat_num = len(list(filter(lambda x: isinstance(x, SparseFeat) or isinstance(x, VarLenSparseFeat),
                                      dnn_feature_columns)))
    inputs_list = list(features.values())

    sparse_embedding_list, _ = input_from_feature_columns(features, dnn_feature_columns,
                                                          l2_reg_embedding, seed)
    if not len(sparse_embedding_list) > 0:
        raise ValueError(""there are no sparse features"")

    dnn_input = combined_dnn_input(sparse_embedding_list, [])
    dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)
    # here, dnn_output is the m'_{x}
    dnn_output = Dense(sparse_feat_num, use_bias=False)(dnn_output)
    # input_aware_factor m_{x,i}
    input_aware_factor = Lambda(lambda x: tf.cast(tf.shape(x)[-1], tf.float32) * softmax(x, dim=1))(dnn_output)

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear, sparse_feat_refine_weight=input_aware_factor)

    fm_input = concat_func(sparse_embedding_list, axis=1)
    refined_fm_input = Lambda(lambda x: x[0] * tf.expand_dims(x[1], axis=-1))(
        [fm_input, input_aware_factor])
    fm_logit = FM()(refined_fm_input)

    final_logit = add_func([linear_logit, fm_logit])

    output = PredictionLayer(task)(final_logit)
    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,pnn.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Qu Y, Cai H, Ren K, et al. Product-based neural networks for user response prediction[C]//Data Mining (ICDM), 2016 IEEE 16th International Conference on. IEEE, 2016: 1149-1154.(https://arxiv.org/pdf/1611.00144.pdf)
""""""

from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense, Reshape, Flatten

from ..feature_column import build_input_features, input_from_feature_columns
from ..layers.core import PredictionLayer, DNN
from ..layers.interaction import InnerProductLayer, OutterProductLayer
from ..layers.utils import concat_func, combined_dnn_input


def PNN(dnn_feature_columns, dnn_hidden_units=(256, 128, 64), l2_reg_embedding=0.00001, l2_reg_dnn=0,
        seed=1024, dnn_dropout=0, dnn_activation='relu', use_inner=True, use_outter=False, kernel_type='mat',
        task='binary'):
    """"""Instantiates the Product-based Neural Network architecture.

    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net
    :param l2_reg_embedding: float . L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param use_inner: bool,whether use inner-product or not.
    :param use_outter: bool,whether use outter-product or not.
    :param kernel_type: str,kernel_type used in outter-product,can be ``'mat'`` , ``'vec'`` or ``'num'``
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    if kernel_type not in ['mat', 'vec', 'num']:
        raise ValueError(""kernel_type must be mat,vec or num"")

    features = build_input_features(dnn_feature_columns)

    inputs_list = list(features.values())

    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                         l2_reg_embedding, seed)
    inner_product = Flatten()(
        InnerProductLayer()(sparse_embedding_list))
    outter_product = OutterProductLayer(kernel_type)(sparse_embedding_list)

    # ipnn deep input
    linear_signal = Reshape(
        [sum(map(lambda x: int(x.shape[-1]), sparse_embedding_list))])(concat_func(sparse_embedding_list))

    if use_inner and use_outter:
        deep_input = concat_func([linear_signal, inner_product, outter_product])
    elif use_inner:
        deep_input = concat_func([linear_signal, inner_product])
    elif use_outter:
        deep_input = concat_func([linear_signal, outter_product])
    else:
        deep_input = linear_signal

    dnn_input = combined_dnn_input([deep_input], dense_value_list)
    dnn_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed=seed)(dnn_input)
    dnn_logit = Dense(1, use_bias=False)(dnn_out)

    output = PredictionLayer(task)(dnn_logit)

    model = Model(inputs=inputs_list,
                  outputs=output)
    return model
"
DeepCTR,dcnmix.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

    Shuxun Zan, zanshuxun@aliyun.com

Reference:
    [1] Wang R, Fu B, Fu G, et al. Deep & cross network for ad click predictions[C]//Proceedings of the ADKDD'17. ACM, 2017: 12. (https://arxiv.org/abs/1708.05123)

    [2] Wang R, Shivanna R, Cheng D Z, et al. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems[J]. 2020. (https://arxiv.org/abs/2008.13535)
""""""
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense, Concatenate

from ..feature_column import build_input_features, get_linear_logit, input_from_feature_columns
from ..layers.core import PredictionLayer, DNN
from ..layers.interaction import CrossNetMix
from ..layers.utils import add_func, combined_dnn_input


def DCNMix(linear_feature_columns, dnn_feature_columns, cross_num=2,
           dnn_hidden_units=(256, 128, 64), l2_reg_linear=1e-5, l2_reg_embedding=1e-5, low_rank=32, num_experts=4,
           l2_reg_cross=1e-5, l2_reg_dnn=0, seed=1024, dnn_dropout=0, dnn_use_bn=False,
           dnn_activation='relu', task='binary'):
    """"""Instantiates the Deep&Cross Network with mixture of experts architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param cross_num: positive integet,cross layer number
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_cross: float. L2 regularizer strength applied to cross net
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not DNN
    :param dnn_activation: Activation function to use in DNN
    :param low_rank: Positive integer, dimensionality of low-rank sapce.
    :param num_experts: Positive integer, number of experts.
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.

    """"""
    if len(dnn_hidden_units) == 0 and cross_num == 0:
        raise ValueError(""Either hidden_layer or cross layer must > 0"")

    features = build_input_features(dnn_feature_columns)
    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear)

    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                         l2_reg_embedding, seed)

    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)

    if len(dnn_hidden_units) > 0 and cross_num > 0:  # Deep & Cross
        deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)
        cross_out = CrossNetMix(low_rank=low_rank, num_experts=num_experts, layer_num=cross_num,
                                l2_reg=l2_reg_cross)(dnn_input)
        stack_out = Concatenate()([cross_out, deep_out])
        final_logit = Dense(1, use_bias=False)(stack_out)
    elif len(dnn_hidden_units) > 0:  # Only Deep
        deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)
        final_logit = Dense(1, use_bias=False,)(deep_out)
    elif cross_num > 0:  # Only Cross
        cross_out = CrossNetMix(low_rank=low_rank, num_experts=num_experts, layer_num=cross_num,
                                l2_reg=l2_reg_cross)(dnn_input)
        final_logit = Dense(1, use_bias=False, )(cross_out)
    else:  # Error
        raise NotImplementedError

    final_logit = add_func([final_logit, linear_logit])
    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)

    return model
"
DeepCTR,deepfm.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Guo H, Tang R, Ye Y, et al. Deepfm: a factorization-machine based neural network for ctr prediction[J]. arXiv preprint arXiv:1703.04247, 2017.(https://arxiv.org/abs/1703.04247)

""""""

from itertools import chain

from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense

from ..feature_column import build_input_features, get_linear_logit, DEFAULT_GROUP_NAME, input_from_feature_columns
from ..layers.core import PredictionLayer, DNN
from ..layers.interaction import FM
from ..layers.utils import concat_func, add_func, combined_dnn_input


def DeepFM(linear_feature_columns, dnn_feature_columns, fm_group=(DEFAULT_GROUP_NAME,), dnn_hidden_units=(256, 128, 64),
           l2_reg_linear=0.00001, l2_reg_embedding=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0,
           dnn_activation='relu', dnn_use_bn=False, task='binary'):
    """"""Instantiates the DeepFM Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by the linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by the deep part of the model.
    :param fm_group: list, group_name of features that will be used to do feature interactions.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    features = build_input_features(
        linear_feature_columns + dnn_feature_columns)

    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear)

    group_embedding_dict, dense_value_list = input_from_feature_columns(features, dnn_feature_columns, l2_reg_embedding,
                                                                        seed, support_group=True)

    fm_logit = add_func([FM()(concat_func(v, axis=1))
                         for k, v in group_embedding_dict.items() if k in fm_group])

    dnn_input = combined_dnn_input(list(chain.from_iterable(
        group_embedding_dict.values())), dense_value_list)
    dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)
    dnn_logit = Dense(1, use_bias=False)(dnn_output)

    final_logit = add_func([linear_logit, fm_logit, dnn_logit])

    output = PredictionLayer(task)(final_logit)
    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,wdl.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Cheng H T, Koc L, Harmsen J, et al. Wide & deep learning for recommender systems[C]//Proceedings of the 1st Workshop on Deep Learning for Recommender Systems. ACM, 2016: 7-10.(https://arxiv.org/pdf/1606.07792.pdf)
""""""

from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense

from ..feature_column import build_input_features, get_linear_logit, input_from_feature_columns
from ..layers.core import PredictionLayer, DNN
from ..layers.utils import add_func, combined_dnn_input


def WDL(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64), l2_reg_linear=0.00001,
        l2_reg_embedding=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0, dnn_activation='relu',
        task='binary'):
    """"""Instantiates the Wide&Deep Learning architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to wide part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    features = build_input_features(
        linear_feature_columns + dnn_feature_columns)

    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear)

    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                         l2_reg_embedding, seed)

    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)
    dnn_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed=seed)(dnn_input)
    dnn_logit = Dense(1, use_bias=False)(dnn_out)

    final_logit = add_func([dnn_logit, linear_logit])

    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,edcn.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Yi He, heyi_jack@163.com

Reference:
    [1] Chen, B., Wang, Y., Liu, et al. Enhancing Explicit and Implicit Feature Interactions via Information Sharing for Parallel Deep CTR Models. CIKM, 2021, October (https://dlp-kdd.github.io/assets/pdf/DLP-KDD_2021_paper_12.pdf)
""""""
from tensorflow.python.keras.layers import Dense, Reshape, Concatenate
from tensorflow.python.keras.models import Model

from ..feature_column import build_input_features, get_linear_logit, input_from_feature_columns
from ..layers.core import PredictionLayer, DNN, RegulationModule
from ..layers.interaction import CrossNet, BridgeModule
from ..layers.utils import add_func, concat_func


def EDCN(linear_feature_columns,
         dnn_feature_columns,
         cross_num=2,
         cross_parameterization='vector',
         bridge_type='concatenation',
         tau=1.0,
         l2_reg_linear=1e-5,
         l2_reg_embedding=1e-5,
         l2_reg_cross=1e-5,
         l2_reg_dnn=0,
         seed=1024,
         dnn_dropout=0,
         dnn_use_bn=False,
         dnn_activation='relu',
         task='binary'):
    """"""Instantiates the Enhanced Deep&Cross Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param cross_num: positive integet,cross layer number
    :param cross_parameterization: str, ``""vector""`` or ``""matrix""``, how to parameterize the cross network.
    :param bridge_type: The type of bridge interaction, one of ``""pointwise_addition""``, ``""hadamard_product""``, ``""concatenation""`` , ``""attention_pooling""``
    :param tau: Positive float, the temperature coefficient to control distribution of field-wise gating unit
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_cross: float. L2 regularizer strength applied to cross net
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not DNN
    :param dnn_activation: Activation function to use in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.

    """"""
    if cross_num == 0:
        raise ValueError(""Cross layer num must > 0"")

    print('EDCN brige type: ', bridge_type)

    features = build_input_features(dnn_feature_columns)
    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear', l2_reg=l2_reg_linear)

    sparse_embedding_list, _ = input_from_feature_columns(
        features, dnn_feature_columns, l2_reg_embedding, seed, support_dense=False)

    emb_input = concat_func(sparse_embedding_list, axis=1)
    deep_in = RegulationModule(tau)(emb_input)
    cross_in = RegulationModule(tau)(emb_input)

    field_size = len(sparse_embedding_list)
    embedding_size = int(sparse_embedding_list[0].shape[-1])
    cross_dim = field_size * embedding_size

    for i in range(cross_num):
        cross_out = CrossNet(1, parameterization=cross_parameterization,
                             l2_reg=l2_reg_cross)(cross_in)
        deep_out = DNN([cross_dim], dnn_activation, l2_reg_dnn,
                       dnn_dropout, dnn_use_bn, seed=seed)(deep_in)
        print(cross_out, deep_out)
        bridge_out = BridgeModule(bridge_type)([cross_out, deep_out])
        if i + 1 < cross_num:
            bridge_out_list = Reshape([field_size, embedding_size])(bridge_out)
            deep_in = RegulationModule(tau)(bridge_out_list)
            cross_in = RegulationModule(tau)(bridge_out_list)

    stack_out = Concatenate()([cross_out, deep_out, bridge_out])
    final_logit = Dense(1, use_bias=False)(stack_out)

    final_logit = add_func([final_logit, linear_logit])
    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)

    return model
"
DeepCTR,onn.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Yang Y, Xu B, Shen F, et al. Operation-aware Neural Networks for User Response Prediction[J]. arXiv preprint arXiv:1904.12579, 2019. （https://arxiv.org/pdf/1904.12579）


""""""

import itertools

from tensorflow.python.keras import backend as K
from tensorflow.python.keras.layers import (Dense, Embedding, Lambda,
                                            multiply, Flatten)
try:
    from tensorflow.python.keras.layers import BatchNormalization
except ImportError:
    import tensorflow as tf
    BatchNormalization = tf.keras.layers.BatchNormalization
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.regularizers import l2

from ..feature_column import SparseFeat, VarLenSparseFeat, build_input_features, get_linear_logit
from ..inputs import get_dense_input
from ..layers.core import DNN, PredictionLayer
from ..layers.sequence import SequencePoolingLayer
from ..layers.utils import concat_func, Hash, NoMask, add_func, combined_dnn_input


def ONN(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64),
        l2_reg_embedding=1e-5, l2_reg_linear=1e-5, l2_reg_dnn=0, dnn_dropout=0,
        seed=1024, use_bn=True, reduce_sum=False, task='binary',
        ):
    """"""Instantiates the Operation-aware Neural Networks  architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part.
    :param l2_reg_dnn: float . L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param use_bn: bool,whether use bn after ffm out or not
    :param reduce_sum: bool,whether apply reduce_sum on cross vector
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    features = build_input_features(linear_feature_columns + dnn_feature_columns)

    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear)

    sparse_feature_columns = list(
        filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns)) if dnn_feature_columns else []
    varlen_sparse_feature_columns = list(
        filter(lambda x: isinstance(x, VarLenSparseFeat), dnn_feature_columns)) if dnn_feature_columns else []

    sparse_embedding = {fc_j.embedding_name: {fc_i.embedding_name: Embedding(fc_j.vocabulary_size, fc_j.embedding_dim,
                                                                             embeddings_initializer=fc_j.embeddings_initializer,
                                                                             embeddings_regularizer=l2(
                                                                                 l2_reg_embedding),
                                                                             mask_zero=isinstance(fc_j,
                                                                                                  VarLenSparseFeat),
                                                                             name='sparse_emb_' + str(
                                                                                 fc_j.embedding_name) + '_' + fc_i.embedding_name)
                                              for fc_i in
                                              sparse_feature_columns + varlen_sparse_feature_columns} for fc_j in
                        sparse_feature_columns + varlen_sparse_feature_columns}

    dense_value_list = get_dense_input(features, dnn_feature_columns)

    embed_list = []
    for fc_i, fc_j in itertools.combinations(sparse_feature_columns + varlen_sparse_feature_columns, 2):
        i_input = features[fc_i.name]
        if fc_i.use_hash:
            i_input = Hash(fc_i.vocabulary_size)(i_input)
        j_input = features[fc_j.name]
        if fc_j.use_hash:
            j_input = Hash(fc_j.vocabulary_size)(j_input)

        fc_i_embedding = feature_embedding(fc_i, fc_j, sparse_embedding, i_input)
        fc_j_embedding = feature_embedding(fc_j, fc_i, sparse_embedding, j_input)

        element_wise_prod = multiply([fc_i_embedding, fc_j_embedding])
        if reduce_sum:
            element_wise_prod = Lambda(lambda element_wise_prod: K.sum(
                element_wise_prod, axis=-1))(element_wise_prod)
        embed_list.append(element_wise_prod)

    ffm_out = Flatten()(concat_func(embed_list, axis=1))
    if use_bn:
        ffm_out = BatchNormalization()(ffm_out)
    dnn_input = combined_dnn_input([ffm_out], dense_value_list)
    dnn_out = DNN(dnn_hidden_units, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout)(dnn_input)
    dnn_logit = Dense(1, use_bias=False)(dnn_out)

    final_logit = add_func([dnn_logit, linear_logit])

    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)
    return model


def feature_embedding(fc_i, fc_j, embedding_dict, input_feature):
    fc_i_embedding = embedding_dict[fc_i.name][fc_j.name](input_feature)
    if isinstance(fc_i, SparseFeat):
        return NoMask()(fc_i_embedding)
    else:
        return SequencePoolingLayer(fc_i.combiner, supports_masking=True)(fc_i_embedding)
"
DeepCTR,__init__.py,"from .afm import AFM
from .autoint import AutoInt
from .ccpm import CCPM
from .dcn import DCN
from .dcnmix import DCNMix
from .deepfefm import DeepFEFM
from .deepfm import DeepFM
from .difm import DIFM
from .fgcnn import FGCNN
from .fibinet import FiBiNET
from .flen import FLEN
from .fnn import FNN
from .fwfm import FwFM
from .ifm import IFM
from .mlr import MLR
from .multitask import SharedBottom, ESMM, MMOE, PLE
from .nfm import NFM
from .onn import ONN
from .pnn import PNN
from .sequence import DIN, DIEN, DSIN, BST
from .wdl import WDL
from .xdeepfm import xDeepFM
from .edcn import EDCN

__all__ = [""AFM"", ""CCPM"", ""DCN"", ""IFM"", ""DIFM"", ""DCNMix"", ""MLR"", ""DeepFM"", ""MLR"", ""NFM"", ""DIN"", ""DIEN"", ""FNN"", ""PNN"",
           ""WDL"", ""xDeepFM"", ""AutoInt"", ""ONN"", ""FGCNN"", ""DSIN"", ""FiBiNET"", 'FLEN', ""FwFM"", ""BST"", ""DeepFEFM"",
           ""SharedBottom"", ""ESMM"", ""MMOE"", ""PLE"", 'EDCN']
"
DeepCTR,fibinet.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Huang T, Zhang Z, Zhang J. FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction[J]. arXiv preprint arXiv:1905.09433, 2019.
""""""

from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense, Flatten

from ..feature_column import build_input_features, get_linear_logit, input_from_feature_columns
from ..layers.core import PredictionLayer, DNN
from ..layers.interaction import SENETLayer, BilinearInteraction
from ..layers.utils import concat_func, add_func, combined_dnn_input


def FiBiNET(linear_feature_columns, dnn_feature_columns, bilinear_type='interaction', reduction_ratio=3,
            dnn_hidden_units=(256, 128, 64), l2_reg_linear=1e-5,
            l2_reg_embedding=1e-5, l2_reg_dnn=0, seed=1024, dnn_dropout=0, dnn_activation='relu',
            task='binary'):
    """"""Instantiates the Feature Importance and Bilinear feature Interaction NETwork architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param bilinear_type: str,bilinear function type used in Bilinear Interaction Layer,can be ``'all'`` , ``'each'`` or ``'interaction'``
    :param reduction_ratio: integer in [1,inf), reduction ratio used in SENET Layer
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to wide part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    features = build_input_features(linear_feature_columns + dnn_feature_columns)

    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear)

    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                         l2_reg_embedding, seed)

    senet_embedding_list = SENETLayer(
        reduction_ratio, seed)(sparse_embedding_list)

    senet_bilinear_out = BilinearInteraction(
        bilinear_type=bilinear_type, seed=seed)(senet_embedding_list)
    bilinear_out = BilinearInteraction(
        bilinear_type=bilinear_type, seed=seed)(sparse_embedding_list)

    dnn_input = combined_dnn_input([Flatten()(concat_func([senet_bilinear_out, bilinear_out]))], dense_value_list)
    dnn_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed=seed)(dnn_input)
    dnn_logit = Dense(1, use_bias=False)(dnn_out)

    final_logit = add_func([linear_logit, dnn_logit])
    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,nfm.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] He X, Chua T S. Neural factorization machines for sparse predictive analytics[C]//Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 2017: 355-364. (https://arxiv.org/abs/1708.05027)
""""""
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense, Dropout

from ..feature_column import build_input_features, get_linear_logit, input_from_feature_columns
from ..layers.core import PredictionLayer, DNN
from ..layers.interaction import BiInteractionPooling
from ..layers.utils import concat_func, add_func, combined_dnn_input


def NFM(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64),
        l2_reg_embedding=1e-5, l2_reg_linear=1e-5, l2_reg_dnn=0, seed=1024, bi_dropout=0,
        dnn_dropout=0, dnn_activation='relu', task='binary'):
    """"""Instantiates the Neural Factorization Machine architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part.
    :param l2_reg_dnn: float . L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param biout_dropout: When not ``None``, the probability we will drop out the output of BiInteractionPooling Layer.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in deep net
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    features = build_input_features(
        linear_feature_columns + dnn_feature_columns)

    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear)

    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                         l2_reg_embedding, seed)

    fm_input = concat_func(sparse_embedding_list, axis=1)
    bi_out = BiInteractionPooling()(fm_input)
    if bi_dropout:
        bi_out = Dropout(bi_dropout)(bi_out, training=None)
    dnn_input = combined_dnn_input([bi_out], dense_value_list)
    dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed=seed)(dnn_input)
    dnn_logit = Dense(1, use_bias=False)(dnn_output)

    final_logit = add_func([linear_logit, dnn_logit])

    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,flen.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Tingyi Tan, 5636374@qq.com

Reference:
    [1] Chen W, Zhan L, Ci Y, Lin C. FLEN: Leveraging Field for Scalable CTR Prediction . arXiv preprint arXiv:1911.04690, 2019.(https://arxiv.org/pdf/1911.04690)

""""""

from itertools import chain

from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense

from ..feature_column import build_input_features, get_linear_logit, input_from_feature_columns
from ..layers.core import PredictionLayer, DNN
from ..layers.interaction import FieldWiseBiInteraction
from ..layers.utils import concat_func, add_func, combined_dnn_input


def FLEN(linear_feature_columns,
         dnn_feature_columns,
         dnn_hidden_units=(256, 128, 64),
         l2_reg_linear=0.00001,
         l2_reg_embedding=0.00001,
         l2_reg_dnn=0,
         seed=1024,
         dnn_dropout=0.0,
         dnn_activation='relu',
         dnn_use_bn=False,
         task='binary'):
    """"""Instantiates the FLEN Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    features = build_input_features(linear_feature_columns +
                                    dnn_feature_columns)

    inputs_list = list(features.values())

    group_embedding_dict, dense_value_list = input_from_feature_columns(
        features,
        dnn_feature_columns,
        l2_reg_embedding,
        seed,
        support_group=True)

    linear_logit = get_linear_logit(features,
                                    linear_feature_columns,
                                    seed=seed,
                                    prefix='linear',
                                    l2_reg=l2_reg_linear)

    fm_mf_out = FieldWiseBiInteraction(seed=seed)(
        [concat_func(v, axis=1) for k, v in group_embedding_dict.items()])

    dnn_input = combined_dnn_input(
        list(chain.from_iterable(group_embedding_dict.values())),
        dense_value_list)
    dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)

    dnn_logit = Dense(1, use_bias=False)(concat_func([fm_mf_out, dnn_output]))

    final_logit = add_func([linear_logit, dnn_logit])
    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,afm.py,"# -*- coding:utf-8 -*-
""""""

Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Xiao J, Ye H, He X, et al. Attentional factorization machines: Learning the weight of feature interactions via attention networks[J]. arXiv preprint arXiv:1708.04617, 2017.
    (https://arxiv.org/abs/1708.04617)

""""""
from tensorflow.python.keras.models import Model
from ..feature_column import build_input_features, get_linear_logit, DEFAULT_GROUP_NAME, input_from_feature_columns
from ..layers.core import PredictionLayer
from ..layers.interaction import AFMLayer, FM
from ..layers.utils import concat_func, add_func


def AFM(linear_feature_columns, dnn_feature_columns, fm_group=DEFAULT_GROUP_NAME, use_attention=True,
        attention_factor=8,
        l2_reg_linear=1e-5, l2_reg_embedding=1e-5, l2_reg_att=1e-5, afm_dropout=0, seed=1024,
        task='binary'):
    """"""Instantiates the Attentional Factorization Machine architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param fm_group: list, group_name of features that will be used to do feature interactions.
    :param use_attention: bool,whether use attention or not,if set to ``False``.it is the same as **standard Factorization Machine**
    :param attention_factor: positive integer,units in attention net
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_att: float. L2 regularizer strength applied to attention net
    :param afm_dropout: float in [0,1), Fraction of the attention net output units to dropout.
    :param seed: integer ,to use as random seed.
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    features = build_input_features(
        linear_feature_columns + dnn_feature_columns)

    inputs_list = list(features.values())

    group_embedding_dict, _ = input_from_feature_columns(features, dnn_feature_columns, l2_reg_embedding,
                                                         seed, support_dense=False, support_group=True)

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear)

    if use_attention:
        fm_logit = add_func([AFMLayer(attention_factor, l2_reg_att, afm_dropout,
                                      seed)(list(v)) for k, v in group_embedding_dict.items() if k in fm_group])
    else:
        fm_logit = add_func([FM()(concat_func(v, axis=1))
                             for k, v in group_embedding_dict.items() if k in fm_group])

    final_logit = add_func([linear_logit, fm_logit])
    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,deepfefm.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Harshit Pande

Reference:
    [1] Field-Embedded Factorization Machines for Click-through Rate Prediction]
    (https://arxiv.org/pdf/2009.09931.pdf)

    this file also supports all the possible Ablation studies for reproducibility

""""""

from itertools import chain

from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense, Lambda

from ..feature_column import input_from_feature_columns, get_linear_logit, build_input_features, DEFAULT_GROUP_NAME
from ..layers.core import PredictionLayer, DNN
from ..layers.interaction import FEFMLayer
from ..layers.utils import concat_func, combined_dnn_input, reduce_sum, add_func


def DeepFEFM(linear_feature_columns, dnn_feature_columns, use_fefm=True,
             dnn_hidden_units=(256, 128, 64), l2_reg_linear=0.00001, l2_reg_embedding_feat=0.00001,
             l2_reg_embedding_field=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0.0,
             exclude_feature_embed_in_dnn=False,
             use_linear=True, use_fefm_embed_in_dnn=True, dnn_activation='relu', dnn_use_bn=False, task='binary'):
    """"""Instantiates the DeepFEFM Network architecture or the shallow FEFM architecture (Ablation studies supported)

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param fm_group: list, group_name of features that will be used to do feature interactions.
    :param use_fefm: bool,use FEFM logit or not (doesn't effect FEFM embeddings in DNN, controls only the use of final FEFM logit)
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding_feat: float. L2 regularizer strength applied to embedding vector of features
    :param l2_reg_embedding_field: float, L2 regularizer to field embeddings
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param exclude_feature_embed_in_dnn: bool, used in ablation studies for removing feature embeddings in DNN
    :param use_linear: bool, used in ablation studies
    :param use_fefm_embed_in_dnn: bool, True if FEFM interaction embeddings are to be used in FEFM (set False for Ablation)
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    features = build_input_features(linear_feature_columns + dnn_feature_columns)

    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, l2_reg=l2_reg_linear, seed=seed, prefix='linear')

    group_embedding_dict, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                        l2_reg_embedding_feat,
                                                                        seed, support_group=True)

    fefm_interaction_embedding = concat_func([FEFMLayer(
        regularizer=l2_reg_embedding_field)(concat_func(v, axis=1))
                                              for k, v in group_embedding_dict.items() if k in [DEFAULT_GROUP_NAME]],
                                             axis=1)

    dnn_input = combined_dnn_input(list(chain.from_iterable(group_embedding_dict.values())), dense_value_list)

    # if use_fefm_embed_in_dnn is set to False it is Ablation4 (Use false only for Ablation)
    if use_fefm_embed_in_dnn:
        if exclude_feature_embed_in_dnn:
            # Ablation3: remove feature vector embeddings from the DNN input
            dnn_input = fefm_interaction_embedding
        else:
            # No ablation
            dnn_input = concat_func([dnn_input, fefm_interaction_embedding], axis=1)

    dnn_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)

    dnn_logit = Dense(1, use_bias=False, )(dnn_out)

    fefm_logit = Lambda(lambda x: reduce_sum(x, axis=1, keep_dims=True))(fefm_interaction_embedding)

    if len(dnn_hidden_units) == 0 and use_fefm is False and use_linear is True:  # only linear
        final_logit = linear_logit
    elif len(dnn_hidden_units) == 0 and use_fefm is True and use_linear is True:  # linear + FEFM
        final_logit = add_func([linear_logit, fefm_logit])
    elif len(dnn_hidden_units) > 0 and use_fefm is False and use_linear is True:  # linear +　Deep # Ablation1
        final_logit = add_func([linear_logit, dnn_logit])
    elif len(dnn_hidden_units) > 0 and use_fefm is True and use_linear is True:  # linear + FEFM + Deep
        final_logit = add_func([linear_logit, fefm_logit, dnn_logit])
    elif len(dnn_hidden_units) == 0 and use_fefm is True and use_linear is False:  # only FEFM (shallow)
        final_logit = fefm_logit
    elif len(dnn_hidden_units) > 0 and use_fefm is False and use_linear is False:  # only Deep
        final_logit = dnn_logit
    elif len(dnn_hidden_units) > 0 and use_fefm is True and use_linear is False:  # FEFM + Deep # Ablation2
        final_logit = add_func([fefm_logit, dnn_logit])
    else:
        raise NotImplementedError

    output = PredictionLayer(task)(final_logit)
    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,fwfm.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Harshit Pande

Reference:
    [1] Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising
    (https://arxiv.org/pdf/1806.03514.pdf)

""""""

from itertools import chain

from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense

from ..feature_column import build_input_features, get_linear_logit, DEFAULT_GROUP_NAME, input_from_feature_columns
from ..layers.core import PredictionLayer, DNN
from ..layers.interaction import FwFMLayer
from ..layers.utils import concat_func, add_func, combined_dnn_input


def FwFM(linear_feature_columns, dnn_feature_columns, fm_group=(DEFAULT_GROUP_NAME,), dnn_hidden_units=(256, 128, 64),
         l2_reg_linear=0.00001, l2_reg_embedding=0.00001, l2_reg_field_strength=0.00001, l2_reg_dnn=0,
         seed=1024, dnn_dropout=0, dnn_activation='relu', dnn_use_bn=False, task='binary'):
    """"""Instantiates the FwFM Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param fm_group: list, group_name of features that will be used to do feature interactions.
    :param dnn_hidden_units: list,list of positive integer or empty list if do not want DNN, the layer number and units
    in each layer of DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_field_strength: float. L2 regularizer strength applied to the field pair strength parameters
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    features = build_input_features(linear_feature_columns + dnn_feature_columns)

    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear)

    group_embedding_dict, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                        l2_reg_embedding, seed,
                                                                        support_group=True)

    fwfm_logit = add_func([FwFMLayer(num_fields=len(v), regularizer=l2_reg_field_strength)
                           (concat_func(v, axis=1)) for k, v in group_embedding_dict.items() if k in fm_group])

    final_logit_components = [linear_logit, fwfm_logit]

    if dnn_hidden_units:
        dnn_input = combined_dnn_input(list(chain.from_iterable(
            group_embedding_dict.values())), dense_value_list)
        dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)
        dnn_logit = Dense(1, use_bias=False)(dnn_output)
        final_logit_components.append(dnn_logit)

    final_logit = add_func(final_logit_components)

    output = PredictionLayer(task)(final_logit)
    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,mlr.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Gai K, Zhu X, Li H, et al. Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction[J]. arXiv preprint arXiv:1704.05194, 2017.(https://arxiv.org/abs/1704.05194)
""""""
from tensorflow.python.keras.layers import Activation, dot
from tensorflow.python.keras.models import Model

from ..feature_column import build_input_features, get_linear_logit
from ..layers.core import PredictionLayer
from ..layers.utils import concat_func


def MLR(region_feature_columns, base_feature_columns=None, region_num=4,
        l2_reg_linear=1e-5, seed=1024, task='binary',
        bias_feature_columns=None):
    """"""Instantiates the Mixed Logistic Regression/Piece-wise Linear Model.

    :param region_feature_columns: An iterable containing all the features used by region part of the model.
    :param base_feature_columns: An iterable containing all the features used by base part of the model.
    :param region_num: integer > 1,indicate the piece number
    :param l2_reg_linear: float. L2 regularizer strength applied to weight
    :param seed: integer ,to use as random seed.
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :param bias_feature_columns: An iterable containing all the features used by bias part of the model.
    :return: A Keras model instance.
    """"""

    if region_num <= 1:
        raise ValueError(""region_num must > 1"")

    if base_feature_columns is None or len(base_feature_columns) == 0:
        base_feature_columns = region_feature_columns

    if bias_feature_columns is None:
        bias_feature_columns = []

    features = build_input_features(region_feature_columns + base_feature_columns + bias_feature_columns)

    inputs_list = list(features.values())

    region_score = get_region_score(features, region_feature_columns, region_num, l2_reg_linear, seed)
    learner_score = get_learner_score(features, base_feature_columns, region_num, l2_reg_linear, seed, task=task)

    final_logit = dot([region_score, learner_score], axes=-1)

    if bias_feature_columns is not None and len(bias_feature_columns) > 0:
        bias_score = get_learner_score(features, bias_feature_columns, 1, l2_reg_linear, seed, prefix='bias_',
                                       task='binary')

        final_logit = dot([final_logit, bias_score], axes=-1)

    model = Model(inputs=inputs_list, outputs=final_logit)
    return model


def get_region_score(features, feature_columns, region_number, l2_reg, seed, prefix='region_', seq_mask_zero=True):
    region_logit = concat_func([get_linear_logit(features, feature_columns, seed=seed + i,
                                                 prefix=prefix + str(i + 1), l2_reg=l2_reg) for i in
                                range(region_number)])
    return Activation('softmax')(region_logit)


def get_learner_score(features, feature_columns, region_number, l2_reg, seed, prefix='learner_', seq_mask_zero=True,
                      task='binary'):
    region_score = [PredictionLayer(task=task, use_bias=False)(
        get_linear_logit(features, feature_columns, seed=seed + i, prefix=prefix + str(i + 1),
                         l2_reg=l2_reg)) for i in
        range(region_number)]

    return concat_func(region_score)
"
DeepCTR,autoint.py,"# -*- coding:utf-8 -*-
""""""

Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Song W, Shi C, Xiao Z, et al. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks[J]. arXiv preprint arXiv:1810.11921, 2018.(https://arxiv.org/abs/1810.11921)

""""""

from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Flatten, Concatenate, Dense

from ..feature_column import build_input_features, get_linear_logit, input_from_feature_columns
from ..layers.core import PredictionLayer, DNN
from ..layers.interaction import InteractingLayer
from ..layers.utils import concat_func, add_func, combined_dnn_input


def AutoInt(linear_feature_columns, dnn_feature_columns, att_layer_num=3, att_embedding_size=8, att_head_num=2,
            att_res=True,
            dnn_hidden_units=(256, 128, 64), dnn_activation='relu', l2_reg_linear=1e-5,
            l2_reg_embedding=1e-5, l2_reg_dnn=0, dnn_use_bn=False, dnn_dropout=0, seed=1024,
            task='binary', ):
    """"""Instantiates the AutoInt Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param att_layer_num: int.The InteractingLayer number to be used.
    :param att_embedding_size: int.The embedding size in multi-head self-attention network.
    :param att_head_num: int.The head number in multi-head  self-attention network.
    :param att_res: bool.Whether or not use standard residual connections before output.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param dnn_activation: Activation function to use in DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param dnn_use_bn:  bool. Whether use BatchNormalization before activation or not in DNN
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param seed: integer ,to use as random seed.
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    if len(dnn_hidden_units) <= 0 and att_layer_num <= 0:
        raise ValueError(""Either hidden_layer or att_layer_num must > 0"")

    features = build_input_features(dnn_feature_columns)
    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear)

    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                         l2_reg_embedding, seed)

    att_input = concat_func(sparse_embedding_list, axis=1)

    for _ in range(att_layer_num):
        att_input = InteractingLayer(
            att_embedding_size, att_head_num, att_res)(att_input)
    att_output = Flatten()(att_input)

    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)

    if len(dnn_hidden_units) > 0 and att_layer_num > 0:  # Deep & Interacting Layer
        deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)
        stack_out = Concatenate()([att_output, deep_out])
        final_logit = Dense(1, use_bias=False)(stack_out)
    elif len(dnn_hidden_units) > 0:  # Only Deep
        deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input, )
        final_logit = Dense(1, use_bias=False)(deep_out)
    elif att_layer_num > 0:  # Only Interacting Layer
        final_logit = Dense(1, use_bias=False)(att_output)
    else:  # Error
        raise NotImplementedError

    final_logit = add_func([final_logit, linear_logit])
    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)

    return model
"
DeepCTR,difm.py,"# -*- coding:utf-8 -*-
""""""
Author:
    zanshuxun, zanshuxun@aliyun.com
Reference:
    [1] Lu W, Yu Y, Chang Y, et al. A Dual Input-aware Factorization Machine for CTR Prediction[C]
    //IJCAI. 2020: 3139-3145.(https://www.ijcai.org/Proceedings/2020/0434.pdf)
""""""
import tensorflow as tf
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense, Lambda, Flatten

from ..feature_column import build_input_features, get_linear_logit, input_from_feature_columns, SparseFeat, \
    VarLenSparseFeat
from ..layers.core import PredictionLayer, DNN
from ..layers.interaction import FM, InteractingLayer
from ..layers.utils import concat_func, add_func, combined_dnn_input


def DIFM(linear_feature_columns, dnn_feature_columns,
         att_embedding_size=8, att_head_num=8, att_res=True, dnn_hidden_units=(256, 128, 64),
         l2_reg_linear=0.00001, l2_reg_embedding=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0,
         dnn_activation='relu', dnn_use_bn=False, task='binary'):
    """"""Instantiates the DIFM Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param att_embedding_size: integer, the embedding size in multi-head self-attention network.
    :param att_head_num: int. The head number in multi-head  self-attention network.
    :param att_res: bool. Whether or not use standard residual connections before output.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    if not len(dnn_hidden_units) > 0:
        raise ValueError(""dnn_hidden_units is null!"")

    features = build_input_features(
        linear_feature_columns + dnn_feature_columns)

    sparse_feat_num = len(list(filter(lambda x: isinstance(x, SparseFeat) or isinstance(x, VarLenSparseFeat),
                                      dnn_feature_columns)))
    inputs_list = list(features.values())

    sparse_embedding_list, _ = input_from_feature_columns(features, dnn_feature_columns,
                                                          l2_reg_embedding, seed)

    if not len(sparse_embedding_list) > 0:
        raise ValueError(""there are no sparse features"")

    att_input = concat_func(sparse_embedding_list, axis=1)
    att_out = InteractingLayer(att_embedding_size, att_head_num, att_res, scaling=True)(att_input)
    att_out = Flatten()(att_out)
    m_vec = Dense(sparse_feat_num, use_bias=False)(att_out)

    dnn_input = combined_dnn_input(sparse_embedding_list, [])
    dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)
    m_bit = Dense(sparse_feat_num, use_bias=False)(dnn_output)

    input_aware_factor = add_func([m_vec, m_bit])  # the complete input-aware factor m_x

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear, sparse_feat_refine_weight=input_aware_factor)

    fm_input = concat_func(sparse_embedding_list, axis=1)
    refined_fm_input = Lambda(lambda x: x[0] * tf.expand_dims(x[1], axis=-1))(
        [fm_input, input_aware_factor])
    fm_logit = FM()(refined_fm_input)

    final_logit = add_func([linear_logit, fm_logit])

    output = PredictionLayer(task)(final_logit)
    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,fnn.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Zhang W, Du T, Wang J. Deep learning over multi-field categorical data[C]//European conference on information retrieval. Springer, Cham, 2016: 45-57.(https://arxiv.org/pdf/1601.02376.pdf)
""""""
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense

from ..feature_column import build_input_features, get_linear_logit, input_from_feature_columns
from ..layers.core import PredictionLayer, DNN
from ..layers.utils import add_func, combined_dnn_input


def FNN(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64),
        l2_reg_embedding=1e-5, l2_reg_linear=1e-5, l2_reg_dnn=0, seed=1024, dnn_dropout=0,
        dnn_activation='relu', task='binary'):
    """"""Instantiates the Factorization-supported Neural Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_linear: float. L2 regularizer strength applied to linear weight
    :param l2_reg_dnn: float . L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""
    features = build_input_features(
        linear_feature_columns + dnn_feature_columns)

    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear)

    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                         l2_reg_embedding, seed)

    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)
    deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed=seed)(dnn_input)
    dnn_logit = Dense(1, use_bias=False)(deep_out)
    final_logit = add_func([dnn_logit, linear_logit])

    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,fgcnn.py,"# -*- coding:utf-8 -*-
""""""

Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Liu B, Tang R, Chen Y, et al. Feature Generation by Convolutional Neural Network for Click-Through Rate Prediction[J]. arXiv preprint arXiv:1904.04447, 2019.
    (https://arxiv.org/pdf/1904.04447)

""""""
import tensorflow as tf
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense, Lambda, Flatten, Concatenate

from ..feature_column import build_input_features, get_linear_logit, input_from_feature_columns
from ..layers.core import PredictionLayer, DNN
from ..layers.interaction import InnerProductLayer, FGCNNLayer
from ..layers.utils import concat_func, add_func


def unstack(input_tensor):
    input_ = tf.expand_dims(input_tensor, axis=2)
    return tf.unstack(input_, input_.shape[1], 1)


def FGCNN(linear_feature_columns, dnn_feature_columns, conv_kernel_width=(7, 7, 7, 7), conv_filters=(14, 16, 18, 20),
          new_maps=(3, 3, 3, 3),
          pooling_width=(2, 2, 2, 2), dnn_hidden_units=(256, 128, 64), l2_reg_linear=1e-5, l2_reg_embedding=1e-5,
          l2_reg_dnn=0,
          dnn_dropout=0,
          seed=1024,
          task='binary', ):
    """"""Instantiates the Feature Generation by Convolutional Neural Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param conv_kernel_width: list,list of positive integer or empty list,the width of filter in each conv layer.
    :param conv_filters: list,list of positive integer or empty list,the number of filters in each conv layer.
    :param new_maps: list, list of positive integer or empty list, the feature maps of generated features.
    :param pooling_width: list, list of positive integer or empty list,the width of pooling layer.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net.
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param seed: integer ,to use as random seed.
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    if not (len(conv_kernel_width) == len(conv_filters) == len(new_maps) == len(pooling_width)):
        raise ValueError(
            ""conv_kernel_width,conv_filters,new_maps  and pooling_width must have same length"")

    features = build_input_features(dnn_feature_columns)

    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear)

    deep_emb_list, _ = input_from_feature_columns(features, dnn_feature_columns, l2_reg_embedding, seed)
    fg_deep_emb_list, _ = input_from_feature_columns(features, dnn_feature_columns, l2_reg_embedding, seed,
                                                     prefix='fg')

    fg_input = concat_func(fg_deep_emb_list, axis=1)
    origin_input = concat_func(deep_emb_list, axis=1)

    if len(conv_filters) > 0:
        new_features = FGCNNLayer(
            conv_filters, conv_kernel_width, new_maps, pooling_width)(fg_input)
        combined_input = concat_func([origin_input, new_features], axis=1)
    else:
        combined_input = origin_input
    inner_product = Flatten()(
        InnerProductLayer()(Lambda(unstack, mask=[None] * int(combined_input.shape[1]))(combined_input)))
    linear_signal = Flatten()(combined_input)
    dnn_input = Concatenate()([linear_signal, inner_product])
    dnn_input = Flatten()(dnn_input)

    final_logit = DNN(dnn_hidden_units, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout)(dnn_input)
    final_logit = Dense(1, use_bias=False)(final_logit)

    final_logit = add_func([final_logit, linear_logit])
    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,dcn.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

    Shuxun Zan, zanshuxun@aliyun.com

Reference:
    [1] Wang R, Fu B, Fu G, et al. Deep & cross network for ad click predictions[C]//Proceedings of the ADKDD'17. ACM, 2017: 12. (https://arxiv.org/abs/1708.05123)

    [2] Wang R, Shivanna R, Cheng D Z, et al. DCN-M: Improved Deep & Cross Network for Feature Cross Learning in Web-scale Learning to Rank Systems[J]. 2020. (https://arxiv.org/abs/2008.13535)
""""""
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense, Concatenate

from ..feature_column import build_input_features, get_linear_logit, input_from_feature_columns
from ..layers.core import PredictionLayer, DNN
from ..layers.interaction import CrossNet
from ..layers.utils import add_func, combined_dnn_input


def DCN(linear_feature_columns, dnn_feature_columns, cross_num=2, cross_parameterization='vector',
        dnn_hidden_units=(256, 128, 64), l2_reg_linear=1e-5, l2_reg_embedding=1e-5,
        l2_reg_cross=1e-5, l2_reg_dnn=0, seed=1024, dnn_dropout=0, dnn_use_bn=False,
        dnn_activation='relu', task='binary'):
    """"""Instantiates the Deep&Cross Network architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param cross_num: positive integet,cross layer number
    :param cross_parameterization: str, ``""vector""`` or ``""matrix""``, how to parameterize the cross network.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_cross: float. L2 regularizer strength applied to cross net
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not DNN
    :param dnn_activation: Activation function to use in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.

    """"""
    if len(dnn_hidden_units) == 0 and cross_num == 0:
        raise ValueError(""Either hidden_layer or cross layer must > 0"")

    features = build_input_features(dnn_feature_columns)
    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear)

    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                         l2_reg_embedding, seed)

    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)

    if len(dnn_hidden_units) > 0 and cross_num > 0:  # Deep & Cross
        deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)
        cross_out = CrossNet(cross_num, parameterization=cross_parameterization, l2_reg=l2_reg_cross)(dnn_input)
        stack_out = Concatenate()([cross_out, deep_out])
        final_logit = Dense(1, use_bias=False)(stack_out)
    elif len(dnn_hidden_units) > 0:  # Only Deep
        deep_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)
        final_logit = Dense(1, use_bias=False)(deep_out)
    elif cross_num > 0:  # Only Cross
        cross_out = CrossNet(cross_num, parameterization=cross_parameterization, l2_reg=l2_reg_cross)(dnn_input)
        final_logit = Dense(1, use_bias=False)(cross_out)
    else:  # Error
        raise NotImplementedError

    final_logit = add_func([final_logit, linear_logit])
    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)

    return model
"
DeepCTR,xdeepfm.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Lian J, Zhou X, Zhang F, et al. xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems[J]. arXiv preprint arXiv:1803.05170, 2018.(https://arxiv.org/pdf/1803.05170.pdf)
""""""
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense

from ..feature_column import build_input_features, get_linear_logit, input_from_feature_columns
from ..layers.core import PredictionLayer, DNN
from ..layers.interaction import CIN
from ..layers.utils import concat_func, add_func, combined_dnn_input


def xDeepFM(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64),
            cin_layer_size=(128, 128,), cin_split_half=True, cin_activation='relu', l2_reg_linear=0.00001,
            l2_reg_embedding=0.00001, l2_reg_dnn=0, l2_reg_cin=0, seed=1024, dnn_dropout=0,
            dnn_activation='relu', dnn_use_bn=False, task='binary'):
    """"""Instantiates the xDeepFM architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net
    :param cin_layer_size: list,list of positive integer or empty list, the feature maps  in each hidden layer of Compressed Interaction Network
    :param cin_split_half: bool.if set to True, half of the feature maps in each hidden will connect to output unit
    :param cin_activation: activation function used on feature maps
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: L2 regularizer strength applied to deep net
    :param l2_reg_cin: L2 regularizer strength applied to CIN.
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    features = build_input_features(
        linear_feature_columns + dnn_feature_columns)

    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',
                                    l2_reg=l2_reg_linear)

    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                         l2_reg_embedding, seed)

    fm_input = concat_func(sparse_embedding_list, axis=1)

    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)
    dnn_output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)
    dnn_logit = Dense(1, use_bias=False)(dnn_output)

    final_logit = add_func([linear_logit, dnn_logit])

    if len(cin_layer_size) > 0:
        exFM_out = CIN(cin_layer_size, cin_activation,
                       cin_split_half, l2_reg_cin, seed)(fm_input)
        exFM_logit = Dense(1, use_bias=False)(exFM_out)
        final_logit = add_func([final_logit, exFM_logit])

    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,ccpm.py,"# -*- coding:utf-8 -*-
""""""

Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Liu Q, Yu F, Wu S, et al. A convolutional click prediction model[C]//Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 2015: 1743-1746.
    (http://ir.ia.ac.cn/bitstream/173211/12337/1/A%20Convolutional%20Click%20Prediction%20Model.pdf)

""""""
import tensorflow as tf
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Lambda

from ..feature_column import build_input_features, get_linear_logit, input_from_feature_columns
from ..layers.core import DNN, PredictionLayer
from ..layers.sequence import KMaxPooling
from ..layers.utils import concat_func, add_func


def CCPM(linear_feature_columns, dnn_feature_columns, conv_kernel_width=(6, 5), conv_filters=(4, 4),
         dnn_hidden_units=(128, 64), l2_reg_linear=1e-5, l2_reg_embedding=1e-5, l2_reg_dnn=0, dnn_dropout=0,
         seed=1024, task='binary'):
    """"""Instantiates the Convolutional Click Prediction Model architecture.

    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.
    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param conv_kernel_width: list,list of positive integer or empty list,the width of filter in each conv layer.
    :param conv_filters: list,list of positive integer or empty list,the number of filters in each conv layer.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN.
    :param l2_reg_linear: float. L2 regularizer strength applied to linear part
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param init_std: float,to use as the initialize std of embedding vector
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.
    """"""

    if len(conv_kernel_width) != len(conv_filters):
        raise ValueError(
            ""conv_kernel_width must have same element with conv_filters"")

    features = build_input_features(
        linear_feature_columns + dnn_feature_columns)
    inputs_list = list(features.values())

    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed,
                                    l2_reg=l2_reg_linear)

    sparse_embedding_list, _ = input_from_feature_columns(features, dnn_feature_columns, l2_reg_embedding,
                                                          seed, support_dense=False)

    n = len(sparse_embedding_list)
    l = len(conv_filters)

    conv_input = concat_func(sparse_embedding_list, axis=1)
    pooling_result = Lambda(
        lambda x: tf.expand_dims(x, axis=3))(conv_input)

    for i in range(1, l + 1):
        filters = conv_filters[i - 1]
        width = conv_kernel_width[i - 1]
        k = max(1, int((1 - pow(i / l, l - i)) * n)) if i < l else 3

        conv_result = Conv2D(filters=filters, kernel_size=(width, 1), strides=(1, 1), padding='same',
                             activation='tanh', use_bias=True, )(pooling_result)
        pooling_result = KMaxPooling(
            k=min(k, int(conv_result.shape[1])), axis=1)(conv_result)

    flatten_result = Flatten()(pooling_result)
    dnn_out = DNN(dnn_hidden_units, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout)(flatten_result)
    dnn_logit = Dense(1, use_bias=False)(
        dnn_out)

    final_logit = add_func([dnn_logit, linear_logit])

    output = PredictionLayer(task)(final_logit)
    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,ple.py,"""""""
Author:
    Mincai Lai, laimc@shanghaitech.edu.cn

    Weichen Shen, weichenswc@163.com

Reference:
    [1] Tang H, Liu J, Zhao M, et al. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations[C]//Fourteenth ACM Conference on Recommender Systems. 2020.(https://dl.acm.org/doi/10.1145/3383313.3412236)
""""""

import tensorflow as tf
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense, Lambda

from ...feature_column import build_input_features, input_from_feature_columns
from ...layers.core import PredictionLayer, DNN
from ...layers.utils import combined_dnn_input, reduce_sum


def PLE(dnn_feature_columns, shared_expert_num=1, specific_expert_num=1, num_levels=2,
        expert_dnn_hidden_units=(256,), tower_dnn_hidden_units=(64,), gate_dnn_hidden_units=(),
        l2_reg_embedding=0.00001,
        l2_reg_dnn=0, seed=1024, dnn_dropout=0, dnn_activation='relu', dnn_use_bn=False,
        task_types=('binary', 'binary'), task_names=('ctr', 'ctcvr')):
    """"""Instantiates the multi level of Customized Gate Control of Progressive Layered Extraction architecture.

    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param shared_expert_num: integer, number of task-shared experts.
    :param specific_expert_num: integer, number of task-specific experts.
    :param num_levels: integer, number of CGC levels.
    :param expert_dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of expert DNN.
    :param tower_dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of task-specific DNN.
    :param gate_dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of gate DNN.
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector.
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN.
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN.
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN.
    :param task_types: list of str, indicating the loss of each tasks, ``""binary""`` for  binary logloss, ``""regression""`` for regression loss. e.g. ['binary', 'regression']
    :param task_names: list of str, indicating the predict target of each tasks

    :return: a Keras model instance.
    """"""
    num_tasks = len(task_names)
    if num_tasks <= 1:
        raise ValueError(""num_tasks must be greater than 1"")

    if len(task_types) != num_tasks:
        raise ValueError(""num_tasks must be equal to the length of task_types"")

    for task_type in task_types:
        if task_type not in ['binary', 'regression']:
            raise ValueError(""task must be binary or regression, {} is illegal"".format(task_type))

    features = build_input_features(dnn_feature_columns)

    inputs_list = list(features.values())

    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                         l2_reg_embedding, seed)
    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)

    # single Extraction Layer
    def cgc_net(inputs, level_name, is_last=False):
        # inputs: [task1, task2, ... taskn, shared task]
        specific_expert_outputs = []
        # build task-specific expert layer
        for i in range(num_tasks):
            for j in range(specific_expert_num):
                expert_network = DNN(expert_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn,
                                     seed=seed,
                                     name=level_name + 'task_' + task_names[i] + '_expert_specific_' + str(j))(
                    inputs[i])
                specific_expert_outputs.append(expert_network)

        # build task-shared expert layer
        shared_expert_outputs = []
        for k in range(shared_expert_num):
            expert_network = DNN(expert_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn,
                                 seed=seed,
                                 name=level_name + 'expert_shared_' + str(k))(inputs[-1])
            shared_expert_outputs.append(expert_network)

        # task_specific gate (count = num_tasks)
        cgc_outs = []
        for i in range(num_tasks):
            # concat task-specific expert and task-shared expert
            cur_expert_num = specific_expert_num + shared_expert_num
            # task_specific + task_shared
            cur_experts = specific_expert_outputs[
                          i * specific_expert_num:(i + 1) * specific_expert_num] + shared_expert_outputs

            expert_concat = Lambda(lambda x: tf.stack(x, axis=1))(cur_experts)

            # build gate layers
            gate_input = DNN(gate_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn,
                             seed=seed,
                             name=level_name + 'gate_specific_' + task_names[i])(
                inputs[i])  # gate[i] for task input[i]
            gate_out = Dense(cur_expert_num, use_bias=False, activation='softmax',
                                             name=level_name + 'gate_softmax_specific_' + task_names[i])(gate_input)
            gate_out = Lambda(lambda x: tf.expand_dims(x, axis=-1))(gate_out)

            # gate multiply the expert
            gate_mul_expert = Lambda(lambda x: reduce_sum(x[0] * x[1], axis=1, keep_dims=False),
                                                     name=level_name + 'gate_mul_expert_specific_' + task_names[i])(
                [expert_concat, gate_out])
            cgc_outs.append(gate_mul_expert)

        # task_shared gate, if the level not in last, add one shared gate
        if not is_last:
            cur_expert_num = num_tasks * specific_expert_num + shared_expert_num
            cur_experts = specific_expert_outputs + shared_expert_outputs  # all the expert include task-specific expert and task-shared expert

            expert_concat = Lambda(lambda x: tf.stack(x, axis=1))(cur_experts)

            # build gate layers
            gate_input = DNN(gate_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn,
                             seed=seed,
                             name=level_name + 'gate_shared')(inputs[-1])  # gate for shared task input

            gate_out = Dense(cur_expert_num, use_bias=False, activation='softmax',
                                             name=level_name + 'gate_softmax_shared')(gate_input)
            gate_out = Lambda(lambda x: tf.expand_dims(x, axis=-1))(gate_out)

            # gate multiply the expert
            gate_mul_expert = Lambda(lambda x: reduce_sum(x[0] * x[1], axis=1, keep_dims=False),
                                                     name=level_name + 'gate_mul_expert_shared')(
                [expert_concat, gate_out])

            cgc_outs.append(gate_mul_expert)
        return cgc_outs

    # build Progressive Layered Extraction
    ple_inputs = [dnn_input] * (num_tasks + 1)  # [task1, task2, ... taskn, shared task]
    ple_outputs = []
    for i in range(num_levels):
        if i == num_levels - 1:  # the last level
            ple_outputs = cgc_net(inputs=ple_inputs, level_name='level_' + str(i) + '_', is_last=True)
        else:
            ple_outputs = cgc_net(inputs=ple_inputs, level_name='level_' + str(i) + '_', is_last=False)
            ple_inputs = ple_outputs

    task_outs = []
    for task_type, task_name, ple_out in zip(task_types, task_names, ple_outputs):
        # build tower layer
        tower_output = DNN(tower_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed,
                           name='tower_' + task_name)(ple_out)
        logit = Dense(1, use_bias=False)(tower_output)
        output = PredictionLayer(task_type, name=task_name)(logit)
        task_outs.append(output)

    model = Model(inputs=inputs_list, outputs=task_outs)
    return model
"
DeepCTR,__init__.py,"from .esmm import ESMM
from .mmoe import MMOE
from .ple import PLE
from .sharedbottom import SharedBottom"
DeepCTR,sharedbottom.py,"""""""
Author:
    Mincai Lai, laimc@shanghaitech.edu.cn

    Weichen Shen, weichenswc@163.com

Reference:
    [1] Ruder S. An overview of multi-task learning in deep neural networks[J]. arXiv preprint arXiv:1706.05098, 2017.(https://arxiv.org/pdf/1706.05098.pdf)
""""""

from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense

from ...feature_column import build_input_features, input_from_feature_columns
from ...layers.core import PredictionLayer, DNN
from ...layers.utils import combined_dnn_input


def SharedBottom(dnn_feature_columns, bottom_dnn_hidden_units=(256, 128), tower_dnn_hidden_units=(64,),
                 l2_reg_embedding=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0, dnn_activation='relu',
                 dnn_use_bn=False, task_types=('binary', 'binary'), task_names=('ctr', 'ctcvr')):
    """"""Instantiates the SharedBottom multi-task learning Network architecture.

    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param bottom_dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of shared bottom DNN.
    :param tower_dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of task-specific DNN.
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param task_types: list of str, indicating the loss of each tasks, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss. e.g. ['binary', 'regression']
    :param task_names: list of str, indicating the predict target of each tasks

    :return: A Keras model instance.
    """"""
    num_tasks = len(task_names)
    if num_tasks <= 1:
        raise ValueError(""num_tasks must be greater than 1"")
    if len(task_types) != num_tasks:
        raise ValueError(""num_tasks must be equal to the length of task_types"")

    for task_type in task_types:
        if task_type not in ['binary', 'regression']:
            raise ValueError(""task must be binary or regression, {} is illegal"".format(task_type))

    features = build_input_features(dnn_feature_columns)
    inputs_list = list(features.values())

    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                         l2_reg_embedding, seed)

    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)
    shared_bottom_output = DNN(bottom_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(
        dnn_input)

    tasks_output = []
    for task_type, task_name in zip(task_types, task_names):
        tower_output = DNN(tower_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed,
                           name='tower_' + task_name)(shared_bottom_output)

        logit = Dense(1, use_bias=False)(tower_output)
        output = PredictionLayer(task_type, name=task_name)(logit)
        tasks_output.append(output)

    model = Model(inputs=inputs_list, outputs=tasks_output)
    return model
"
DeepCTR,mmoe.py,"""""""
Author:
    Mincai Lai, laimc@shanghaitech.edu.cn

    Weichen Shen, weichenswc@163.com

Reference:
    [1] Ma J, Zhao Z, Yi X, et al. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2018.(https://dl.acm.org/doi/abs/10.1145/3219819.3220007)
""""""

import tensorflow as tf
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense, Lambda

from ...feature_column import build_input_features, input_from_feature_columns
from ...layers.core import PredictionLayer, DNN
from ...layers.utils import combined_dnn_input, reduce_sum


def MMOE(dnn_feature_columns, num_experts=3, expert_dnn_hidden_units=(256, 128), tower_dnn_hidden_units=(64,),
         gate_dnn_hidden_units=(), l2_reg_embedding=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0,
         dnn_activation='relu',
         dnn_use_bn=False, task_types=('binary', 'binary'), task_names=('ctr', 'ctcvr')):
    """"""Instantiates the Multi-gate Mixture-of-Experts multi-task learning architecture.

    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param num_experts: integer, number of experts.
    :param expert_dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of expert DNN.
    :param tower_dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of task-specific DNN.
    :param gate_dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of gate DNN.
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param task_types: list of str, indicating the loss of each tasks, ``""binary""`` for  binary logloss, ``""regression""`` for regression loss. e.g. ['binary', 'regression']
    :param task_names: list of str, indicating the predict target of each tasks

    :return: a Keras model instance
    """"""
    num_tasks = len(task_names)
    if num_tasks <= 1:
        raise ValueError(""num_tasks must be greater than 1"")
    if num_experts <= 1:
        raise ValueError(""num_experts must be greater than 1"")

    if len(task_types) != num_tasks:
        raise ValueError(""num_tasks must be equal to the length of task_types"")

    for task_type in task_types:
        if task_type not in ['binary', 'regression']:
            raise ValueError(""task must be binary or regression, {} is illegal"".format(task_type))

    features = build_input_features(dnn_feature_columns)

    inputs_list = list(features.values())

    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                         l2_reg_embedding, seed)
    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)

    # build expert layer
    expert_outs = []
    for i in range(num_experts):
        expert_network = DNN(expert_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed,
                             name='expert_' + str(i))(dnn_input)
        expert_outs.append(expert_network)

    expert_concat = Lambda(lambda x: tf.stack(x, axis=1))(expert_outs)  # None,num_experts,dim

    mmoe_outs = []
    for i in range(num_tasks):  # one mmoe layer: nums_tasks = num_gates
        # build gate layers
        gate_input = DNN(gate_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed,
                         name='gate_' + task_names[i])(dnn_input)
        gate_out = Dense(num_experts, use_bias=False, activation='softmax',
                         name='gate_softmax_' + task_names[i])(gate_input)
        gate_out = Lambda(lambda x: tf.expand_dims(x, axis=-1))(gate_out)

        # gate multiply the expert
        gate_mul_expert = Lambda(lambda x: reduce_sum(x[0] * x[1], axis=1, keep_dims=False),
                                 name='gate_mul_expert_' + task_names[i])([expert_concat, gate_out])
        mmoe_outs.append(gate_mul_expert)

    task_outs = []
    for task_type, task_name, mmoe_out in zip(task_types, task_names, mmoe_outs):
        # build tower layer
        tower_output = DNN(tower_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed,
                           name='tower_' + task_name)(mmoe_out)

        logit = Dense(1, use_bias=False)(tower_output)
        output = PredictionLayer(task_type, name=task_name)(logit)
        task_outs.append(output)

    model = Model(inputs=inputs_list, outputs=task_outs)
    return model
"
DeepCTR,esmm.py,"""""""
Author:
    Mincai Lai, laimc@shanghaitech.edu.cn

    Weichen Shen, weichenswc@163.com

Reference:
    [1] Ma X, Zhao L, Huang G, et al. Entire space multi-task model: An effective approach for estimating post-click conversion rate[C]//The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. 2018.(https://arxiv.org/abs/1804.07931)
""""""

from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Dense, Multiply

from ...feature_column import build_input_features, input_from_feature_columns
from ...layers.core import PredictionLayer, DNN
from ...layers.utils import combined_dnn_input


def ESMM(dnn_feature_columns, tower_dnn_hidden_units=(256, 128, 64), l2_reg_embedding=0.00001, l2_reg_dnn=0,
         seed=1024, dnn_dropout=0, dnn_activation='relu', dnn_use_bn=False, task_types=('binary', 'binary'),
         task_names=('ctr', 'ctcvr')):
    """"""Instantiates the Entire Space Multi-Task Model architecture.

    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param tower_dnn_hidden_units:  list,list of positive integer or empty list, the layer number and units in each layer of task DNN.
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector.
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN.
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param task_types:  str, indicating the loss of each tasks, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss.
    :param task_names: list of str, indicating the predict target of each tasks. default value is ['ctr', 'ctcvr']

    :return: A Keras model instance.
    """"""
    if len(task_names) != 2:
        raise ValueError(""the length of task_names must be equal to 2"")

    for task_type in task_types:
        if task_type != 'binary':
            raise ValueError(""task must be binary in ESMM, {} is illegal"".format(task_type))

    features = build_input_features(dnn_feature_columns)
    inputs_list = list(features.values())

    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,
                                                                         l2_reg_embedding, seed)

    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)

    ctr_output = DNN(tower_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(
        dnn_input)
    cvr_output = DNN(tower_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(
        dnn_input)

    ctr_logit = Dense(1, use_bias=False)(ctr_output)
    cvr_logit = Dense(1, use_bias=False)(cvr_output)

    ctr_pred = PredictionLayer('binary', name=task_names[0])(ctr_logit)
    cvr_pred = PredictionLayer('binary')(cvr_logit)

    ctcvr_pred = Multiply(name=task_names[1])([ctr_pred, cvr_pred])  # CTCVR = CTR * CVR

    model = Model(inputs=inputs_list, outputs=[ctr_pred, ctcvr_pred])
    return model
"
DeepCTR,dsin.py,"# coding: utf-8
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Feng Y, Lv F, Shen W, et al. Deep Session Interest Network for Click-Through Rate Prediction[J]. arXiv preprint arXiv:1905.06482, 2019.(https://arxiv.org/abs/1905.06482)

""""""

from collections import OrderedDict

from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import (Concatenate, Dense, Embedding,
                                            Flatten, Input)
from tensorflow.python.keras.regularizers import l2

from ...feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, build_input_features
from ...inputs import (get_embedding_vec_list, get_inputs_list, embedding_lookup, get_dense_input)
from ...layers.core import DNN, PredictionLayer
from ...layers.sequence import (AttentionSequencePoolingLayer, BiasEncoding,
                                BiLSTM, Transformer)
from ...layers.utils import concat_func, combined_dnn_input


def DSIN(dnn_feature_columns, sess_feature_list, sess_max_count=5, bias_encoding=False,
         att_embedding_size=1, att_head_num=8, dnn_hidden_units=(256, 128, 64), dnn_activation='relu', dnn_dropout=0,
         dnn_use_bn=False, l2_reg_dnn=0, l2_reg_embedding=1e-6, seed=1024, task='binary',
         ):
    """"""Instantiates the Deep Session Interest Network architecture.

    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param sess_feature_list: list,to indicate  sequence sparse field
    :param sess_max_count: positive int, to indicate the max number of sessions
    :param sess_len_max: positive int, to indicate the max length of each session
    :param bias_encoding: bool. Whether use bias encoding or postional encoding
    :param att_embedding_size: positive int, the embedding size of each attention head
    :param att_head_num: positive int, the number of attention head
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net
    :param dnn_activation: Activation function to use in deep net
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in deep net
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param seed: integer ,to use as random seed.
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.

    """"""

    hist_emb_size = sum(
        map(lambda fc: fc.embedding_dim, filter(lambda fc: fc.name in sess_feature_list, dnn_feature_columns)))

    if (att_embedding_size * att_head_num != hist_emb_size):
        raise ValueError(
            ""hist_emb_size must equal to att_embedding_size * att_head_num ,got %d != %d *%d"" % (
                hist_emb_size, att_embedding_size, att_head_num))

    features = build_input_features(dnn_feature_columns)

    sparse_feature_columns = list(
        filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns)) if dnn_feature_columns else []
    dense_feature_columns = list(
        filter(lambda x: isinstance(x, DenseFeat), dnn_feature_columns)) if dnn_feature_columns else []
    varlen_sparse_feature_columns = list(
        filter(lambda x: isinstance(x, VarLenSparseFeat), dnn_feature_columns)) if dnn_feature_columns else []

    sparse_varlen_feature_columns = []
    history_fc_names = list(map(lambda x: ""sess"" + x, sess_feature_list))
    for fc in varlen_sparse_feature_columns:
        feature_name = fc.name
        if feature_name in history_fc_names:
            continue
        else:
            sparse_varlen_feature_columns.append(fc)

    inputs_list = list(features.values())

    user_behavior_input_dict = {}
    for idx in range(sess_max_count):
        sess_input = OrderedDict()
        for i, feat in enumerate(sess_feature_list):
            sess_input[feat] = features[""sess_"" + str(idx) + ""_"" + feat]

        user_behavior_input_dict[""sess_"" + str(idx)] = sess_input

    user_sess_length = Input(shape=(1,), name='sess_length')

    embedding_dict = {feat.embedding_name: Embedding(feat.vocabulary_size, feat.embedding_dim,
                                                     embeddings_initializer=feat.embeddings_initializer,
                                                     embeddings_regularizer=l2(
                                                         l2_reg_embedding),
                                                     name='sparse_emb_' +
                                                          str(i) + '-' + feat.name,
                                                     mask_zero=(feat.name in sess_feature_list)) for i, feat in
                      enumerate(sparse_feature_columns)}

    query_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns, sess_feature_list,
                                      sess_feature_list, to_list=True)
    dnn_input_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns,
                                          mask_feat_list=sess_feature_list, to_list=True)
    dense_value_list = get_dense_input(features, dense_feature_columns)

    query_emb = concat_func(query_emb_list, mask=True)

    dnn_input_emb = Flatten()(concat_func(dnn_input_emb_list))

    tr_input = sess_interest_division(embedding_dict, user_behavior_input_dict, sparse_feature_columns,
                                      sess_feature_list, sess_max_count, bias_encoding=bias_encoding)

    Self_Attention = Transformer(att_embedding_size, att_head_num, dropout_rate=0, use_layer_norm=False,
                                 use_positional_encoding=(not bias_encoding), seed=seed, supports_masking=True,
                                 blinding=True)
    sess_fea = sess_interest_extractor(
        tr_input, sess_max_count, Self_Attention)

    interest_attention_layer = AttentionSequencePoolingLayer(att_hidden_units=(64, 16), weight_normalization=True,
                                                             supports_masking=False)(
        [query_emb, sess_fea, user_sess_length])

    lstm_outputs = BiLSTM(hist_emb_size,
                          layers=2, res_layers=0, dropout_rate=0.2, )(sess_fea)
    lstm_attention_layer = AttentionSequencePoolingLayer(att_hidden_units=(64, 16), weight_normalization=True)(
        [query_emb, lstm_outputs, user_sess_length])

    dnn_input_emb = Concatenate()(
        [dnn_input_emb, Flatten()(interest_attention_layer), Flatten()(lstm_attention_layer)])

    dnn_input_emb = combined_dnn_input([dnn_input_emb], dense_value_list)
    output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input_emb)
    output = Dense(1, use_bias=False)(output)
    output = PredictionLayer(task)(output)

    sess_input_list = []
    for i in range(sess_max_count):
        sess_name = ""sess_"" + str(i)
        sess_input_list.extend(get_inputs_list(
            [user_behavior_input_dict[sess_name]]))

    model = Model(inputs=inputs_list + [user_sess_length], outputs=output)

    return model


def sess_interest_division(sparse_embedding_dict, user_behavior_input_dict, sparse_fg_list, sess_feture_list,
                           sess_max_count,
                           bias_encoding=True):
    tr_input = []
    for i in range(sess_max_count):
        sess_name = ""sess_"" + str(i)
        keys_emb_list = get_embedding_vec_list(sparse_embedding_dict, user_behavior_input_dict[sess_name],
                                               sparse_fg_list, sess_feture_list, sess_feture_list)

        keys_emb = concat_func(keys_emb_list, mask=True)
        tr_input.append(keys_emb)
    if bias_encoding:
        tr_input = BiasEncoding(sess_max_count)(tr_input)
    return tr_input


def sess_interest_extractor(tr_input, sess_max_count, TR):
    tr_out = []
    for i in range(sess_max_count):
        tr_out.append(TR(
            [tr_input[i], tr_input[i]]))
    sess_fea = concat_func(tr_out, axis=1)
    return sess_fea
"
DeepCTR,din.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068. (https://arxiv.org/pdf/1706.06978.pdf)
""""""
from tensorflow.python.keras.layers import Dense, Flatten
from tensorflow.python.keras.models import Model

from ...feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, build_input_features
from ...inputs import create_embedding_matrix, embedding_lookup, get_dense_input, varlen_embedding_lookup, \
    get_varlen_pooling_list
from ...layers.core import DNN, PredictionLayer
from ...layers.sequence import AttentionSequencePoolingLayer
from ...layers.utils import concat_func, combined_dnn_input


def DIN(dnn_feature_columns, history_feature_list, dnn_use_bn=False,
        dnn_hidden_units=(256, 128, 64), dnn_activation='relu', att_hidden_size=(80, 40), att_activation=""dice"",
        att_weight_normalization=False, l2_reg_dnn=0, l2_reg_embedding=1e-6, dnn_dropout=0, seed=1024,
        task='binary'):
    """"""Instantiates the Deep Interest Network architecture.

    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param history_feature_list: list,to indicate  sequence sparse field
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in deep net
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net
    :param dnn_activation: Activation function to use in deep net
    :param att_hidden_size: list,list of positive integer , the layer number and units in each layer of attention net
    :param att_activation: Activation function to use in attention net
    :param att_weight_normalization: bool.Whether normalize the attention score of local activation unit.
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param seed: integer ,to use as random seed.
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.

    """"""

    features = build_input_features(dnn_feature_columns)

    sparse_feature_columns = list(
        filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns)) if dnn_feature_columns else []
    dense_feature_columns = list(
        filter(lambda x: isinstance(x, DenseFeat), dnn_feature_columns)) if dnn_feature_columns else []
    varlen_sparse_feature_columns = list(
        filter(lambda x: isinstance(x, VarLenSparseFeat), dnn_feature_columns)) if dnn_feature_columns else []

    history_feature_columns = []
    sparse_varlen_feature_columns = []
    history_fc_names = list(map(lambda x: ""hist_"" + x, history_feature_list))
    for fc in varlen_sparse_feature_columns:
        feature_name = fc.name
        if feature_name in history_fc_names:
            history_feature_columns.append(fc)
        else:
            sparse_varlen_feature_columns.append(fc)

    inputs_list = list(features.values())

    embedding_dict = create_embedding_matrix(dnn_feature_columns, l2_reg_embedding, seed, prefix="""")

    query_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns, history_feature_list,
                                      history_feature_list, to_list=True)
    keys_emb_list = embedding_lookup(embedding_dict, features, history_feature_columns, history_fc_names,
                                     history_fc_names, to_list=True)
    dnn_input_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns,
                                          mask_feat_list=history_feature_list, to_list=True)
    dense_value_list = get_dense_input(features, dense_feature_columns)

    sequence_embed_dict = varlen_embedding_lookup(embedding_dict, features, sparse_varlen_feature_columns)
    sequence_embed_list = get_varlen_pooling_list(sequence_embed_dict, features, sparse_varlen_feature_columns,
                                                  to_list=True)

    dnn_input_emb_list += sequence_embed_list

    keys_emb = concat_func(keys_emb_list, mask=True)
    deep_input_emb = concat_func(dnn_input_emb_list)
    query_emb = concat_func(query_emb_list, mask=True)
    hist = AttentionSequencePoolingLayer(att_hidden_size, att_activation,
                                         weight_normalization=att_weight_normalization, supports_masking=True)([
        query_emb, keys_emb])

    deep_input_emb = concat_func([deep_input_emb, hist])
    deep_input_emb = Flatten()(deep_input_emb)
    dnn_input = combined_dnn_input([deep_input_emb], dense_value_list)
    output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed)(dnn_input)
    final_logit = Dense(1, use_bias=False)(output)

    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)
    return model
"
DeepCTR,dien.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Weichen Shen, weichenswc@163.com

Reference:
    [1] Zhou G, Mou N, Fan Y, et al. Deep Interest Evolution Network for Click-Through Rate Prediction[J]. arXiv preprint arXiv:1809.03672, 2018. (https://arxiv.org/pdf/1809.03672.pdf)
""""""

import tensorflow as tf
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import (Concatenate, Dense, Permute, multiply, Flatten)

from ...feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, build_input_features
from ...inputs import get_varlen_pooling_list, create_embedding_matrix, embedding_lookup, varlen_embedding_lookup, \
    get_dense_input
from ...layers.core import DNN, PredictionLayer
from ...layers.sequence import AttentionSequencePoolingLayer, DynamicGRU
from ...layers.utils import concat_func, reduce_mean, combined_dnn_input


def auxiliary_loss(h_states, click_seq, noclick_seq, mask, stag=None):
    #:param h_states:
    #:param click_seq:
    #:param noclick_seq: #[B,T-1,E]
    #:param mask:#[B,1]
    #:param stag:
    #:return:
    hist_len, _ = click_seq.get_shape().as_list()[1:]
    mask = tf.sequence_mask(mask, hist_len)
    mask = mask[:, 0, :]

    mask = tf.cast(mask, tf.float32)

    click_input_ = tf.concat([h_states, click_seq], -1)

    noclick_input_ = tf.concat([h_states, noclick_seq], -1)

    auxiliary_nn = DNN([100, 50, 1], activation='sigmoid')

    click_prop_ = auxiliary_nn(click_input_, stag=stag)[:, :, 0]

    noclick_prop_ = auxiliary_nn(noclick_input_, stag=stag)[
                    :, :, 0]  # [B,T-1]

    try:
        click_loss_ = - tf.reshape(tf.log(click_prop_),
                                   [-1, tf.shape(click_seq)[1]]) * mask
    except AttributeError:
        click_loss_ = - tf.reshape(tf.compat.v1.log(click_prop_),
                                   [-1, tf.shape(click_seq)[1]]) * mask
    try:
        noclick_loss_ = - \
                            tf.reshape(tf.log(1.0 - noclick_prop_),
                                       [-1, tf.shape(noclick_seq)[1]]) * mask
    except AttributeError:
        noclick_loss_ = - \
                            tf.reshape(tf.compat.v1.log(1.0 - noclick_prop_),
                                       [-1, tf.shape(noclick_seq)[1]]) * mask

    loss_ = reduce_mean(click_loss_ + noclick_loss_)

    return loss_


def interest_evolution(concat_behavior, deep_input_item, user_behavior_length, gru_type=""GRU"", use_neg=False,
                       neg_concat_behavior=None, att_hidden_size=(64, 16), att_activation='sigmoid',
                       att_weight_normalization=False, ):
    if gru_type not in [""GRU"", ""AIGRU"", ""AGRU"", ""AUGRU""]:
        raise ValueError(""gru_type error "")
    aux_loss_1 = None
    embedding_size = None
    rnn_outputs = DynamicGRU(embedding_size, return_sequence=True,
                             name=""gru1"")([concat_behavior, user_behavior_length])

    if gru_type == ""AUGRU"" and use_neg:
        aux_loss_1 = auxiliary_loss(rnn_outputs[:, :-1, :], concat_behavior[:, 1:, :],

                                    neg_concat_behavior[:, 1:, :],

                                    tf.subtract(user_behavior_length, 1), stag=""gru"")  # [:, 1:]

    if gru_type == ""GRU"":
        rnn_outputs2 = DynamicGRU(embedding_size, return_sequence=True,
                                  name=""gru2"")([rnn_outputs, user_behavior_length])
        # attention_score = AttentionSequencePoolingLayer(hidden_size=att_hidden_size, activation=att_activation, weight_normalization=att_weight_normalization, return_score=True)([
        #     deep_input_item, rnn_outputs2, user_behavior_length])
        # outputs = Lambda(lambda x: tf.matmul(x[0], x[1]))(
        #     [attention_score, rnn_outputs2])
        # hist = outputs
        hist = AttentionSequencePoolingLayer(att_hidden_units=att_hidden_size, att_activation=att_activation,
                                             weight_normalization=att_weight_normalization, return_score=False)([
            deep_input_item, rnn_outputs2, user_behavior_length])

    else:  # AIGRU AGRU AUGRU

        scores = AttentionSequencePoolingLayer(att_hidden_units=att_hidden_size, att_activation=att_activation,
                                               weight_normalization=att_weight_normalization, return_score=True)([
            deep_input_item, rnn_outputs, user_behavior_length])

        if gru_type == ""AIGRU"":
            hist = multiply([rnn_outputs, Permute([2, 1])(scores)])
            final_state2 = DynamicGRU(embedding_size, gru_type=""GRU"", return_sequence=False, name='gru2')(
                [hist, user_behavior_length])
        else:  # AGRU AUGRU
            final_state2 = DynamicGRU(embedding_size, gru_type=gru_type, return_sequence=False,
                                      name='gru2')([rnn_outputs, user_behavior_length, Permute([2, 1])(scores)])
        hist = final_state2
    return hist, aux_loss_1


def DIEN(dnn_feature_columns, history_feature_list,
         gru_type=""GRU"", use_negsampling=False, alpha=1.0, use_bn=False, dnn_hidden_units=(256, 128, 64),
         dnn_activation='relu',
         att_hidden_units=(64, 16), att_activation=""dice"", att_weight_normalization=True,
         l2_reg_dnn=0, l2_reg_embedding=1e-6, dnn_dropout=0, seed=1024, task='binary'):
    """"""Instantiates the Deep Interest Evolution Network architecture.

    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param history_feature_list: list,to indicate  sequence sparse field
    :param gru_type: str,can be GRU AIGRU AUGRU AGRU
    :param use_negsampling: bool, whether or not use negtive sampling
    :param alpha: float ,weight of auxiliary_loss
    :param use_bn: bool. Whether use BatchNormalization before activation or not in deep net
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
    :param dnn_activation: Activation function to use in DNN
    :param att_hidden_units: list,list of positive integer , the layer number and units in each layer of attention net
    :param att_activation: Activation function to use in attention net
    :param att_weight_normalization: bool.Whether normalize the attention score of local activation unit.
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param init_std: float,to use as the initialize std of embedding vector
    :param seed: integer ,to use as random seed.
    :param task: str, ``""binary""`` for  binary logloss or  ``""regression""`` for regression loss
    :return: A Keras model instance.

    """"""
    features = build_input_features(dnn_feature_columns)

    user_behavior_length = features[""seq_length""]

    sparse_feature_columns = list(
        filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns)) if dnn_feature_columns else []
    dense_feature_columns = list(
        filter(lambda x: isinstance(x, DenseFeat), dnn_feature_columns)) if dnn_feature_columns else []
    varlen_sparse_feature_columns = list(
        filter(lambda x: isinstance(x, VarLenSparseFeat), dnn_feature_columns)) if dnn_feature_columns else []

    history_feature_columns = []
    neg_history_feature_columns = []
    sparse_varlen_feature_columns = []
    history_fc_names = list(map(lambda x: ""hist_"" + x, history_feature_list))
    neg_history_fc_names = list(map(lambda x: ""neg_"" + x, history_fc_names))
    for fc in varlen_sparse_feature_columns:
        feature_name = fc.name
        if feature_name in history_fc_names:
            history_feature_columns.append(fc)
        elif feature_name in neg_history_fc_names:
            neg_history_feature_columns.append(fc)
        else:
            sparse_varlen_feature_columns.append(fc)

    inputs_list = list(features.values())

    embedding_dict = create_embedding_matrix(dnn_feature_columns, l2_reg_embedding, seed, prefix="""",
                                             seq_mask_zero=False)

    query_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns,
                                      return_feat_list=history_feature_list, to_list=True)

    keys_emb_list = embedding_lookup(embedding_dict, features, history_feature_columns,
                                     return_feat_list=history_fc_names, to_list=True)
    dnn_input_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns,
                                          mask_feat_list=history_feature_list, to_list=True)
    dense_value_list = get_dense_input(features, dense_feature_columns)

    sequence_embed_dict = varlen_embedding_lookup(embedding_dict, features, sparse_varlen_feature_columns)
    sequence_embed_list = get_varlen_pooling_list(sequence_embed_dict, features, sparse_varlen_feature_columns,
                                                  to_list=True)
    dnn_input_emb_list += sequence_embed_list
    keys_emb = concat_func(keys_emb_list)
    deep_input_emb = concat_func(dnn_input_emb_list)
    query_emb = concat_func(query_emb_list)

    if use_negsampling:

        neg_uiseq_embed_list = embedding_lookup(embedding_dict, features, neg_history_feature_columns,
                                                neg_history_fc_names, to_list=True)

        neg_concat_behavior = concat_func(neg_uiseq_embed_list)

    else:
        neg_concat_behavior = None
    hist, aux_loss_1 = interest_evolution(keys_emb, query_emb, user_behavior_length, gru_type=gru_type,
                                          use_neg=use_negsampling, neg_concat_behavior=neg_concat_behavior,
                                          att_hidden_size=att_hidden_units,
                                          att_activation=att_activation,
                                          att_weight_normalization=att_weight_normalization, )

    deep_input_emb = Concatenate()([deep_input_emb, hist])

    deep_input_emb = Flatten()(deep_input_emb)

    dnn_input = combined_dnn_input([deep_input_emb], dense_value_list)
    output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, use_bn, seed=seed)(dnn_input)
    final_logit = Dense(1, use_bias=False, kernel_initializer=tf.keras.initializers.glorot_normal(seed))(output)
    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)

    if use_negsampling:
        model.add_loss(alpha * aux_loss_1)
    try:
        tf.keras.backend.get_session().run(tf.global_variables_initializer())
    except AttributeError:
        tf.compat.v1.keras.backend.get_session().run(tf.compat.v1.global_variables_initializer())
        tf.compat.v1.experimental.output_all_intermediates(True)
    return model
"
DeepCTR,bst.py,"# -*- coding:utf-8 -*-
""""""
Author:
    Zichao Li, 2843656167@qq.com

Reference:
    Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior sequence transformer for e-commerce recommendation in Alibaba. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data (DLP-KDD '19). Association for Computing Machinery, New York, NY, USA, Article 12, 1–4. DOI:https://doi.org/10.1145/3326937.3341261
""""""

from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import (Dense, Flatten)

from ...feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, build_input_features
from ...inputs import get_varlen_pooling_list, create_embedding_matrix, embedding_lookup, varlen_embedding_lookup, \
    get_dense_input
from ...layers.core import DNN, PredictionLayer
from ...layers.sequence import Transformer, AttentionSequencePoolingLayer
from ...layers.utils import concat_func, combined_dnn_input


def BST(dnn_feature_columns, history_feature_list, transformer_num=1, att_head_num=8,
        use_bn=False, dnn_hidden_units=(256, 128, 64), dnn_activation='relu', l2_reg_dnn=0,
        l2_reg_embedding=1e-6, dnn_dropout=0.0, seed=1024, task='binary'):
    """"""Instantiates the BST architecture.

     :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
     :param history_feature_list: list, to indicate sequence sparse field.
     :param transformer_num: int, the number of transformer layer.
     :param att_head_num: int, the number of heads in multi-head self attention.
     :param use_bn: bool. Whether use BatchNormalization before activation or not in deep net
     :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN
     :param dnn_activation: Activation function to use in DNN
     :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
     :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
     :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
     :param seed: integer ,to use as random seed.
     :param task: str, ``""binary""`` for  binary logloss or ``""regression""`` for regression loss
     :return: A Keras model instance.

     """"""

    features = build_input_features(dnn_feature_columns)
    inputs_list = list(features.values())

    user_behavior_length = features[""seq_length""]

    sparse_feature_columns = list(
        filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns)) if dnn_feature_columns else []
    dense_feature_columns = list(
        filter(lambda x: isinstance(x, DenseFeat), dnn_feature_columns)) if dnn_feature_columns else []
    varlen_sparse_feature_columns = list(
        filter(lambda x: isinstance(x, VarLenSparseFeat), dnn_feature_columns)) if dnn_feature_columns else []

    history_feature_columns = []
    sparse_varlen_feature_columns = []
    history_fc_names = list(map(lambda x: ""hist_"" + x, history_feature_list))

    for fc in varlen_sparse_feature_columns:
        feature_name = fc.name
        if feature_name in history_fc_names:
            history_feature_columns.append(fc)
        else:
            sparse_varlen_feature_columns.append(fc)

    embedding_dict = create_embedding_matrix(dnn_feature_columns, l2_reg_embedding, seed, prefix="""",
                                             seq_mask_zero=True)

    query_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns,
                                      return_feat_list=history_feature_list, to_list=True)
    hist_emb_list = embedding_lookup(embedding_dict, features, history_feature_columns,
                                     return_feat_list=history_fc_names, to_list=True)
    dnn_input_emb_list = embedding_lookup(embedding_dict, features, sparse_feature_columns,
                                          mask_feat_list=history_feature_list, to_list=True)
    dense_value_list = get_dense_input(features, dense_feature_columns)
    sequence_embed_dict = varlen_embedding_lookup(embedding_dict, features, sparse_varlen_feature_columns)
    sequence_embed_list = get_varlen_pooling_list(sequence_embed_dict, features, sparse_varlen_feature_columns,
                                                  to_list=True)

    dnn_input_emb_list += sequence_embed_list
    query_emb = concat_func(query_emb_list)
    deep_input_emb = concat_func(dnn_input_emb_list)
    hist_emb = concat_func(hist_emb_list)

    transformer_output = hist_emb
    for _ in range(transformer_num):
        att_embedding_size = transformer_output.get_shape().as_list()[-1] // att_head_num
        transformer_layer = Transformer(att_embedding_size=att_embedding_size, head_num=att_head_num,
                                        dropout_rate=dnn_dropout, use_positional_encoding=True, use_res=True,
                                        use_feed_forward=True, use_layer_norm=True, blinding=False, seed=seed,
                                        supports_masking=False, output_type=None)
        transformer_output = transformer_layer([transformer_output, transformer_output,
                                                user_behavior_length, user_behavior_length])

    attn_output = AttentionSequencePoolingLayer(att_hidden_units=(64, 16), weight_normalization=True,
                                                supports_masking=False)([query_emb, transformer_output,
                                                                         user_behavior_length])
    deep_input_emb = concat_func([deep_input_emb, attn_output], axis=-1)
    deep_input_emb = Flatten()(deep_input_emb)

    dnn_input = combined_dnn_input([deep_input_emb], dense_value_list)
    output = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, use_bn, seed=seed)(dnn_input)
    final_logit = Dense(1, use_bias=False)(output)
    output = PredictionLayer(task)(final_logit)

    model = Model(inputs=inputs_list, outputs=output)

    return model
"
DeepCTR,__init__.py,"from .bst import BST
from .dien import DIEN
from .din import DIN
from .dsin import DSIN
"
DeepCTR,__init__.py,
DeepCTR,utils.py,"from tensorflow.python.ops import array_ops
from tensorflow.python.ops import init_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import nn_ops
from tensorflow.python.ops import variable_scope as vs
from tensorflow.python.ops.rnn_cell import *
from tensorflow.python.util import nest

_BIAS_VARIABLE_NAME = ""bias""

_WEIGHTS_VARIABLE_NAME = ""kernel""


class _Linear_(object):
    """"""Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.



    Args:

      args: a 2D Tensor or a list of 2D, batch x n, Tensors.

      output_size: int, second dimension of weight variable.

      dtype: data type for variables.

      build_bias: boolean, whether to build a bias variable.

      bias_initializer: starting value to initialize the bias

        (default is all zeros).

      kernel_initializer: starting value to initialize the weight.



    Raises:

      ValueError: if inputs_shape is wrong.

    """"""

    def __init__(self,

                 args,

                 output_size,

                 build_bias,

                 bias_initializer=None,

                 kernel_initializer=None):

        self._build_bias = build_bias

        if args is None or (nest.is_sequence(args) and not args):
            raise ValueError(""`args` must be specified"")

        if not nest.is_sequence(args):

            args = [args]

            self._is_sequence = False

        else:

            self._is_sequence = True

        # Calculate the total size of arguments on dimension 1.

        total_arg_size = 0

        shapes = [a.get_shape() for a in args]

        for shape in shapes:

            if shape.ndims != 2:
                raise ValueError(
                    ""linear is expecting 2D arguments: %s"" % shapes)

            if shape[1] is None:

                raise ValueError(""linear expects shape[1] to be provided for shape %s, ""

                                 ""but saw %s"" % (shape, shape[1]))

            else:

                total_arg_size += int(shape[1])#.value

        dtype = [a.dtype for a in args][0]

        scope = vs.get_variable_scope()

        with vs.variable_scope(scope) as outer_scope:

            self._weights = vs.get_variable(

                _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],

                dtype=dtype,

                initializer=kernel_initializer)

            if build_bias:

                with vs.variable_scope(outer_scope) as inner_scope:

                    inner_scope.set_partitioner(None)

                    if bias_initializer is None:
                        bias_initializer = init_ops.constant_initializer(
                            0.0, dtype=dtype)

                    self._biases = vs.get_variable(

                        _BIAS_VARIABLE_NAME, [output_size],

                        dtype=dtype,

                        initializer=bias_initializer)

    def __call__(self, args):

        if not self._is_sequence:
            args = [args]

        if len(args) == 1:

            res = math_ops.matmul(args[0], self._weights)

        else:

            res = math_ops.matmul(array_ops.concat(args, 1), self._weights)

        if self._build_bias:
            res = nn_ops.bias_add(res, self._biases)

        return res


try:
    from tensorflow.python.ops.rnn_cell_impl import _Linear
except:
    _Linear = _Linear_


class QAAttGRUCell(RNNCell):
    """"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).

    Args:

      num_units: int, The number of units in the GRU cell.

      activation: Nonlinearity to use.  Default: `tanh`.

      reuse: (optional) Python boolean describing whether to reuse variables

       in an existing scope.  If not `True`, and the existing scope already has

       the given variables, an error is raised.

      kernel_initializer: (optional) The initializer to use for the weight and

      projection matrices.

      bias_initializer: (optional) The initializer to use for the bias.

    """"""

    def __init__(self,

                 num_units,

                 activation=None,

                 reuse=None,

                 kernel_initializer=None,

                 bias_initializer=None):

        super(QAAttGRUCell, self).__init__(_reuse=reuse)

        self._num_units = num_units

        self._activation = activation or math_ops.tanh

        self._kernel_initializer = kernel_initializer

        self._bias_initializer = bias_initializer

        self._gate_linear = None

        self._candidate_linear = None

    @property
    def state_size(self):

        return self._num_units

    @property
    def output_size(self):

        return self._num_units

    def __call__(self, inputs, state, att_score):

        return self.call(inputs, state, att_score)

    def call(self, inputs, state, att_score=None):
        """"""Gated recurrent unit (GRU) with nunits cells.""""""

        if self._gate_linear is None:

            bias_ones = self._bias_initializer

            if self._bias_initializer is None:
                bias_ones = init_ops.constant_initializer(
                    1.0, dtype=inputs.dtype)

            with vs.variable_scope(""gates""):  # Reset gate and update gate.

                self._gate_linear = _Linear(

                    [inputs, state],

                    2 * self._num_units,

                    True,

                    bias_initializer=bias_ones,

                    kernel_initializer=self._kernel_initializer)

        value = math_ops.sigmoid(self._gate_linear([inputs, state]))

        r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)

        r_state = r * state

        if self._candidate_linear is None:
            with vs.variable_scope(""candidate""):
                self._candidate_linear = _Linear(

                    [inputs, r_state],

                    self._num_units,

                    True,

                    bias_initializer=self._bias_initializer,

                    kernel_initializer=self._kernel_initializer)

        c = self._activation(self._candidate_linear([inputs, r_state]))

        new_h = (1. - att_score) * state + att_score * c

        return new_h, new_h


class VecAttGRUCell(RNNCell):
    """"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).

    Args:

      num_units: int, The number of units in the GRU cell.

      activation: Nonlinearity to use.  Default: `tanh`.

      reuse: (optional) Python boolean describing whether to reuse variables

       in an existing scope.  If not `True`, and the existing scope already has

       the given variables, an error is raised.

      kernel_initializer: (optional) The initializer to use for the weight and

      projection matrices.

      bias_initializer: (optional) The initializer to use for the bias.

    """"""

    def __init__(self,

                 num_units,

                 activation=None,

                 reuse=None,

                 kernel_initializer=None,

                 bias_initializer=None):

        super(VecAttGRUCell, self).__init__(_reuse=reuse)

        self._num_units = num_units

        self._activation = activation or math_ops.tanh

        self._kernel_initializer = kernel_initializer

        self._bias_initializer = bias_initializer

        self._gate_linear = None

        self._candidate_linear = None

    @property
    def state_size(self):

        return self._num_units

    @property
    def output_size(self):

        return self._num_units

    def __call__(self, inputs, state, att_score):

        return self.call(inputs, state, att_score)

    def call(self, inputs, state, att_score=None):
        """"""Gated recurrent unit (GRU) with nunits cells.""""""

        if self._gate_linear is None:

            bias_ones = self._bias_initializer

            if self._bias_initializer is None:
                bias_ones = init_ops.constant_initializer(
                    1.0, dtype=inputs.dtype)

            with vs.variable_scope(""gates""):  # Reset gate and update gate.

                self._gate_linear = _Linear(

                    [inputs, state],

                    2 * self._num_units,

                    True,

                    bias_initializer=bias_ones,

                    kernel_initializer=self._kernel_initializer)

        value = math_ops.sigmoid(self._gate_linear([inputs, state]))

        r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)

        r_state = r * state

        if self._candidate_linear is None:
            with vs.variable_scope(""candidate""):
                self._candidate_linear = _Linear(

                    [inputs, r_state],

                    self._num_units,

                    True,

                    bias_initializer=self._bias_initializer,

                    kernel_initializer=self._kernel_initializer)

        c = self._activation(self._candidate_linear([inputs, r_state]))

        u = (1.0 - att_score) * u

        new_h = u * state + (1 - u) * c

        return new_h, new_h
"
DeepCTR,rnn_v2.py,"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.

#

# Licensed under the Apache License, Version 2.0 (the ""License"");

# you may not use this file except in compliance with the License.

# You may obtain a copy of the License at

#

#     http://www.apache.org/licenses/LICENSE-2.0

#

# Unless required by applicable law or agreed to in writing, software

# distributed under the License is distributed on an ""AS IS"" BASIS,

# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

# See the License for the specific language governing permissions and

# limitations under the License.

# ==============================================================================


""""""RNN helpers for TensorFlow models.





@@bidirectional_dynamic_rnn

@@dynamic_rnn

@@raw_rnn

@@static_rnn

@@static_state_saving_rnn

@@static_bidirectional_rnn

""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.framework import tensor_shape
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import rnn_cell_impl
from tensorflow.python.ops import tensor_array_ops
from tensorflow.python.ops import variable_scope as vs
from tensorflow.python.util import nest
import tensorflow as tf


def _like_rnncell_(cell):
    """"""Checks that a given object is an RNNCell by using duck typing.""""""

    conditions = [hasattr(cell, ""output_size""), hasattr(cell, ""state_size""),

                  hasattr(cell, ""zero_state""), callable(cell)]

    return all(conditions)


# pylint: disable=protected-access

_concat = rnn_cell_impl._concat
try:
    _like_rnncell = rnn_cell_impl._like_rnncell
except:
    _like_rnncell = _like_rnncell_


# pylint: enable=protected-access


def _transpose_batch_time(x):
    """"""Transpose the batch and time dimensions of a Tensor.



    Retains as much of the static shape information as possible.



    Args:

      x: A tensor of rank 2 or higher.



    Returns:

      x transposed along the first two dimensions.



    Raises:

      ValueError: if `x` is rank 1 or lower.

    """"""

    x_static_shape = x.get_shape()

    if x_static_shape.ndims is not None and x_static_shape.ndims < 2:
        raise ValueError(

            ""Expected input tensor %s to have rank at least 2, but saw shape: %s"" %

            (x, x_static_shape))

    x_rank = array_ops.rank(x)

    x_t = array_ops.transpose(

        x, array_ops.concat(

            ([1, 0], math_ops.range(2, x_rank)), axis=0))

    x_t.set_shape(

        tensor_shape.TensorShape([

            x_static_shape[1], x_static_shape[0]

        ]).concatenate(x_static_shape[2:]))

    return x_t


def _best_effort_input_batch_size(flat_input):
    """"""Get static input batch size if available, with fallback to the dynamic one.



    Args:

      flat_input: An iterable of time major input Tensors of shape [max_time,

        batch_size, ...]. All inputs should have compatible batch sizes.



    Returns:

      The batch size in Python integer if available, or a scalar Tensor otherwise.



    Raises:

      ValueError: if there is any input with an invalid shape.

    """"""

    for input_ in flat_input:

        shape = input_.shape

        if shape.ndims is None:
            continue

        if shape.ndims < 2:
            raise ValueError(

                ""Expected input tensor %s to have rank at least 2"" % input_)

        batch_size = shape[1]

        if batch_size is not None:
            return batch_size

    # Fallback to the dynamic batch size of the first input.

    return array_ops.shape(flat_input[0])[1]


def _infer_state_dtype(explicit_dtype, state):
    """"""Infer the dtype of an RNN state.



    Args:

      explicit_dtype: explicitly declared dtype or None.

      state: RNN's hidden state. Must be a Tensor or a nested iterable containing

        Tensors.



    Returns:

      dtype: inferred dtype of hidden state.



    Raises:

      ValueError: if `state` has heterogeneous dtypes or is empty.

    """"""

    if explicit_dtype is not None:

        return explicit_dtype

    elif nest.is_sequence(state):

        inferred_dtypes = [element.dtype for element in nest.flatten(state)]

        if not inferred_dtypes:
            raise ValueError(""Unable to infer dtype from empty state."")

        all_same = all([x == inferred_dtypes[0] for x in inferred_dtypes])

        if not all_same:
            raise ValueError(

                ""State has tensors of different inferred_dtypes. Unable to infer a ""

                ""single representative dtype."")

        return inferred_dtypes[0]

    else:

        return state.dtype


# pylint: disable=unused-argument

def _rnn_step(

        time, sequence_length, min_sequence_length, max_sequence_length,

        zero_output, state, call_cell, state_size, skip_conditionals=False):
    """"""Calculate one step of a dynamic RNN minibatch.



    Returns an (output, state) pair conditioned on the sequence_lengths.

    When skip_conditionals=False, the pseudocode is something like:



    if t >= max_sequence_length:

      return (zero_output, state)

    if t < min_sequence_length:

      return call_cell()



    # Selectively output zeros or output, old state or new state depending

    # on if we've finished calculating each row.

    new_output, new_state = call_cell()

    final_output = np.vstack([

      zero_output if time >= sequence_lengths[r] else new_output_r

      for r, new_output_r in enumerate(new_output)

    ])

    final_state = np.vstack([

      state[r] if time >= sequence_lengths[r] else new_state_r

      for r, new_state_r in enumerate(new_state)

    ])

    return (final_output, final_state)



    Args:

      time: Python int, the current time step

      sequence_length: int32 `Tensor` vector of size [batch_size]

      min_sequence_length: int32 `Tensor` scalar, min of sequence_length

      max_sequence_length: int32 `Tensor` scalar, max of sequence_length

      zero_output: `Tensor` vector of shape [output_size]

      state: Either a single `Tensor` matrix of shape `[batch_size, state_size]`,

        or a list/tuple of such tensors.

      call_cell: lambda returning tuple of (new_output, new_state) where

        new_output is a `Tensor` matrix of shape `[batch_size, output_size]`.

        new_state is a `Tensor` matrix of shape `[batch_size, state_size]`.

      state_size: The `cell.state_size` associated with the state.

      skip_conditionals: Python bool, whether to skip using the conditional

        calculations.  This is useful for `dynamic_rnn`, where the input tensor

        matches `max_sequence_length`, and using conditionals just slows

        everything down.



    Returns:

      A tuple of (`final_output`, `final_state`) as given by the pseudocode above:

        final_output is a `Tensor` matrix of shape [batch_size, output_size]

        final_state is either a single `Tensor` matrix, or a tuple of such

          matrices (matching length and shapes of input `state`).



    Raises:

      ValueError: If the cell returns a state tuple whose length does not match

        that returned by `state_size`.

    """"""

    # Convert state to a list for ease of use

    flat_state = nest.flatten(state)

    flat_zero_output = nest.flatten(zero_output)

    def _copy_one_through(output, new_output):

        # If the state contains a scalar value we simply pass it through.

        if output.shape.ndims == 0:
            return new_output

        copy_cond = (time >= sequence_length)

        with ops.colocate_with(new_output):
            return array_ops.where(copy_cond, output, new_output)

    def _copy_some_through(flat_new_output, flat_new_state):

        # Use broadcasting select to determine which values should get

        # the previous state & zero output, and which values should get

        # a calculated state & output.

        flat_new_output = [

            _copy_one_through(zero_output, new_output)

            for zero_output, new_output in zip(flat_zero_output, flat_new_output)]

        flat_new_state = [

            _copy_one_through(state, new_state)

            for state, new_state in zip(flat_state, flat_new_state)]

        return flat_new_output + flat_new_state

    def _maybe_copy_some_through():

        """"""Run RNN step.  Pass through either no or some past state.""""""

        new_output, new_state = call_cell()

        nest.assert_same_structure(state, new_state)

        flat_new_state = nest.flatten(new_state)

        flat_new_output = nest.flatten(new_output)

        return control_flow_ops.cond(

            # if t < min_seq_len: calculate and return everything

            time < min_sequence_length, lambda: flat_new_output + flat_new_state,

            # else copy some of it through

            lambda: _copy_some_through(flat_new_output, flat_new_state))

    # TODO(ebrevdo): skipping these conditionals may cause a slowdown,

    # but benefits from removing cond() and its gradient.  We should

    # profile with and without this switch here.

    if skip_conditionals:

        # Instead of using conditionals, perform the selective copy at all time

        # steps.  This is faster when max_seq_len is equal to the number of unrolls

        # (which is typical for dynamic_rnn).

        new_output, new_state = call_cell()

        nest.assert_same_structure(state, new_state)

        new_state = nest.flatten(new_state)

        new_output = nest.flatten(new_output)

        final_output_and_state = _copy_some_through(new_output, new_state)

    else:

        empty_update = lambda: flat_zero_output + flat_state

        final_output_and_state = control_flow_ops.cond(

            # if t >= max_seq_len: copy all state through, output zeros

            time >= max_sequence_length, empty_update,

            # otherwise calculation is required: copy some or all of it through

            _maybe_copy_some_through)

    if len(final_output_and_state) != len(flat_zero_output) + len(flat_state):
        raise ValueError(""Internal error: state and output were not concatenated ""

                         ""correctly."")

    final_output = final_output_and_state[:len(flat_zero_output)]

    final_state = final_output_and_state[len(flat_zero_output):]

    for output, flat_output in zip(final_output, flat_zero_output):
        output.set_shape(flat_output.get_shape())

    for substate, flat_substate in zip(final_state, flat_state):
        substate.set_shape(flat_substate.get_shape())

    final_output = nest.pack_sequence_as(

        structure=zero_output, flat_sequence=final_output)

    final_state = nest.pack_sequence_as(

        structure=state, flat_sequence=final_state)

    return final_output, final_state


def _reverse_seq(input_seq, lengths):
    """"""Reverse a list of Tensors up to specified lengths.



    Args:

      input_seq: Sequence of seq_len tensors of dimension (batch_size, n_features)

                 or nested tuples of tensors.

      lengths:   A `Tensor` of dimension batch_size, containing lengths for each

                 sequence in the batch. If ""None"" is specified, simply reverses

                 the list.



    Returns:

      time-reversed sequence

    """"""

    if lengths is None:
        return list(reversed(input_seq))

    flat_input_seq = tuple(nest.flatten(input_) for input_ in input_seq)

    flat_results = [[] for _ in range(len(input_seq))]

    for sequence in zip(*flat_input_seq):

        input_shape = tensor_shape.unknown_shape(

            ndims=sequence[0].get_shape().ndims)

        for input_ in sequence:
            input_shape.merge_with(input_.get_shape())

            input_.set_shape(input_shape)

        # Join into (time, batch_size, depth)

        s_joined = array_ops.stack(sequence)

        # Reverse along dimension 0

        s_reversed = array_ops.reverse_sequence(s_joined, lengths, 0, 1)

        # Split again into list

        result = array_ops.unstack(s_reversed)

        for r, flat_result in zip(result, flat_results):
            r.set_shape(input_shape)

            flat_result.append(r)

    results = [nest.pack_sequence_as(structure=input_, flat_sequence=flat_result)

               for input_, flat_result in zip(input_seq, flat_results)]

    return results


#
# def bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, sequence_length=None,
#
#                               initial_state_fw=None, initial_state_bw=None,
#
#                               dtype=None, parallel_iterations=None,
#
#                               swap_memory=False, time_major=False, scope=None):
#
#   """"""Creates a dynamic version of bidirectional recurrent neural network.
#
#
#
#   Takes input and builds independent forward and backward RNNs. The input_size
#
#   of forward and backward cell must match. The initial state for both directions
#
#   is zero by default (but can be set optionally) and no intermediate states are
#
#   ever returned -- the network is fully unrolled for the given (passed in)
#
#   length(s) of the sequence(s) or completely unrolled if length(s) is not
#
#   given.
#
#
#
#   Args:
#
#     cell_fw: An instance of RNNCell, to be used for forward direction.
#
#     cell_bw: An instance of RNNCell, to be used for backward direction.
#
#     inputs: The RNN inputs.
#
#       If time_major == False (default), this must be a tensor of shape:
#
#         `[batch_size, max_time, ...]`, or a nested tuple of such elements.
#
#       If time_major == True, this must be a tensor of shape:
#
#         `[max_time, batch_size, ...]`, or a nested tuple of such elements.
#
#     sequence_length: (optional) An int32/int64 vector, size `[batch_size]`,
#
#       containing the actual lengths for each of the sequences in the batch.
#
#       If not provided, all batch entries are assumed to be full sequences; and
#
#       time reversal is applied from time `0` to `max_time` for each sequence.
#
#     initial_state_fw: (optional) An initial state for the forward RNN.
#
#       This must be a tensor of appropriate type and shape
#
#       `[batch_size, cell_fw.state_size]`.
#
#       If `cell_fw.state_size` is a tuple, this should be a tuple of
#
#       tensors having shapes `[batch_size, s] for s in cell_fw.state_size`.
#
#     initial_state_bw: (optional) Same as for `initial_state_fw`, but using
#
#       the corresponding properties of `cell_bw`.
#
#     dtype: (optional) The data type for the initial states and expected output.
#
#       Required if initial_states are not provided or RNN states have a
#
#       heterogeneous dtype.
#
#     parallel_iterations: (Default: 32).  The number of iterations to run in
#
#       parallel.  Those operations which do not have any temporal dependency
#
#       and can be run in parallel, will be.  This parameter trades off
#
#       time for space.  Values >> 1 use more memory but take less time,
#
#       while smaller values use less memory but computations take longer.
#
#     swap_memory: Transparently swap the tensors produced in forward inference
#
#       but needed for back prop from GPU to CPU.  This allows training RNNs
#
#       which would typically not fit on a single GPU, with very minimal (or no)
#
#       performance penalty.
#
#     time_major: The shape format of the `inputs` and `outputs` Tensors.
#
#       If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.
#
#       If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.
#
#       Using `time_major = True` is a bit more efficient because it avoids
#
#       transposes at the beginning and end of the RNN calculation.  However,
#
#       most TensorFlow data is batch-major, so by default this function
#
#       accepts input and emits output in batch-major form.
#
#     scope: VariableScope for the created subgraph; defaults to
#
#       ""bidirectional_rnn""
#
#
#
#   Returns:
#
#     A tuple (outputs, output_states) where:
#
#       outputs: A tuple (output_fw, output_bw) containing the forward and
#
#         the backward rnn output `Tensor`.
#
#         If time_major == False (default),
#
#           output_fw will be a `Tensor` shaped:
#
#           `[batch_size, max_time, cell_fw.output_size]`
#
#           and output_bw will be a `Tensor` shaped:
#
#           `[batch_size, max_time, cell_bw.output_size]`.
#
#         If time_major == True,
#
#           output_fw will be a `Tensor` shaped:
#
#           `[max_time, batch_size, cell_fw.output_size]`
#
#           and output_bw will be a `Tensor` shaped:
#
#           `[max_time, batch_size, cell_bw.output_size]`.
#
#         It returns a tuple instead of a single concatenated `Tensor`, unlike
#
#         in the `bidirectional_rnn`. If the concatenated one is preferred,
#
#         the forward and backward outputs can be concatenated as
#
#         `tf.concat(outputs, 2)`.
#
#       output_states: A tuple (output_state_fw, output_state_bw) containing
#
#         the forward and the backward final states of bidirectional rnn.
#
#
#
#   Raises:
#
#     TypeError: If `cell_fw` or `cell_bw` is not an instance of `RNNCell`.
#
#   """"""
#
#
#
#   if not _like_rnncell(cell_fw):
#
#     raise TypeError(""cell_fw must be an instance of RNNCell"")
#
#   if not _like_rnncell(cell_bw):
#
#     raise TypeError(""cell_bw must be an instance of RNNCell"")
#
#
#
#   with vs.variable_scope(scope or ""bidirectional_rnn""):
#
#     # Forward direction
#
#     with vs.variable_scope(""fw"") as fw_scope:
#
#       output_fw, output_state_fw = dynamic_rnn(
#
#           cell=cell_fw, inputs=inputs, sequence_length=sequence_length,
#
#           initial_state=initial_state_fw, dtype=dtype,
#
#           parallel_iterations=parallel_iterations, swap_memory=swap_memory,
#
#           time_major=time_major, scope=fw_scope)
#
#
#
#     # Backward direction
#
#     if not time_major:
#
#       time_dim = 1
#
#       batch_dim = 0
#
#     else:
#
#       time_dim = 0
#
#       batch_dim = 1
#
#
#
#     def _reverse(input_, seq_lengths, seq_dim, batch_dim):
#
#       if seq_lengths is not None:
#
#         return array_ops.reverse_sequence(
#
#             input=input_, seq_lengths=seq_lengths,
#
#             seq_dim=seq_dim, batch_dim=batch_dim)
#
#       else:
#
#         return array_ops.reverse(input_, axis=[seq_dim])
#
#
#
#     with vs.variable_scope(""bw"") as bw_scope:
#
#       inputs_reverse = _reverse(
#
#           inputs, seq_lengths=sequence_length,
#
#           seq_dim=time_dim, batch_dim=batch_dim)
#
#       tmp, output_state_bw = dynamic_rnn(
#
#           cell=cell_bw, inputs=inputs_reverse, sequence_length=sequence_length,
#
#           initial_state=initial_state_bw, dtype=dtype,
#
#           parallel_iterations=parallel_iterations, swap_memory=swap_memory,
#
#           time_major=time_major, scope=bw_scope)
#
#
#
#   output_bw = _reverse(
#
#       tmp, seq_lengths=sequence_length,
#
#       seq_dim=time_dim, batch_dim=batch_dim)
#
#
#
#   outputs = (output_fw, output_bw)
#
#   output_states = (output_state_fw, output_state_bw)
#
#
#
#   return (outputs, output_states)
#


def dynamic_rnn(cell, inputs, att_scores=None, sequence_length=None, initial_state=None,

                dtype=None, parallel_iterations=None, swap_memory=False,

                time_major=False, scope=None):
    """"""Creates a recurrent neural network specified by RNNCell `cell`.



    Performs fully dynamic unrolling of `inputs`.



    Example:



    ```python

    # create a BasicRNNCell

    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)



    # 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]



    # defining initial state

    initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)



    # 'state' is a tensor of shape [batch_size, cell_state_size]

    outputs, state = tf.nn.dynamic_rnn(rnn_cell, input_data,

                                       initial_state=initial_state,

                                       dtype=tf.float32)

    ```



    ```python

    # create 2 LSTMCells

    rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [128, 256]]



    # create a RNN cell composed sequentially of a number of RNNCells

    multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)



    # 'outputs' is a tensor of shape [batch_size, max_time, 256]

    # 'state' is a N-tuple where N is the number of LSTMCells containing a

    # tf.contrib.rnn.LSTMStateTuple for each cell

    outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,

                                       inputs=data,

                                       dtype=tf.float32)

    ```





    Args:

      cell: An instance of RNNCell.

      inputs: The RNN inputs.

        If `time_major == False` (default), this must be a `Tensor` of shape:

          `[batch_size, max_time, ...]`, or a nested tuple of such

          elements.

        If `time_major == True`, this must be a `Tensor` of shape:

          `[max_time, batch_size, ...]`, or a nested tuple of such

          elements.

        This may also be a (possibly nested) tuple of Tensors satisfying

        this property.  The first two dimensions must match across all the inputs,

        but otherwise the ranks and other shape components may differ.

        In this case, input to `cell` at each time-step will replicate the

        structure of these tuples, except for the time dimension (from which the

        time is taken).

        The input to `cell` at each time step will be a `Tensor` or (possibly

        nested) tuple of Tensors each with dimensions `[batch_size, ...]`.

      sequence_length: (optional) An int32/int64 vector sized `[batch_size]`.

        Used to copy-through state and zero-out outputs when past a batch

        element's sequence length.  So it's more for correctness than performance.

      initial_state: (optional) An initial state for the RNN.

        If `cell.state_size` is an integer, this must be

        a `Tensor` of appropriate type and shape `[batch_size, cell.state_size]`.

        If `cell.state_size` is a tuple, this should be a tuple of

        tensors having shapes `[batch_size, s] for s in cell.state_size`.

      dtype: (optional) The data type for the initial state and expected output.

        Required if initial_state is not provided or RNN state has a heterogeneous

        dtype.

      parallel_iterations: (Default: 32).  The number of iterations to run in

        parallel.  Those operations which do not have any temporal dependency

        and can be run in parallel, will be.  This parameter trades off

        time for space.  Values >> 1 use more memory but take less time,

        while smaller values use less memory but computations take longer.

      swap_memory: Transparently swap the tensors produced in forward inference

        but needed for back prop from GPU to CPU.  This allows training RNNs

        which would typically not fit on a single GPU, with very minimal (or no)

        performance penalty.

      time_major: The shape format of the `inputs` and `outputs` Tensors.

        If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.

        If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.

        Using `time_major = True` is a bit more efficient because it avoids

        transposes at the beginning and end of the RNN calculation.  However,

        most TensorFlow data is batch-major, so by default this function

        accepts input and emits output in batch-major form.

      scope: VariableScope for the created subgraph; defaults to ""rnn"".



    Returns:

      A pair (outputs, state) where:



      outputs: The RNN output `Tensor`.



        If time_major == False (default), this will be a `Tensor` shaped:

          `[batch_size, max_time, cell.output_size]`.



        If time_major == True, this will be a `Tensor` shaped:

          `[max_time, batch_size, cell.output_size]`.



        Note, if `cell.output_size` is a (possibly nested) tuple of integers

        or `TensorShape` objects, then `outputs` will be a tuple having the

        same structure as `cell.output_size`, containing Tensors having shapes

        corresponding to the shape data in `cell.output_size`.



      state: The final state.  If `cell.state_size` is an int, this

        will be shaped `[batch_size, cell.state_size]`.  If it is a

        `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.

        If it is a (possibly nested) tuple of ints or `TensorShape`, this will

        be a tuple having the corresponding shapes. If cells are `LSTMCells`

        `state` will be a tuple containing a `LSTMStateTuple` for each cell.



    Raises:

      TypeError: If `cell` is not an instance of RNNCell.

      ValueError: If inputs is None or an empty list.

    """"""

    if not _like_rnncell(cell):
        raise TypeError(""cell must be an instance of RNNCell"")

    # By default, time_major==False and inputs are batch-major: shaped

    #   [batch, time, depth]

    # For internal calculations, we transpose to [time, batch, depth]

    flat_input = nest.flatten(inputs)

    if not time_major:
        # (B,T,D) => (T,B,D)

        flat_input = [ops.convert_to_tensor(input_) for input_ in flat_input]

        flat_input = tuple(_transpose_batch_time(input_) for input_ in flat_input)

    parallel_iterations = parallel_iterations or 32

    if sequence_length is not None:

        sequence_length = math_ops.to_int32(sequence_length)

        if sequence_length.get_shape().ndims not in (None, 1):
            raise ValueError(

                ""sequence_length must be a vector of length batch_size, ""

                ""but saw shape: %s"" % sequence_length.get_shape())

        sequence_length = array_ops.identity(  # Just to find it in the graph.

            sequence_length, name=""sequence_length"")

    # Create a new scope in which the caching device is either

    # determined by the parent scope, or is set to place the cached

    # Variable using the same placement as for the rest of the RNN.

    try:
        resue = tf.AUTO_REUSE
    except:
        resue = tf.compat.v1.AUTO_REUSE

    with vs.variable_scope(scope or ""rnn"",reuse=resue) as varscope:#TODO:user defined reuse

        if varscope.caching_device is None:
            varscope.set_caching_device(lambda op: op.device)

        batch_size = _best_effort_input_batch_size(flat_input)

        if initial_state is not None:

            state = initial_state

        else:

            if not dtype:
                raise ValueError(""If there is no initial_state, you must give a dtype."")

            state = cell.zero_state(batch_size, dtype)

        def _assert_has_shape(x, shape):

            x_shape = array_ops.shape(x)

            packed_shape = array_ops.stack(shape)

            return control_flow_ops.Assert(

                math_ops.reduce_all(math_ops.equal(x_shape, packed_shape)),

                [""Expected shape for Tensor %s is "" % x.name,

                 packed_shape, "" but saw shape: "", x_shape])

        if sequence_length is not None:
            # Perform some shape validation

            with ops.control_dependencies(

                    [_assert_has_shape(sequence_length, [batch_size])]):
                sequence_length = array_ops.identity(

                    sequence_length, name=""CheckSeqLen"")

        inputs = nest.pack_sequence_as(structure=inputs, flat_sequence=flat_input)

        (outputs, final_state) = _dynamic_rnn_loop(

            cell,

            inputs,

            state,

            parallel_iterations=parallel_iterations,

            swap_memory=swap_memory,

            att_scores=att_scores,

            sequence_length=sequence_length,

            dtype=dtype)

        # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].

        # If we are performing batch-major calculations, transpose output back

        # to shape [batch, time, depth]

        if not time_major:
            # (T,B,D) => (B,T,D)

            outputs = nest.map_structure(_transpose_batch_time, outputs)

        return (outputs, final_state)


def _dynamic_rnn_loop(cell,

                      inputs,

                      initial_state,

                      parallel_iterations,

                      swap_memory,

                      att_scores=None,

                      sequence_length=None,

                      dtype=None):
    """"""Internal implementation of Dynamic RNN.



    Args:

      cell: An instance of RNNCell.

      inputs: A `Tensor` of shape [time, batch_size, input_size], or a nested

        tuple of such elements.

      initial_state: A `Tensor` of shape `[batch_size, state_size]`, or if

        `cell.state_size` is a tuple, then this should be a tuple of

        tensors having shapes `[batch_size, s] for s in cell.state_size`.

      parallel_iterations: Positive Python int.

      swap_memory: A Python boolean

      sequence_length: (optional) An `int32` `Tensor` of shape [batch_size].

      dtype: (optional) Expected dtype of output. If not specified, inferred from

        initial_state.



    Returns:

      Tuple `(final_outputs, final_state)`.

      final_outputs:

        A `Tensor` of shape `[time, batch_size, cell.output_size]`.  If

        `cell.output_size` is a (possibly nested) tuple of ints or `TensorShape`

        objects, then this returns a (possibly nsted) tuple of Tensors matching

        the corresponding shapes.

      final_state:

        A `Tensor`, or possibly nested tuple of Tensors, matching in length

        and shapes to `initial_state`.



    Raises:

      ValueError: If the input depth cannot be inferred via shape inference

        from the inputs.

    """"""

    state = initial_state

    assert isinstance(parallel_iterations, int), ""parallel_iterations must be int""

    state_size = cell.state_size

    flat_input = nest.flatten(inputs)

    flat_output_size = nest.flatten(cell.output_size)

    # Construct an initial output

    input_shape = array_ops.shape(flat_input[0])

    time_steps = input_shape[0]

    batch_size = _best_effort_input_batch_size(flat_input)

    inputs_got_shape = tuple(input_.get_shape().with_rank_at_least(3)

                             for input_ in flat_input)

    const_time_steps, const_batch_size = inputs_got_shape[0].as_list()[:2]

    for shape in inputs_got_shape:

        if not shape[2:].is_fully_defined():
            raise ValueError(

                ""Input size (depth of inputs) must be accessible via shape inference,""

                "" but saw value None."")

        got_time_steps = shape[0]

        got_batch_size = shape[1]

        if const_time_steps != got_time_steps:
            raise ValueError(

                ""Time steps is not the same for all the elements in the input in a ""

                ""batch."")

        if const_batch_size != got_batch_size:
            raise ValueError(

                ""Batch_size is not the same for all the elements in the input."")

    # Prepare dynamic conditional copying of state & output

    def _create_zero_arrays(size):

        size = _concat(batch_size, size)

        return array_ops.zeros(

            array_ops.stack(size), _infer_state_dtype(dtype, state))

    flat_zero_output = tuple(_create_zero_arrays(output)

                             for output in flat_output_size)

    zero_output = nest.pack_sequence_as(structure=cell.output_size,

                                        flat_sequence=flat_zero_output)

    if sequence_length is not None:
        min_sequence_length = math_ops.reduce_min(sequence_length)

        max_sequence_length = math_ops.reduce_max(sequence_length)

    time = array_ops.constant(0, dtype=dtypes.int32, name=""time"")

    with ops.name_scope(""dynamic_rnn"") as scope:

        base_name = scope

    def _create_ta(name, dtype):

        return tensor_array_ops.TensorArray(dtype=dtype,

                                            size=time_steps,

                                            tensor_array_name=base_name + name)

    output_ta = tuple(_create_ta(""output_%d"" % i,

                                 _infer_state_dtype(dtype, state))

                      for i in range(len(flat_output_size)))

    input_ta = tuple(_create_ta(""input_%d"" % i, flat_input[i].dtype)

                     for i in range(len(flat_input)))

    input_ta = tuple(ta.unstack(input_)

                     for ta, input_ in zip(input_ta, flat_input))

    def _time_step(time, output_ta_t, state, att_scores=None):

        """"""Take a time step of the dynamic RNN.



        Args:

          time: int32 scalar Tensor.

          output_ta_t: List of `TensorArray`s that represent the output.

          state: nested tuple of vector tensors that represent the state.



        Returns:

          The tuple (time + 1, output_ta_t with updated flow, new_state).

        """"""

        input_t = tuple(ta.read(time) for ta in input_ta)

        # Restore some shape information

        for input_, shape in zip(input_t, inputs_got_shape):
            input_.set_shape(shape[1:])

        input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)

        if att_scores is not None:

            att_score = att_scores[:, time, :]

            call_cell = lambda: cell(input_t, state, att_score)

        else:

            call_cell = lambda: cell(input_t, state)

        if sequence_length is not None:

            (output, new_state) = _rnn_step(

                time=time,

                sequence_length=sequence_length,

                min_sequence_length=min_sequence_length,

                max_sequence_length=max_sequence_length,

                zero_output=zero_output,

                state=state,

                call_cell=call_cell,

                state_size=state_size,

                skip_conditionals=True)

        else:

            (output, new_state) = call_cell()

        # Pack state if using state tuples

        output = nest.flatten(output)

        output_ta_t = tuple(

            ta.write(time, out) for ta, out in zip(output_ta_t, output))

        if att_scores is not None:

            return (time + 1, output_ta_t, new_state, att_scores)

        else:

            return (time + 1, output_ta_t, new_state)

    if att_scores is not None:

        _, output_final_ta, final_state, _ = control_flow_ops.while_loop(

            cond=lambda time, *_: time < time_steps,

            body=_time_step,

            loop_vars=(time, output_ta, state, att_scores),

            parallel_iterations=parallel_iterations,

            swap_memory=swap_memory)

    else:

        _, output_final_ta, final_state = control_flow_ops.while_loop(

            cond=lambda time, *_: time < time_steps,

            body=_time_step,

            loop_vars=(time, output_ta, state),

            parallel_iterations=parallel_iterations,

            swap_memory=swap_memory)

    # Unpack final output if not using output tuples.

    final_outputs = tuple(ta.stack() for ta in output_final_ta)

    # Restore some shape information

    for output, output_size in zip(final_outputs, flat_output_size):
        shape = _concat(

            [const_time_steps, const_batch_size], output_size, static=True)

        output.set_shape(shape)

    final_outputs = nest.pack_sequence_as(

        structure=cell.output_size, flat_sequence=final_outputs)

    return (final_outputs, final_state)
"
DeepCTR,rnn.py,"# Copyright 2015 The TensorFlow Authors. All Rights Reserved.

#

# Licensed under the Apache License, Version 2.0 (the ""License"");

# you may not use this file except in compliance with the License.

# You may obtain a copy of the License at

#

#     http://www.apache.org/licenses/LICENSE-2.0

#

# Unless required by applicable law or agreed to in writing, software

# distributed under the License is distributed on an ""AS IS"" BASIS,

# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

# See the License for the specific language governing permissions and

# limitations under the License.

# ==============================================================================


""""""RNN helpers for TensorFlow models.
@@bidirectional_dynamic_rnn
@@dynamic_rnn
@@raw_rnn
@@static_rnn
@@static_state_saving_rnn
@@static_bidirectional_rnn
""""""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.framework import tensor_shape
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import rnn_cell_impl
from tensorflow.python.ops import tensor_array_ops
from tensorflow.python.ops import variable_scope as vs
from tensorflow.python.util import nest
import tensorflow as tf


def _like_rnncell_(cell):
    """"""Checks that a given object is an RNNCell by using duck typing.""""""

    conditions = [hasattr(cell, ""output_size""), hasattr(cell, ""state_size""),

                  hasattr(cell, ""zero_state""), callable(cell)]

    return all(conditions)


# pylint: disable=protected-access

_concat = rnn_cell_impl._concat
try:
    _like_rnncell = rnn_cell_impl._like_rnncell
except Exception as e:
    _like_rnncell = _like_rnncell_


# pylint: enable=protected-access


def _transpose_batch_time(x):
    """"""Transpose the batch and time dimensions of a Tensor.
    Retains as much of the static shape information as possible.
    Args:
      x: A tensor of rank 2 or higher.
    Returns:
      x transposed along the first two dimensions.
    Raises:
      ValueError: if `x` is rank 1 or lower.
    """"""

    x_static_shape = x.get_shape()

    if x_static_shape.ndims is not None and x_static_shape.ndims < 2:
        raise ValueError(

            ""Expected input tensor %s to have rank at least 2, but saw shape: %s"" %

            (x, x_static_shape))

    x_rank = array_ops.rank(x)

    x_t = array_ops.transpose(

        x, array_ops.concat(

            ([1, 0], math_ops.range(2, x_rank)), axis=0))

    x_t.set_shape(

        tensor_shape.TensorShape([

            x_static_shape[1].value, x_static_shape[0].value

        ]).concatenate(x_static_shape[2:]))

    return x_t


def _best_effort_input_batch_size(flat_input):
    """"""Get static input batch size if available, with fallback to the dynamic one.
    Args:
      flat_input: An iterable of time major input Tensors of shape [max_time,
        batch_size, ...]. All inputs should have compatible batch sizes.
    Returns:
      The batch size in Python integer if available, or a scalar Tensor otherwise.
    Raises:
      ValueError: if there is any input with an invalid shape.
    """"""

    for input_ in flat_input:

        shape = input_.shape

        if shape.ndims is None:
            continue

        if shape.ndims < 2:
            raise ValueError(

                ""Expected input tensor %s to have rank at least 2"" % input_)

        batch_size = shape[1].value

        if batch_size is not None:
            return batch_size

    # Fallback to the dynamic batch size of the first input.

    return array_ops.shape(flat_input[0])[1]


def _infer_state_dtype(explicit_dtype, state):
    """"""Infer the dtype of an RNN state.
    Args:
      explicit_dtype: explicitly declared dtype or None.
      state: RNN's hidden state. Must be a Tensor or a nested iterable containing
        Tensors.
    Returns:
      dtype: inferred dtype of hidden state.
    Raises:
      ValueError: if `state` has heterogeneous dtypes or is empty.
    """"""

    if explicit_dtype is not None:

        return explicit_dtype

    elif nest.is_sequence(state):

        inferred_dtypes = [element.dtype for element in nest.flatten(state)]

        if not inferred_dtypes:
            raise ValueError(""Unable to infer dtype from empty state."")

        all_same = all([x == inferred_dtypes[0] for x in inferred_dtypes])

        if not all_same:
            raise ValueError(

                ""State has tensors of different inferred_dtypes. Unable to infer a ""

                ""single representative dtype."")

        return inferred_dtypes[0]

    else:

        return state.dtype


# pylint: disable=unused-argument

def _rnn_step(

        time, sequence_length, min_sequence_length, max_sequence_length,

        zero_output, state, call_cell, state_size, skip_conditionals=False):
    """"""Calculate one step of a dynamic RNN minibatch.
    Returns an (output, state) pair conditioned on the sequence_lengths.
    When skip_conditionals=False, the pseudocode is something like:
    if t >= max_sequence_length:
      return (zero_output, state)
    if t < min_sequence_length:
      return call_cell()
    # Selectively output zeros or output, old state or new state depending
    # on if we've finished calculating each row.
    new_output, new_state = call_cell()
    final_output = np.vstack([
      zero_output if time >= sequence_lengths[r] else new_output_r
      for r, new_output_r in enumerate(new_output)
    ])
    final_state = np.vstack([
      state[r] if time >= sequence_lengths[r] else new_state_r
      for r, new_state_r in enumerate(new_state)
    ])
    return (final_output, final_state)
    Args:
      time: Python int, the current time step
      sequence_length: int32 `Tensor` vector of size [batch_size]
      min_sequence_length: int32 `Tensor` scalar, min of sequence_length
      max_sequence_length: int32 `Tensor` scalar, max of sequence_length
      zero_output: `Tensor` vector of shape [output_size]
      state: Either a single `Tensor` matrix of shape `[batch_size, state_size]`,
        or a list/tuple of such tensors.
      call_cell: lambda returning tuple of (new_output, new_state) where
        new_output is a `Tensor` matrix of shape `[batch_size, output_size]`.
        new_state is a `Tensor` matrix of shape `[batch_size, state_size]`.
      state_size: The `cell.state_size` associated with the state.
      skip_conditionals: Python bool, whether to skip using the conditional
        calculations.  This is useful for `dynamic_rnn`, where the input tensor
        matches `max_sequence_length`, and using conditionals just slows
        everything down.
    Returns:
      A tuple of (`final_output`, `final_state`) as given by the pseudocode above:
        final_output is a `Tensor` matrix of shape [batch_size, output_size]
        final_state is either a single `Tensor` matrix, or a tuple of such
          matrices (matching length and shapes of input `state`).
    Raises:
      ValueError: If the cell returns a state tuple whose length does not match
        that returned by `state_size`.
    """"""

    # Convert state to a list for ease of use

    flat_state = nest.flatten(state)

    flat_zero_output = nest.flatten(zero_output)

    def _copy_one_through(output, new_output):

        # If the state contains a scalar value we simply pass it through.

        if output.shape.ndims == 0:
            return new_output

        copy_cond = (time >= sequence_length)

        with ops.colocate_with(new_output):
            return array_ops.where(copy_cond, output, new_output)

    def _copy_some_through(flat_new_output, flat_new_state):

        # Use broadcasting select to determine which values should get

        # the previous state & zero output, and which values should get

        # a calculated state & output.

        flat_new_output = [

            _copy_one_through(zero_output, new_output)

            for zero_output, new_output in zip(flat_zero_output, flat_new_output)]

        flat_new_state = [

            _copy_one_through(state, new_state)

            for state, new_state in zip(flat_state, flat_new_state)]

        return flat_new_output + flat_new_state

    def _maybe_copy_some_through():

        """"""Run RNN step.  Pass through either no or some past state.""""""

        new_output, new_state = call_cell()

        nest.assert_same_structure(state, new_state)

        flat_new_state = nest.flatten(new_state)

        flat_new_output = nest.flatten(new_output)

        return control_flow_ops.cond(

            # if t < min_seq_len: calculate and return everything

            time < min_sequence_length, lambda: flat_new_output + flat_new_state,

            # else copy some of it through

            lambda: _copy_some_through(flat_new_output, flat_new_state))

    # TODO(ebrevdo): skipping these conditionals may cause a slowdown,

    # but benefits from removing cond() and its gradient.  We should

    # profile with and without this switch here.

    if skip_conditionals:

        # Instead of using conditionals, perform the selective copy at all time

        # steps.  This is faster when max_seq_len is equal to the number of unrolls

        # (which is typical for dynamic_rnn).

        new_output, new_state = call_cell()

        nest.assert_same_structure(state, new_state)

        new_state = nest.flatten(new_state)

        new_output = nest.flatten(new_output)

        final_output_and_state = _copy_some_through(new_output, new_state)

    else:

        empty_update = lambda: flat_zero_output + flat_state

        final_output_and_state = control_flow_ops.cond(

            # if t >= max_seq_len: copy all state through, output zeros

            time >= max_sequence_length, empty_update,

            # otherwise calculation is required: copy some or all of it through

            _maybe_copy_some_through)

    if len(final_output_and_state) != len(flat_zero_output) + len(flat_state):
        raise ValueError(""Internal error: state and output were not concatenated ""

                         ""correctly."")

    final_output = final_output_and_state[:len(flat_zero_output)]

    final_state = final_output_and_state[len(flat_zero_output):]

    for output, flat_output in zip(final_output, flat_zero_output):
        output.set_shape(flat_output.get_shape())

    for substate, flat_substate in zip(final_state, flat_state):
        substate.set_shape(flat_substate.get_shape())

    final_output = nest.pack_sequence_as(

        structure=zero_output, flat_sequence=final_output)

    final_state = nest.pack_sequence_as(

        structure=state, flat_sequence=final_state)

    return final_output, final_state


def _reverse_seq(input_seq, lengths):
    """"""Reverse a list of Tensors up to specified lengths.
    Args:
      input_seq: Sequence of seq_len tensors of dimension (batch_size, n_features)
                 or nested tuples of tensors.
      lengths:   A `Tensor` of dimension batch_size, containing lengths for each
                 sequence in the batch. If ""None"" is specified, simply reverses
                 the list.
    Returns:
      time-reversed sequence
    """"""

    if lengths is None:
        return list(reversed(input_seq))

    flat_input_seq = tuple(nest.flatten(input_) for input_ in input_seq)

    flat_results = [[] for _ in range(len(input_seq))]

    for sequence in zip(*flat_input_seq):

        input_shape = tensor_shape.unknown_shape(

            ndims=sequence[0].get_shape().ndims)

        for input_ in sequence:
            input_shape.merge_with(input_.get_shape())

            input_.set_shape(input_shape)

        # Join into (time, batch_size, depth)

        s_joined = array_ops.stack(sequence)

        # Reverse along dimension 0

        s_reversed = array_ops.reverse_sequence(s_joined, lengths, 0, 1)

        # Split again into list

        result = array_ops.unstack(s_reversed)

        for r, flat_result in zip(result, flat_results):
            r.set_shape(input_shape)

            flat_result.append(r)

    results = [nest.pack_sequence_as(structure=input_, flat_sequence=flat_result)

               for input_, flat_result in zip(input_seq, flat_results)]

    return results


#
# def bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs, sequence_length=None,
#
#                               initial_state_fw=None, initial_state_bw=None,
#
#                               dtype=None, parallel_iterations=None,
#
#                               swap_memory=False, time_major=False, scope=None):
#
#   """"""Creates a dynamic version of bidirectional recurrent neural network.
#
#
#
#   Takes input and builds independent forward and backward RNNs. The input_size
#
#   of forward and backward cell must match. The initial state for both directions
#
#   is zero by default (but can be set optionally) and no intermediate states are
#
#   ever returned -- the network is fully unrolled for the given (passed in)
#
#   length(s) of the sequence(s) or completely unrolled if length(s) is not
#
#   given.
#
#
#
#   Args:
#
#     cell_fw: An instance of RNNCell, to be used for forward direction.
#
#     cell_bw: An instance of RNNCell, to be used for backward direction.
#
#     inputs: The RNN inputs.
#
#       If time_major == False (default), this must be a tensor of shape:
#
#         `[batch_size, max_time, ...]`, or a nested tuple of such elements.
#
#       If time_major == True, this must be a tensor of shape:
#
#         `[max_time, batch_size, ...]`, or a nested tuple of such elements.
#
#     sequence_length: (optional) An int32/int64 vector, size `[batch_size]`,
#
#       containing the actual lengths for each of the sequences in the batch.
#
#       If not provided, all batch entries are assumed to be full sequences; and
#
#       time reversal is applied from time `0` to `max_time` for each sequence.
#
#     initial_state_fw: (optional) An initial state for the forward RNN.
#
#       This must be a tensor of appropriate type and shape
#
#       `[batch_size, cell_fw.state_size]`.
#
#       If `cell_fw.state_size` is a tuple, this should be a tuple of
#
#       tensors having shapes `[batch_size, s] for s in cell_fw.state_size`.
#
#     initial_state_bw: (optional) Same as for `initial_state_fw`, but using
#
#       the corresponding properties of `cell_bw`.
#
#     dtype: (optional) The data type for the initial states and expected output.
#
#       Required if initial_states are not provided or RNN states have a
#
#       heterogeneous dtype.
#
#     parallel_iterations: (Default: 32).  The number of iterations to run in
#
#       parallel.  Those operations which do not have any temporal dependency
#
#       and can be run in parallel, will be.  This parameter trades off
#
#       time for space.  Values >> 1 use more memory but take less time,
#
#       while smaller values use less memory but computations take longer.
#
#     swap_memory: Transparently swap the tensors produced in forward inference
#
#       but needed for back prop from GPU to CPU.  This allows training RNNs
#
#       which would typically not fit on a single GPU, with very minimal (or no)
#
#       performance penalty.
#
#     time_major: The shape format of the `inputs` and `outputs` Tensors.
#
#       If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.
#
#       If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.
#
#       Using `time_major = True` is a bit more efficient because it avoids
#
#       transposes at the beginning and end of the RNN calculation.  However,
#
#       most TensorFlow data is batch-major, so by default this function
#
#       accepts input and emits output in batch-major form.
#
#     scope: VariableScope for the created subgraph; defaults to
#
#       ""bidirectional_rnn""
#
#
#
#   Returns:
#
#     A tuple (outputs, output_states) where:
#
#       outputs: A tuple (output_fw, output_bw) containing the forward and
#
#         the backward rnn output `Tensor`.
#
#         If time_major == False (default),
#
#           output_fw will be a `Tensor` shaped:
#
#           `[batch_size, max_time, cell_fw.output_size]`
#
#           and output_bw will be a `Tensor` shaped:
#
#           `[batch_size, max_time, cell_bw.output_size]`.
#
#         If time_major == True,
#
#           output_fw will be a `Tensor` shaped:
#
#           `[max_time, batch_size, cell_fw.output_size]`
#
#           and output_bw will be a `Tensor` shaped:
#
#           `[max_time, batch_size, cell_bw.output_size]`.
#
#         It returns a tuple instead of a single concatenated `Tensor`, unlike
#
#         in the `bidirectional_rnn`. If the concatenated one is preferred,
#
#         the forward and backward outputs can be concatenated as
#
#         `tf.concat(outputs, 2)`.
#
#       output_states: A tuple (output_state_fw, output_state_bw) containing
#
#         the forward and the backward final states of bidirectional rnn.
#
#
#
#   Raises:
#
#     TypeError: If `cell_fw` or `cell_bw` is not an instance of `RNNCell`.
#
#   """"""
#
#
#
#   if not _like_rnncell(cell_fw):
#
#     raise TypeError(""cell_fw must be an instance of RNNCell"")
#
#   if not _like_rnncell(cell_bw):
#
#     raise TypeError(""cell_bw must be an instance of RNNCell"")
#
#
#
#   with vs.variable_scope(scope or ""bidirectional_rnn""):
#
#     # Forward direction
#
#     with vs.variable_scope(""fw"") as fw_scope:
#
#       output_fw, output_state_fw = dynamic_rnn(
#
#           cell=cell_fw, inputs=inputs, sequence_length=sequence_length,
#
#           initial_state=initial_state_fw, dtype=dtype,
#
#           parallel_iterations=parallel_iterations, swap_memory=swap_memory,
#
#           time_major=time_major, scope=fw_scope)
#
#
#
#     # Backward direction
#
#     if not time_major:
#
#       time_dim = 1
#
#       batch_dim = 0
#
#     else:
#
#       time_dim = 0
#
#       batch_dim = 1
#
#
#
#     def _reverse(input_, seq_lengths, seq_dim, batch_dim):
#
#       if seq_lengths is not None:
#
#         return array_ops.reverse_sequence(
#
#             input=input_, seq_lengths=seq_lengths,
#
#             seq_dim=seq_dim, batch_dim=batch_dim)
#
#       else:
#
#         return array_ops.reverse(input_, axis=[seq_dim])
#
#
#
#     with vs.variable_scope(""bw"") as bw_scope:
#
#       inputs_reverse = _reverse(
#
#           inputs, seq_lengths=sequence_length,
#
#           seq_dim=time_dim, batch_dim=batch_dim)
#
#       tmp, output_state_bw = dynamic_rnn(
#
#           cell=cell_bw, inputs=inputs_reverse, sequence_length=sequence_length,
#
#           initial_state=initial_state_bw, dtype=dtype,
#
#           parallel_iterations=parallel_iterations, swap_memory=swap_memory,
#
#           time_major=time_major, scope=bw_scope)
#
#
#
#   output_bw = _reverse(
#
#       tmp, seq_lengths=sequence_length,
#
#       seq_dim=time_dim, batch_dim=batch_dim)
#
#
#
#   outputs = (output_fw, output_bw)
#
#   output_states = (output_state_fw, output_state_bw)
#
#
#
#   return (outputs, output_states)
#


def dynamic_rnn(cell, inputs, att_scores=None, sequence_length=None, initial_state=None,

                dtype=None, parallel_iterations=None, swap_memory=False,

                time_major=False, scope=None):
    """"""Creates a recurrent neural network specified by RNNCell `cell`.
    Performs fully dynamic unrolling of `inputs`.
    Example:
    ```python
    # create a BasicRNNCell
    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)
    # 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]
    # defining initial state
    initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)
    # 'state' is a tensor of shape [batch_size, cell_state_size]
    outputs, state = tf.nn.dynamic_rnn(rnn_cell, input_data,
                                       initial_state=initial_state,
                                       dtype=tf.float32)
    ```
    ```python
    # create 2 LSTMCells
    rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [128, 256]]
    # create a RNN cell composed sequentially of a number of RNNCells
    multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)
    # 'outputs' is a tensor of shape [batch_size, max_time, 256]
    # 'state' is a N-tuple where N is the number of LSTMCells containing a
    # tf.contrib.rnn.LSTMStateTuple for each cell
    outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,
                                       inputs=data,
                                       dtype=tf.float32)
    ```
    Args:
      cell: An instance of RNNCell.
      inputs: The RNN inputs.
        If `time_major == False` (default), this must be a `Tensor` of shape:
          `[batch_size, max_time, ...]`, or a nested tuple of such
          elements.
        If `time_major == True`, this must be a `Tensor` of shape:
          `[max_time, batch_size, ...]`, or a nested tuple of such
          elements.
        This may also be a (possibly nested) tuple of Tensors satisfying
        this property.  The first two dimensions must match across all the inputs,
        but otherwise the ranks and other shape components may differ.
        In this case, input to `cell` at each time-step will replicate the
        structure of these tuples, except for the time dimension (from which the
        time is taken).
        The input to `cell` at each time step will be a `Tensor` or (possibly
        nested) tuple of Tensors each with dimensions `[batch_size, ...]`.
      sequence_length: (optional) An int32/int64 vector sized `[batch_size]`.
        Used to copy-through state and zero-out outputs when past a batch
        element's sequence length.  So it's more for correctness than performance.
      initial_state: (optional) An initial state for the RNN.
        If `cell.state_size` is an integer, this must be
        a `Tensor` of appropriate type and shape `[batch_size, cell.state_size]`.
        If `cell.state_size` is a tuple, this should be a tuple of
        tensors having shapes `[batch_size, s] for s in cell.state_size`.
      dtype: (optional) The data type for the initial state and expected output.
        Required if initial_state is not provided or RNN state has a heterogeneous
        dtype.
      parallel_iterations: (Default: 32).  The number of iterations to run in
        parallel.  Those operations which do not have any temporal dependency
        and can be run in parallel, will be.  This parameter trades off
        time for space.  Values >> 1 use more memory but take less time,
        while smaller values use less memory but computations take longer.
      swap_memory: Transparently swap the tensors produced in forward inference
        but needed for back prop from GPU to CPU.  This allows training RNNs
        which would typically not fit on a single GPU, with very minimal (or no)
        performance penalty.
      time_major: The shape format of the `inputs` and `outputs` Tensors.
        If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.
        If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.
        Using `time_major = True` is a bit more efficient because it avoids
        transposes at the beginning and end of the RNN calculation.  However,
        most TensorFlow data is batch-major, so by default this function
        accepts input and emits output in batch-major form.
      scope: VariableScope for the created subgraph; defaults to ""rnn"".
    Returns:
      A pair (outputs, state) where:
      outputs: The RNN output `Tensor`.
        If time_major == False (default), this will be a `Tensor` shaped:
          `[batch_size, max_time, cell.output_size]`.
        If time_major == True, this will be a `Tensor` shaped:
          `[max_time, batch_size, cell.output_size]`.
        Note, if `cell.output_size` is a (possibly nested) tuple of integers
        or `TensorShape` objects, then `outputs` will be a tuple having the
        same structure as `cell.output_size`, containing Tensors having shapes
        corresponding to the shape data in `cell.output_size`.
      state: The final state.  If `cell.state_size` is an int, this
        will be shaped `[batch_size, cell.state_size]`.  If it is a
        `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.
        If it is a (possibly nested) tuple of ints or `TensorShape`, this will
        be a tuple having the corresponding shapes. If cells are `LSTMCells`
        `state` will be a tuple containing a `LSTMStateTuple` for each cell.
    Raises:
      TypeError: If `cell` is not an instance of RNNCell.
      ValueError: If inputs is None or an empty list.
    """"""

    if not _like_rnncell(cell):
        raise TypeError(""cell must be an instance of RNNCell"")

    # By default, time_major==False and inputs are batch-major: shaped

    #   [batch, time, depth]

    # For internal calculations, we transpose to [time, batch, depth]

    flat_input = nest.flatten(inputs)

    if not time_major:
        # (B,T,D) => (T,B,D)

        flat_input = [ops.convert_to_tensor(input_) for input_ in flat_input]

        flat_input = tuple(_transpose_batch_time(input_) for input_ in flat_input)

    parallel_iterations = parallel_iterations or 32

    if sequence_length is not None:

        sequence_length = math_ops.to_int32(sequence_length)

        if sequence_length.get_shape().ndims not in (None, 1):
            raise ValueError(

                ""sequence_length must be a vector of length batch_size, ""

                ""but saw shape: %s"" % sequence_length.get_shape())

        sequence_length = array_ops.identity(  # Just to find it in the graph.

            sequence_length, name=""sequence_length"")

    # Create a new scope in which the caching device is either

    # determined by the parent scope, or is set to place the cached

    # Variable using the same placement as for the rest of the RNN.

    with vs.variable_scope(scope or ""rnn"",reuse=tf.AUTO_REUSE) as varscope:#TODO:user defined reuse

        if varscope.caching_device is None:
            varscope.set_caching_device(lambda op: op.device)

        batch_size = _best_effort_input_batch_size(flat_input)

        if initial_state is not None:

            state = initial_state

        else:

            if not dtype:
                raise ValueError(""If there is no initial_state, you must give a dtype."")

            state = cell.zero_state(batch_size, dtype)

        def _assert_has_shape(x, shape):

            x_shape = array_ops.shape(x)

            packed_shape = array_ops.stack(shape)

            return control_flow_ops.Assert(

                math_ops.reduce_all(math_ops.equal(x_shape, packed_shape)),

                [""Expected shape for Tensor %s is "" % x.name,

                 packed_shape, "" but saw shape: "", x_shape])

        if sequence_length is not None:
            # Perform some shape validation

            with ops.control_dependencies(

                    [_assert_has_shape(sequence_length, [batch_size])]):
                sequence_length = array_ops.identity(

                    sequence_length, name=""CheckSeqLen"")

        inputs = nest.pack_sequence_as(structure=inputs, flat_sequence=flat_input)

        (outputs, final_state) = _dynamic_rnn_loop(

            cell,

            inputs,

            state,

            parallel_iterations=parallel_iterations,

            swap_memory=swap_memory,

            att_scores=att_scores,

            sequence_length=sequence_length,

            dtype=dtype)

        # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].

        # If we are performing batch-major calculations, transpose output back

        # to shape [batch, time, depth]

        if not time_major:
            # (T,B,D) => (B,T,D)

            outputs = nest.map_structure(_transpose_batch_time, outputs)

        return (outputs, final_state)


def _dynamic_rnn_loop(cell,

                      inputs,

                      initial_state,

                      parallel_iterations,

                      swap_memory,

                      att_scores=None,

                      sequence_length=None,

                      dtype=None):
    """"""Internal implementation of Dynamic RNN.
    Args:
      cell: An instance of RNNCell.
      inputs: A `Tensor` of shape [time, batch_size, input_size], or a nested
        tuple of such elements.
      initial_state: A `Tensor` of shape `[batch_size, state_size]`, or if
        `cell.state_size` is a tuple, then this should be a tuple of
        tensors having shapes `[batch_size, s] for s in cell.state_size`.
      parallel_iterations: Positive Python int.
      swap_memory: A Python boolean
      sequence_length: (optional) An `int32` `Tensor` of shape [batch_size].
      dtype: (optional) Expected dtype of output. If not specified, inferred from
        initial_state.
    Returns:
      Tuple `(final_outputs, final_state)`.
      final_outputs:
        A `Tensor` of shape `[time, batch_size, cell.output_size]`.  If
        `cell.output_size` is a (possibly nested) tuple of ints or `TensorShape`
        objects, then this returns a (possibly nsted) tuple of Tensors matching
        the corresponding shapes.
      final_state:
        A `Tensor`, or possibly nested tuple of Tensors, matching in length
        and shapes to `initial_state`.
    Raises:
      ValueError: If the input depth cannot be inferred via shape inference
        from the inputs.
    """"""

    state = initial_state

    assert isinstance(parallel_iterations, int), ""parallel_iterations must be int""

    state_size = cell.state_size

    flat_input = nest.flatten(inputs)

    flat_output_size = nest.flatten(cell.output_size)

    # Construct an initial output

    input_shape = array_ops.shape(flat_input[0])

    time_steps = input_shape[0]

    batch_size = _best_effort_input_batch_size(flat_input)

    inputs_got_shape = tuple(input_.get_shape().with_rank_at_least(3)

                             for input_ in flat_input)

    const_time_steps, const_batch_size = inputs_got_shape[0].as_list()[:2]

    for shape in inputs_got_shape:

        if not shape[2:].is_fully_defined():
            raise ValueError(

                ""Input size (depth of inputs) must be accessible via shape inference,""

                "" but saw value None."")

        got_time_steps = shape[0].value

        got_batch_size = shape[1].value

        if const_time_steps != got_time_steps:
            raise ValueError(

                ""Time steps is not the same for all the elements in the input in a ""

                ""batch."")

        if const_batch_size != got_batch_size:
            raise ValueError(

                ""Batch_size is not the same for all the elements in the input."")

    # Prepare dynamic conditional copying of state & output

    def _create_zero_arrays(size):

        size = _concat(batch_size, size)

        return array_ops.zeros(

            array_ops.stack(size), _infer_state_dtype(dtype, state))

    flat_zero_output = tuple(_create_zero_arrays(output)

                             for output in flat_output_size)

    zero_output = nest.pack_sequence_as(structure=cell.output_size,

                                        flat_sequence=flat_zero_output)

    if sequence_length is not None:
        min_sequence_length = math_ops.reduce_min(sequence_length)

        max_sequence_length = math_ops.reduce_max(sequence_length)

    time = array_ops.constant(0, dtype=dtypes.int32, name=""time"")

    with ops.name_scope(""dynamic_rnn"") as scope:

        base_name = scope

    def _create_ta(name, dtype):

        return tensor_array_ops.TensorArray(dtype=dtype,

                                            size=time_steps,

                                            tensor_array_name=base_name + name)

    output_ta = tuple(_create_ta(""output_%d"" % i,

                                 _infer_state_dtype(dtype, state))

                      for i in range(len(flat_output_size)))

    input_ta = tuple(_create_ta(""input_%d"" % i, flat_input[i].dtype)

                     for i in range(len(flat_input)))

    input_ta = tuple(ta.unstack(input_)

                     for ta, input_ in zip(input_ta, flat_input))

    def _time_step(time, output_ta_t, state, att_scores=None):

        """"""Take a time step of the dynamic RNN.
        Args:
          time: int32 scalar Tensor.
          output_ta_t: List of `TensorArray`s that represent the output.
          state: nested tuple of vector tensors that represent the state.
        Returns:
          The tuple (time + 1, output_ta_t with updated flow, new_state).
        """"""

        input_t = tuple(ta.read(time) for ta in input_ta)

        # Restore some shape information

        for input_, shape in zip(input_t, inputs_got_shape):
            input_.set_shape(shape[1:])

        input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)

        if att_scores is not None:

            att_score = att_scores[:, time, :]

            call_cell = lambda: cell(input_t, state, att_score)

        else:

            call_cell = lambda: cell(input_t, state)

        if sequence_length is not None:

            (output, new_state) = _rnn_step(

                time=time,

                sequence_length=sequence_length,

                min_sequence_length=min_sequence_length,

                max_sequence_length=max_sequence_length,

                zero_output=zero_output,

                state=state,

                call_cell=call_cell,

                state_size=state_size,

                skip_conditionals=True)

        else:

            (output, new_state) = call_cell()

        # Pack state if using state tuples

        output = nest.flatten(output)

        output_ta_t = tuple(

            ta.write(time, out) for ta, out in zip(output_ta_t, output))

        if att_scores is not None:

            return (time + 1, output_ta_t, new_state, att_scores)

        else:

            return (time + 1, output_ta_t, new_state)

    if att_scores is not None:

        _, output_final_ta, final_state, _ = control_flow_ops.while_loop(

            cond=lambda time, *_: time < time_steps,

            body=_time_step,

            loop_vars=(time, output_ta, state, att_scores),

            parallel_iterations=parallel_iterations,

            swap_memory=swap_memory)

    else:

        _, output_final_ta, final_state = control_flow_ops.while_loop(

            cond=lambda time, *_: time < time_steps,

            body=_time_step,

            loop_vars=(time, output_ta, state),

            parallel_iterations=parallel_iterations,

            swap_memory=swap_memory)

    # Unpack final output if not using output tuples.

    final_outputs = tuple(ta.stack() for ta in output_final_ta)

    # Restore some shape information

    for output, output_size in zip(final_outputs, flat_output_size):
        shape = _concat(

            [const_time_steps, const_batch_size], output_size, static=True)

        output.set_shape(shape)

    final_outputs = nest.pack_sequence_as(

        structure=cell.output_size, flat_sequence=final_outputs)

    return (final_outputs, final_state)"
StringSifter,setup.py,"# Copyright (C) 2019 FireEye, Inc. All Rights Reserved.

from setuptools import setup
import os
import re

__all__ = ['metadata', 'setup']

# Get the base directory
here = os.path.dirname(__file__)
if not here:
    here = os.path.curdir

# Text describing the module
long_description = 'stringsifter is a machine learning-based tool ' + \
                   'that automatically ranks the output of the ' + \
                   '`strings` program for binary triage analysis.'

# Get the version
versfile = os.path.join(here, 'stringsifter', 'version.py')
_version = {}
with open(versfile, 'r') as fid:
    exec(fid.read(), _version)

# Do some Pipfile parsing to avoid two copies of the requirements,
# but this is fragile
reqsfile = os.path.join(here, 'Pipfile')
requirements = []
with open(reqsfile, 'r') as fid:
    in_packages_section = False
    for line in fid.readlines():
        if line.startswith('['):
            in_packages_section = line.rstrip() == '[packages]'
            continue
        if in_packages_section:
            m = re.match(r'([\w-]+) *= *""(.*)""', line)
            if m:
                if m.group(2) == '*':
                    requirements.append(m.group(1))
                else:
                    requirements.append(m.group(1) + m.group(2))

# Get the list of scripts
scripts = []

_packages = ['stringsifter', 'stringsifter/lib']

_package_data = {'stringsifter': ['model/*.pkl',
                                  'lib/*.pkl',
                                  'lib/*.ftz',
                                  'lib/*.json']}

# Set the parameters for the setup script
metadata = {
    # Setup instructions
    'provides': ['stringsifter'],
    'packages': _packages,
    'package_data': _package_data,
    'scripts': scripts,
    'entry_points': {
        'console_scripts': ['rank_strings=stringsifter.rank_strings:argmain',
                            'flarestrings=stringsifter.flarestrings:main']
    },
    'install_requires': requirements,
    'python_requires': '>=3.8',
    # Metadata
    'name': 'stringsifter',
    'version': _version['__version__'],
    'description': 'stringsifter is a machine learning-based tool that ' + \
                   'automatically ranks the output of the `strings` ' + \
                   'program for binary triage analysis.',
    'long_description': long_description,
    'url': 'https://github.com/fireeye/stringsifter',
    'download_url': 'https://github.com/fireeye/stringsifter',
    'keywords': ['stringsifter', 'rank', 'strings', 'binary', 'triage'],
    }

# Execute the setup script
if __name__ == '__main__':
    setup(**metadata)
"
StringSifter,version.py,"__version__ = '2.20201202'
"
StringSifter,preprocess.py,"# Copyright (C) 2019 FireEye, Inc. All Rights Reserved.

import re
import os
import json
import math
import numpy
import base64
import joblib
import string
import binascii
import fasttext
import unicodedata
import collections
import sklearn.pipeline
import sklearn.feature_extraction.text
from sklearn.base import BaseEstimator, TransformerMixin


if __package__ is None or __package__ == """":
    from lib import util
    from lib import stats
else:
    from .lib import util
    from .lib import stats

# preload from lib
dirname = os.path.dirname(os.path.abspath(__file__))
with open(os.path.join(dirname, 'lib/constants.json'), 'rb') as fid:
    constants = {k: set(v) for k, v in json.load(fid).items()}

with util.redirect_stderr():
    lid_model = fasttext.load_model(os.path.join(dirname, 'lib/lid.176.ftz'))
markov_model = joblib.load(os.path.join(dirname, 'lib/markov.pkl'))
log_transition_probas = markov_model['transition_matrix']
char_idx_mapper = markov_model['key_to_idx_map']


class Mapper(BaseEstimator, TransformerMixin):
    def __init__(self, func):
        self.func = func

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return numpy.array(list(map(self.func, X))).reshape(-1, 1)

    def get_feature_names(self):
        return '0'


class Featurizer():
    def __init__(self):
        self.b64chars = set(string.ascii_letters + string.digits + '+/_-=')
        dnsroot_cache = list(constants['dnsroot tlds']) + \
                        ['bit', 'dev', 'onion']
        self.tldstr = '|'.join(dnsroot_cache)

        self.mac_only_regex = \
            re.compile(r""""""
                ^
                (?:[A-Fa-f0-9]{2}:){5}
                [A-Fa-f0-9]{2}
                $
                """""", re.VERBOSE)

        fqdn_base = r'(([a-z0-9_-]{1,63}\.){1,10}(%s))' % self.tldstr
        fqdn_str = fqdn_base + r'(?:\W|$)'
        self.fqdn_strict_only_regex = re.compile(r'^' + fqdn_base + r'$', re.I)
        self.fqdn_regex = re.compile(fqdn_str, re.I)
        self.email_valid = re.compile(r'([a-z0-9_\.\-+]{1,256}@%s)' % fqdn_base, re.I)

        _u8 = r'(?:[1-9]?\d|1\d\d|2[0-4]\d|25[0-5])'
        _ipv4pre = r'(?:[^\w.]|^)'
        _ipv4suf = r'(?=(?:[^\w.]|\.(?:\W|$)|$))'
        ip_base = r'((?:%s\.){3}%s)' % (_u8, _u8)
        self.ip_regex = re.compile(_ipv4pre + ip_base + _ipv4suf)

        svc_base = r'(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?):[0-9]{1,5}'
        self.svc_regex = re.compile(svc_base)
        self.md5_only_regex = re.compile(r'^[A-Fa-f0-9]{32}$')
        self.sha1_only_regex = re.compile(r'^[A-Fa-f0-9]{40}$')
        self.sha256_only_regex = re.compile(r'^[A-Fa-f0-9]{64}$')
        self.url_regex = re.compile(r'\w+://[^ \'""\t\n\r\f\v]+')
        self.pkcs_regex = re.compile(r'-----BEGIN ([a-zA-Z0-9 ]+)-----')
        self.format_regex = re.compile(r'%[-|\+|#|0]?([\*|0-9])?(\.[\*|0-9])?[h|l|j|z|t|L]?[diuoxXfFeEgGaAcspn%]')
        self.linefeed_regex = re.compile(r'\\\\n$')
        self.path_regex = re.compile(r'[A-Z|a-z]\:\\\\[A-Za-z0-9]')
        self.pdb_regex = re.compile(r'\w+\.pdb\b')
        self.guid_regex = re.compile(r'[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[1-5][0-9a-fA-F]{3}-[89ab][0-9a-fA-F]{3}-[0-9a-fA-F]{12}')
        self.event_regex = re.compile(r'On(?!User|Board|Media|Global)(?:[A-Z][a-z]+)+')
        self.keylogger_regex = re.compile(r'\[[A-Za-z0-9\_\-\+ ]{2,13}\]')
        self.oid_regex = re.compile(r'((0\.([0-5]|9))|(1\.[0-3])|(2\.(([0-2][0-8])|(4[0-2])|(4[8-9])|(5[0-2])|(999))))(\.[0-9])+')
        self.ext_regex = re.compile(r'\w+\.[a-z]{3,4}\b')
        self.prod_id_regex = re.compile(r'[0-9]{5}-[0-9A-Z]{3}-[0-9]{7}-[0-9]{5}')
        self.priv_regex = re.compile(r'Se[A-Z][A-z]+Privilege')
        self.sddl_regex = re.compile(r'[DSO]:.+;;;.+$')
        self.sid_regex = re.compile(r'S-(?:[0-5]|9|(11)|(12)|(16))-')
        self.whitespace_regex = re.compile(r'\s+')
        self.letters_regex = re.compile(r'[^A-Za-z]')
        self.english_ignores = constants['windows api'].union(constants['pma important functions']).union\
                               (constants['dates']).union(constants['languages'])
        self.not_latin_unicode_names = ['ARABIC', 'SYRIAC', 'CYRILLIC', 'CJK', 'GEORGIAN']
        self.uppercase_var_name = re.compile(r'(?:\(| |^)[A-Z]+(?:\_[A-Z]+)+(?:\)| |$)')
        self.period_delimited_var_name = re.compile(r'(?:\(| |^)[a-z]{2,}(?:\.[a-z]{2,})+(?:\)| |$)')
        self.oss_substr_regex = re.compile(
            r'^(?:NT(?:3\.1|3\.5|3\.51))|' +
            r'(?:Ultimate(?: N| Edition|\_Edition))|' +
            r'(?:Business(?: N| Edition|\_Edition))|' +
            r'(?:Professional(?: Edition| x64 Edition))|' +
            r'(?:Microsoft Windows (?:ME|95|98|2000|XP))|' +
            r'(?:Storage(?: Server 2003 R2| Server 2003))|' +
            r'(?:Server(?: 2008| 2003| 2003 R2|2008|2008R2))|' +
            r'(?:Windows\+(?:2000|Home Server|Vista|Server\+2003|7|8|XP|8\.1))|' +
            r'(?:WIN(?:32\_NT|\_2008R2|\_7|\_2008|\_VISTA|\_2003|\_XPe|\_XP|\_2000))|' +
            r'(?:(?:Small\_Business\_|Small Business |Advanced )Server)|(?:Windows Storage Server 2003)|' +
            r'(?:Windows (?:7 \(6\.1\)|2000|Me|98|95|NT|Vista|7|8|10|XP|8\.1|Server|Home Server))|' +
            r'(?:Windows Server (?:2012 R2|2003|2003 R2|2008|2008 R2|R2|2000|2012|2003 R2 \(5\.2\)|2008 \(6\.0\)|2008 R2 \(6\.1\)))|' +
            r'(?:Standard(?:\_Edition|\_Edition\_core\_installation| Edition| Edition\(Core\)| x64 Edition| Edition \(core installation\)))|' +
            r'(?:Win(?:8|7|Server2003R2|Server2003|2K|XP64|XP| XP| 2000|HomeServer|NT|Server2012|Server2008R2|Server2008| Vista| Srv 2008| 7| 8| Srv 2003| Srv| ))|' +
            r'(?:Datacenter(?: Edition\(Core\)| Server| Edition for Itanium\-based Systems| x64 Edition| Edition| Edition \(core installation\)|\_Edition\_core\_installation|\_Edition))|' +
            r'(?:Home(?: Premium N| Premium| Basic N| Basic| Edition| Basic Edition| Premium Edition|\_Premium\_Edition|\_Basic\_Edition|\_Server|\-Premium\-Edition|\-Basic\-Edition|\_Edition))|' +
            r'(?:Enterprise(?:\_Edition|\_Edition\_for\_ItaniumBased\_System|\_Edition\_core\_installation| N| Edition| Edition\(Core\)| x64 Edition| Edition \(core installation\)| Edition for Itanium\-based Systems))|' +
            r'(?:(?:Small\_Business\_Server\_Premium\_|Small Business Server Premium |Small\_Business\_Server\_Premium\_Edition|Web\_Server\_|Cluster Server |Starter |Starter\_|Cluster\_Server\_|32\-bit |64\-bit |Embedded |Tablet PC |Media Center |Web |Compute Cluster |Web Server )Edition)$')
        self.oss_exact_regex = re.compile(r'^(?:2008|2003|2000|Business|Ultimate|Vista|Seven|Professional)$')
        self.user_agents_regex = re.compile(r'[\w\-]+\/[\w\-]+\.[\w\-]+(?:\.[\w\-])* ?(?:\[[a-z]{2}\] )?\((?:.+[:;\-].+|[+ ]?http://.+)\)')
        self.hive_regex = re.compile(r'[^0-9a-zA-Z](?:hkcu|hklm|hkey\_current\_user|hkey\_local\_machine)[^0-9a-zA-Z]')
        self.namespace_regex = re.compile(r'\\\\\.\\.*')
        self.msword_regex = re.compile(r'Word\.Document')
        self.mozilla_api_regex = re.compile(r'PR\_(?:[A-Z][a-z]{2,})+')
        self.privilege_constant_regex = re.compile(r'SE\_(?:[A-Z]+\_)+NAME')
        self.upx_regex = re.compile(r'\b(?:[a-z]?upx|[A-Z]?UPX)(?:\d|\b)')
        self.crypto_common_regex = re.compile(r'\b(?:rsa|aes|rc4|salt|md5)\b')
        self.features = [
            'string_length',
            'has_english_text',
            'entropy_rate',
            'english_letter_freq_div',
            'average_scrabble_score',
            'whitespace_percentage',
            'alpha_percentage',
            'digit_percentage',
            'punctuation_percentage',
            'vowel_consenant_ratio',
            'capital_letter_ratio',
            'title_words_ratio',
            'average_word_length',
            'has_ip',
            'has_ip_srv',
            'has_url',
            'has_email',
            'has_fqdn',
            'has_namespace',
            'has_msword_version',
            'has_packer',
            'has_crypto_related',
            'is_blacklisted',
            'has_privilege_constant',
            'has_mozilla_api',
            'is_strict_fqdn',
            'has_hive_name',
            'is_mac',
            'has_extension',
            'is_md5',
            'is_sha1',
            'is_sha256',
            'is_irrelevant_windows_api',
            'has_guid',
            'is_antivirus',
            'is_whitelisted',
            'is_common_dll',
            'is_boost_lib',
            'is_delphi_lib',
            'has_event',
            'is_registry',
            'has_malware_identifier',
            'has_sid',
            'has_keylogger',
            'has_oid',
            'has_product_id',
            'is_oss',
            'is_user_agent',
            'has_sddl',
            'has_protocol',
            'is_protocol_method',
            'is_base64',
            'is_hex_not_numeric_not_alpha',
            'has_format_specifier',
            'ends_with_line_feed',
            'has_path',
            'has_pdb',
            'has_privilege',
            'is_known_xml',
            'is_cpp_runtime',
            'is_library',
            'is_date',
            'is_pe_artifact',
            'has_public_key',
            'markov_junk',
            'is_x86',
            'is_common_path',
            'is_code_page',
            'is_language',
            'is_region_tag',
            'has_not_latin',
            'is_known_folder',
            'is_malware_api',
            'is_environment_variable',
            'has_variable_name',
            'has_padding_string'
        ]

    def _substring_match_bool(self, string_i, corpus):
        return int(any([(s in string_i) for s in corpus]))

    def _exact_match_bool(self, string_i, corpus):
        return int(string_i in corpus)

    def string_length(self, string_i):
        return len(string_i)

    def has_english_text(self, string_i, thresh_upper=0.9):
        string_i_replace_newlines = ' '.join(string_i.split('\n'))
        fasttext_prediction = lid_model.predict(string_i_replace_newlines)
        english_prediction = '__label__en' in fasttext_prediction[0]
        confident_prediction = fasttext_prediction[1] > thresh_upper
        num_punctuation = [string_i.count(punc) for punc in string.punctuation]
        contains_no_punctuation = sum(num_punctuation) == 0
        contains_no_path = not self.has_path(string_i)
        contains_no_ext = not self.has_extension(string_i)
        contains_no_fmtSpec = not self.has_format_specifier(string_i)

        is_not_ignored = string_i not in self.english_ignores

        if english_prediction and confident_prediction and contains_no_path \
           and contains_no_ext and contains_no_fmtSpec and is_not_ignored:
            return 1
        else:
            return 0

    def entropy_rate(self, string_i, base=2,
                     thresh_upper=3.65, thresh_lower=1.45):
        entropy_rate = 0
        characters = list(string_i)

        if len(characters) <= 1:
            return 1

        _, letters = numpy.unique(characters, return_counts=True)
        probabilities = letters / len(characters)

        if numpy.count_nonzero(probabilities) <= 1:
            return 1

        for i in probabilities:
            entropy_rate -= i * math.log(i, base)

        below_thresh_lower = entropy_rate <= thresh_lower
        above_thresh_upper = entropy_rate >= thresh_upper
        if below_thresh_lower or above_thresh_upper:
            return 1
        else:
            return 0

    def english_letter_freq_div(self, string_i, thresh_upper=3.0):
        """"""
        estimated KL divergence from english letter distribution
        (case insensitive). Non-alpha bytes are ignored
         low KL divergence <=> letter freqs similar to English
        """"""
        counts = collections.Counter([c for c in string_i.lower() if
                                      c in string.ascii_lowercase])
        n = sum(counts.values())
        kl = 0.0
        for lett, ct in counts.items():
            p = ct / n
            q = stats.english_letter_probs[lett]
            kl += p * math.log2(p / q)
        return 1 if int(kl <= thresh_upper) else -1

    def average_scrabble_score(self, string_i, thresh_lower=1.,
                               thresh_upper=3.51):
        lowered_letters = [char for char in string_i.lower() if char.isalpha()]
        if len(lowered_letters) > 0:
            raw_scrabble_score = sum(
                [stats.scrabble_dict.get(char, 0) for char in lowered_letters])
            has_low_score = (raw_scrabble_score / len(lowered_letters) <=
                             thresh_lower)
            has_high_score = (raw_scrabble_score / len(lowered_letters) >=
                              thresh_upper)
            has_extension = self.has_extension(string_i)
            has_path = self.has_path(string_i)
            has_format_specifier = self.has_format_specifier(string_i)
            has_low_score_substr = self._substring_match_bool(
                string_i.lower(),
                constants[""low_scrabble_score_strings""])
            has_relevant_noise = (has_extension or has_path or
                                  has_format_specifier or has_low_score_substr)
            if not has_relevant_noise and (has_low_score or
                                           has_high_score):
                return -1
            else:
                return 0
        else:
            return 0

    def whitespace_percentage(self, string_i):
        if len(string_i) > 0:
            whitespace_removed = re.sub(self.whitespace_regex, '', string_i)
            return (len(string_i) - len(whitespace_removed)) / len(string_i)
        else:
            return 0

    def alpha_percentage(self, string_i):
        whitespace_removed = re.sub(self.whitespace_regex, '', string_i)
        if len(whitespace_removed) > 0:
            num_alpha = len([char_i for char_i in whitespace_removed
                            if char_i.isalpha()])
            return num_alpha / len(whitespace_removed)
        else:
            return 0

    def digit_percentage(self, string_i):
        whitespace_removed = re.sub(self.whitespace_regex, '', string_i)
        if len(whitespace_removed) > 0:
            num_digits = len([char_i for char_i in whitespace_removed
                             if char_i.isdigit()])
            return num_digits / len(whitespace_removed)
        else:
            return 0

    def punctuation_percentage(self, string_i):
        whitespace_removed = re.sub(self.whitespace_regex, '', string_i)
        if len(whitespace_removed) > 0:
            num_punctuation = sum(whitespace_removed.count(punc) for
                                  punc in string.punctuation)
            return num_punctuation / len(whitespace_removed)
        else:
            return 0

    def vowel_consenant_ratio(self, string_i):
        only_letters = re.sub(self.letters_regex, '', string_i).lower()
        if len(only_letters) > 0:
            vowels = set(constants['vowel list'])
            num_vowels = sum(only_letters.count(vowel) for vowel in vowels)
            return num_vowels / len(only_letters)
        else:
            return 0

    def capital_letter_ratio(self, string_i):
        only_letters = re.sub(self.letters_regex, '', string_i)
        if len(only_letters) > 0:
            num_capital_letters = sum(1 for letter in only_letters if
                                      letter.isupper())
            return num_capital_letters / len(only_letters)
        else:
            return 0

    def title_words_ratio(self, string_i):
        words = string_i.split()
        if len(words) > 0:
            title_words = [word for word in words if word.istitle()]
            return len(title_words) / len(words)
        else:
            return 0

    def average_word_length(self, string_i):
        words = string_i.split()
        word_lengths = [len(word) for word in words]
        if len(word_lengths) > 0:
            return sum(word_lengths) / len(word_lengths)
        else:
            return 0

    def has_ip_srv(self, string_i):
        has_ip_address = 1 if self.ip_regex.search(string_i) else 0
        exceptions = self._substring_match_bool(
            string_i.lower(), constants['ip exceptions'])
        return int(has_ip_address and not exceptions)

    def is_base64(self, string_i):
        # known FPs
        pre_list = ['Create', 'Array', 'GetSystem', 'Windows', 'Direct']
        if any([string_i.startswith(pre) for pre in pre_list]):
            return 0

        # base64 character set
        if set(string_i) - self.b64chars:
            return 0

        # length is multiple of 4
        if len(string_i) % 4 != 0:
            return 0

        try:
            # note: base64 decoder may return without
            # error without decoding the full string
            # -> check decoded length before declaring success
            decoded = base64.b64decode(string_i)
            declen = len(decoded)
            if declen < 0.75 * len(string_i.rstrip('=')) - 2:
                return 0
        except (UnicodeDecodeError, binascii.Error, ValueError):
            return 0

        # require one item from each character class,
        # with alphabetic > F (to avoid detecting hex strings)
        groups = [
            string.ascii_uppercase[6:],
            string.ascii_lowercase[6:],
            string.digits
        ]
        if not all([any([c for c in string_i if c in grp]) for grp in groups]):
            return 0

        # padding test
        if string_i.endswith('=') and '=' not in string_i.rstrip('='):
            return 1

        if len(string_i) <= 20:
            # be picky with short strings without padding;
            #  otherwise we get lots of false positives
            if '+' in string_i:
                return 1
            return 0

        if len(string_i.rstrip(string_i[-1])) < 4:
            # filter out 'AAAAAAAAAAAA' strings and friends
            return 0

        if string.ascii_uppercase in string_i:
            # base64 alphabet
            return 0

        return 1

    def is_hex_not_numeric_not_alpha(self, string_i):
        is_hex = all(c in string.hexdigits for c in string_i)
        is_not_numeric_not_alpha = not (string_i.isalpha() or
                                        string_i.isdigit())
        return int(is_hex and is_not_numeric_not_alpha)

    def is_strict_fqdn(self, string_i):
        return 1 if self.fqdn_strict_only_regex.match(string_i) else 0

    def has_email(self, string_i):
        return 1 if self.email_valid.match(string_i) else 0

    def is_md5(self, string_i):
        return 1 if self.md5_only_regex.match(string_i) else 0

    def is_sha1(self, string_i):
        return 1 if self.sha1_only_regex.match(string_i) else 0

    def is_sha256(self, string_i):
        return 1 if self.sha256_only_regex.match(string_i) else 0

    def is_mac(self, string_i):
        return 1 if self.mac_only_regex.match(string_i) else 0

    def has_keylogger(self, string_i):
        return 1 if self.keylogger_regex.match(string_i) else 0

    def has_oid(self, string_i):
        return 1 if self.oid_regex.match(string_i) else 0

    def has_privilege(self, string_i):
        return 1 if self.priv_regex.match(string_i) else 0

    def has_sddl(self, string_i):
        return 1 if self.sddl_regex.match(string_i) else 0

    def has_mozilla_api(self, string_i):
        return 1 if self.mozilla_api_regex.match(string_i) else 0

    def is_oss(self, string_i):
        is_oss_exact = 1 if self.oss_exact_regex.match(string_i) else 0
        is_oss_substr = 1 if self.oss_substr_regex.search(string_i) else 0
        return is_oss_exact or is_oss_substr

    def has_packer(self, string_i):
        has_upx_packer = 1 if self.upx_regex.search(string_i) else 0
        has_other_packer = 1 if self._substring_match_bool(
            string_i.lower(), constants['packers']) else 0
        return has_upx_packer or has_other_packer

    def has_crypto_related(self, string_i):
        has_crypto_common = \
            1 if self.crypto_common_regex.search(string_i) else 0
        has_crypto_uncommon = 1 if self._substring_match_bool(
            string_i.lower(), constants['crypto uncommon']) else 0
        return has_crypto_common or has_crypto_uncommon

    def is_blacklisted(self, string_i):
        is_exact_blacklist = 1 if self._exact_match_bool(
            string_i, constants['blacklist fullmatch']) else 0
        is_substring_blacklist = 1 if self._substring_match_bool(
            string_i, constants['blacklist substring']) else 0
        is_substring_lower_blacklist = 1 if self._substring_match_bool(
            string_i.lower(), constants['blacklist substring lower']) else 0

        is_windows_api = self._exact_match_bool(
            string_i, constants['windows api'])
        is_pma_api = self._exact_match_bool(
            string_i, constants['pma important functions'])
        is_not_api_blacklist = not (is_windows_api or is_pma_api) and \
            is_substring_lower_blacklist

        return is_exact_blacklist or is_substring_blacklist or \
            is_not_api_blacklist

    def has_namespace(self, string_i):
        return 1 if self.namespace_regex.search(string_i) else 0

    def has_msword_version(self, string_i):
        return 1 if self.msword_regex.search(string_i) else 0

    def has_privilege_constant(self, string_i):
        return 1 if self.privilege_constant_regex.search(string_i) else 0

    def has_fqdn(self, string_i):
        return 1 if self.fqdn_regex.search(string_i) else 0

    def has_product_id(self, string_i):
        return 1 if self.prod_id_regex.search(string_i) else 0

    def has_ip(self, string_i):
        return 1 if self.svc_regex.search(string_i) else 0

    def has_sid(self, string_i):
        return 1 if self.sid_regex.search(string_i) else 0

    def has_url(self, string_i):
        return 1 if self.url_regex.search(string_i) else 0

    def ends_with_line_feed(self, string_i):
        return 1 if self.linefeed_regex.search(string_i) else 0

    def has_path(self, string_i):
        return 1 if self.path_regex.search(string_i) else 0

    def has_event(self, string_i):
        return 1 if self.event_regex.search(string_i) else 0

    def has_guid(self, string_i):
        return 1 if self.guid_regex.search(string_i) else 0

    def has_public_key(self, string_i):
        return 1 if self.pkcs_regex.search(string_i) else 0

    def has_pdb(self, string_i):
        return 1 if self.pdb_regex.search(string_i) else 0

    def is_user_agent(self, string_i):
        return 1 if self.user_agents_regex.search(string_i) else 0

    def has_hive_name(self, string_i):
        return 1 if self.hive_regex.search(string_i) else 0

    def has_variable_name(self, string_i):
        has_uppercase_var_name = self.uppercase_var_name.search(string_i)
        has_period_delimited_var_name = \
            self.period_delimited_var_name.search(string_i)
        has_no_extension = not self.has_extension(string_i)
        return 1 if (has_uppercase_var_name or
                     (has_period_delimited_var_name and
                      has_no_extension)) else 0

    def has_format_specifier(self, string_i):
        if len(string_i) < 5:
            return 0
        return 1 if self.format_regex.search(string_i) else 0

    def has_extension(self, string_i):
        is_not_common_dll = not self.is_common_dll(string_i)
        return 1 if (is_not_common_dll and
                     self.ext_regex.search(string_i)) else 0

    def has_padding_string(self, string_i):
        return self._substring_match_bool(string_i,
                                          ['PADDING'])

    def has_malware_identifier(self, string_i):
        return self._substring_match_bool(string_i.lower(),
                                          constants['malware identifiers'])

    def is_registry(self, string_i):
        return self._substring_match_bool(string_i,
                                          constants['regs'])

    def is_antivirus(self, string_i):
        return self._substring_match_bool(string_i.lower(),
                                          constants['avs'])

    def is_whitelisted(self, string_i):
        return self._substring_match_bool(string_i,
                                          constants['white'])

    def has_protocol(self, string_i):
        return self._substring_match_bool(string_i.upper(),
                                          constants['protocols'])

    def is_protocol_method(self, string_i):
        return self._substring_match_bool(string_i,
                                          constants['protocol methods'])

    def is_common_path(self, string_i):
        return self._substring_match_bool(string_i.lower(),
                                          constants['paths'])

    def is_common_dll(self, string_i):
        has_common_dll = self._exact_match_bool(
            string_i.split('.')[0].lower(), constants['common dlls'])
        has_malware_dll = self._exact_match_bool(
            string_i.split('.')[0].lower(), constants['malware dlls'])
        return has_common_dll and not has_malware_dll

    def is_boost_lib(self, string_i):
        return self._exact_match_bool(string_i, constants['lib boost'])

    def is_delphi_lib(self, string_i):
        return self._exact_match_bool(string_i, constants['lib delphi'])

    def is_irrelevant_windows_api(self, string_i):
        return self._exact_match_bool(string_i, constants['windows api'])

    def is_cpp_runtime(self, string_i):
        return self._exact_match_bool(string_i, constants['cpp'])

    def is_library(self, string_i):
        return self._exact_match_bool(string_i, constants['lib'])

    def is_date(self, string_i):
        return self._exact_match_bool(string_i, constants['dates'])

    def is_known_xml(self, string_i):
        return self._exact_match_bool(string_i, constants['known xml'])

    def is_pe_artifact(self, string_i):
        return self._exact_match_bool(string_i, constants['pe artifacts'])

    def is_language(self, string_i):
        return self._exact_match_bool(string_i, constants['languages'])

    def is_code_page(self, string_i):
        return self._exact_match_bool(string_i, constants['code pages'])

    def is_region_tag(self, string_i):
        return self._exact_match_bool(string_i, constants['region tags'])

    def is_known_folder(self, string_i):
        return self._exact_match_bool(
            string_i, constants['known folders'])

    def is_malware_api(self, string_i):
        return self._exact_match_bool(
            string_i, constants['pma important functions'])

    def is_environment_variable(self, string_i):
        if len(string_i) > 0:
            return int(string_i[0] == '%' and string_i[-1] == '%')
        else:
            return 0

    def is_x86(self, string_i):
        if len(string_i) <= 5:
            if len(set(list(string_i))) == 1:
                return 1
            if len(string_i) >= 2 and string_i[1] == '$':
                return 1
        return 0

    def has_not_latin(self, string_i):
        try:
            unicode_names = [unicodedata.name(char) for char in string_i]
            for unicode_name in unicode_names:
                if self._substring_match_bool(unicode_name,
                                              self.not_latin_unicode_names):
                    return 1
            return 0
        except ValueError:
            return 0

    def markov_junk(self, string_i, thresh_lower=0.004):
        log_prob = 0.0
        transition_count = 0
        for char_i, char_j in self._two_gram(string_i.lower()):
            char_i_idx = char_idx_mapper.get(char_i, char_idx_mapper['unk'])
            char_j_idx = char_idx_mapper.get(char_j, char_idx_mapper['unk'])
            log_prob += log_transition_probas[char_i_idx][char_j_idx]
            transition_count += 1
        if transition_count >= 1:
            below_markov_threshold = \
                math.exp(log_prob / transition_count) <= thresh_lower
        else:
            below_markov_threshold = math.exp(log_prob) <= thresh_lower
        has_no_format_specifier = not self.has_format_specifier(string_i)
        return below_markov_threshold and has_no_format_specifier

    def _two_gram(self, string_i):
        for start in range(0, len(string_i) - 2 + 1):
            yield ''.join(string_i[start:start + 2])
"
StringSifter,flarestrings.py,"# Copyright (C) 2019 FireEye, Inc. All Rights Reserved.

import re
import sys
import argparse

if __package__ is None or __package__ == """":
    from version import __version__
else:
    from .version import __version__

ASCII_BYTE = b"" !\""#\$%&\'\(\)\*\+,-\./0123456789:;<=>\?@ABCDEFGHIJKLMNOPQRSTUVWXYZ\[\]\^_`abcdefghijklmnopqrstuvwxyz\{\|\}\\\~\t""


def main():
    parser = argparse.ArgumentParser()
    # to read binary data from stdin use sys.stdin.buffer.
    #   sys.stdin is in 'r' mode, not 'rb'
    parser.add_argument('files', nargs='*', type=argparse.FileType('rb'),
                        default=[sys.stdin.buffer], help='files to process, or pipe to stdin')
    parser.add_argument('--version', action='version', version=__version__)
    parser.add_argument('-n', '--min-len', type=int, default=4,
                        help='Print sequences of characters that are at least ' +
                             'min-len characters long, instead of the default 4.')
    args = parser.parse_args()

    # regular expressions from flare-floss:
    #  https://github.com/fireeye/flare-floss/blob/master/floss/strings.py#L7-L9
    re_narrow = re.compile(b'([%s]{%d,})' % (ASCII_BYTE, args.min_len))
    re_wide = re.compile(b'((?:[%s]\x00){%d,})' % (ASCII_BYTE, args.min_len))

    for f in args.files:
        b = f.read()
        for match in re_narrow.finditer(b):
            print(match.group().decode('ascii'))
        for match in re_wide.finditer(b):
            try:
                print(match.group().decode('utf-16'))
            except UnicodeDecodeError:
                pass


if __name__ == '__main__':
    main()
"
StringSifter,rank_strings.py,"# Copyright (C) 2019 FireEye, Inc. All Rights Reserved.

import os
import sys
import numpy
import joblib
import argparse


if __package__ is None or __package__ == """":
    from lib import util
    from version import __version__
else:
    from .lib import util
    from .version import __version__


def is_valid_dir(parser, arg):
    arg = os.path.abspath(arg)
    if not os.path.exists(arg):
        parser.error(""The directory %s does not exist!"" % arg)
    else:
        return arg


def main(input_strings, cutoff, cutoff_score, scores, batch):
    modeldir = os.path.join(util.package_base(), ""model"")
    featurizer = joblib.load(os.path.join(modeldir, ""featurizer.pkl""))
    ranker = joblib.load(os.path.join(modeldir, ""ranker.pkl""))

    if not batch:
        strings = numpy.array([line.strip() for line in
                               input_strings.readlines()], dtype=object)

        if len(strings) < 1:
            raise ValueError(""No strings found within input."")

        X_test = featurizer.transform(strings)
        y_scores = ranker.predict(X_test)

        if not numpy.isnan(cutoff_score):
            above_cutoff_indices = numpy.where(y_scores >= cutoff_score)
            y_scores = y_scores[above_cutoff_indices]
            strings = strings[above_cutoff_indices]

        argsorted_y_scores = numpy.argsort(y_scores)[::-1]
        sorted_strings = strings[argsorted_y_scores]
        cutoff_sorted_strings = sorted_strings.tolist()[:cutoff]

        if scores:
            sorted_y_scores = y_scores[argsorted_y_scores]
            print(""\n"".join([""%.2f,%s"" % pair for pair in
                             zip(sorted_y_scores, cutoff_sorted_strings)]))
        else:
            print(""\n"".join(cutoff_sorted_strings))
    else:
        strings = []
        qids = []
        batch_files = os.listdir(batch)

        for batch_input_file in batch_files:
            with open(os.path.join(batch, batch_input_file)) as batch_input_fp:
                string_i = [line.strip() for line in
                            batch_input_fp.readlines()]
            strings.extend(string_i)
            qids.append(len(string_i))

        if len(strings) < 1:
            raise ValueError(""No strings found in batch directory."")

        X_test = featurizer.transform(strings)
        y_scores = ranker.predict(X_test)

        strings_grouped = numpy.split(strings,
                                      numpy.cumsum(qids))[:-1]
        y_scores_grouped = numpy.split(y_scores, numpy.cumsum(qids))[:-1]

        batch_file_suffix = "".ranked_strings""
        for batch_file, strings_i, y_scores_i in zip(batch_files,
                                                     strings_grouped,
                                                     y_scores_grouped):
            with open(os.path.join(batch, batch_file + batch_file_suffix),
                      ""w"") as batch_output_fp:

                if not numpy.isnan(cutoff_score):
                    above_cutoff_indices_i = numpy.where(
                        y_scores_i >= cutoff_score)
                    y_scores_i = y_scores_i[above_cutoff_indices_i]
                    strings_i = strings_i[above_cutoff_indices_i]

                argsorted_y_scores_i = numpy.argsort(y_scores_i)[::-1]
                sorted_strings_i = strings_i[argsorted_y_scores_i]
                cutoff_sorted_strings_i = sorted_strings_i.tolist()[:cutoff]
                cutoff_sorted_strings_newlines_i = map(lambda s: s + ""\n"",
                                                       cutoff_sorted_strings_i)
                if scores:
                    sorted_y_scores_i = y_scores_i[argsorted_y_scores_i]
                    scores_strings_i = zip(sorted_y_scores_i,
                                           cutoff_sorted_strings_newlines_i)
                    scores_strings_combined_i = [""%.2f,%s"" % (score_i, string_i)
                                                 for score_i, string_i in
                                                 scores_strings_i]
                    batch_output_fp.writelines(scores_strings_combined_i)
                else:
                    batch_output_fp.writelines(
                        cutoff_sorted_strings_newlines_i)


# entry point for script
def argmain():
    parser = argparse.ArgumentParser(
        description=""StringSifter ranks strings based on their \
                     relevance for malware analysis."",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument(
        ""input_strings"", nargs=""?"", type=argparse.FileType(""r""),
        default=sys.stdin, help=""Read input strings from stdin"")
    parser.add_argument('--version', action='version', version=__version__)
    parser.add_argument(
        '--scores', '-s', action='store_true',
        help=""display rank scores within output  \
              (default: scores not displayed)"")
    parser.add_argument(
        '--batch', '-b', type=lambda adir: is_valid_dir(parser, adir),
        help=""enable batch mode, where dir contains files  \
              containing Strings outputs to be ranked by  \
              StringSifter. This creates new files in dir \
              with StringSifter results denoted with the \
              .ranked_strings extention"")

    group = parser.add_mutually_exclusive_group()
    group.add_argument(
        '--limit', '-l', type=int, default=None,
        help=""limit output to the top `limit` ranked strings (default: no limit)"")
    group.add_argument(
        '--min-score', '-m', type=float, default=numpy.nan,
        help=""limit output to strings with score >= `min-score` (default: no min score)"")
    args = parser.parse_args()

    main(args.input_strings, args.limit, args.min_score,
         args.scores, args.batch)


if __name__ == '__main__':
    argmain()
"
StringSifter,util.py,"# Copyright (C) 2019 FireEye, Inc. All Rights Reserved.

import io
import os
import sys
import contextlib


def package_base():
    """"""
    return package base folder (one level up from here)
    """"""
    pth = os.path.join(os.path.dirname(__file__), '..')
    return os.path.abspath(pth)


@contextlib.contextmanager
def redirect_stderr():
    _stderr = sys.stderr
    sys.stderr = io.StringIO()
    yield
    sys.stderr = _stderr
"
StringSifter,__init__.py,
StringSifter,stats.py,"# Copyright (C) 2019 FireEye, Inc. All Rights Reserved.

""""""
english letter probabilities

table from http://en.algoritmy.net/article/40379/Letter-frequency-English
""""""

english_letter_probs_percent = [
    ['a', 8.167],
    ['b', 1.492],
    ['c', 2.782],
    ['d', 4.253],
    ['e', 12.702],
    ['f', 2.228],
    ['g', 2.015],
    ['h', 6.094],
    ['i', 6.966],
    ['j', 0.153],
    ['k', 0.772],
    ['l', 4.025],
    ['m', 2.406],
    ['n', 6.749],
    ['o', 7.507],
    ['p', 1.929],
    ['q', 0.095],
    ['r', 5.987],
    ['s', 6.327],
    ['t', 9.056],
    ['u', 2.758],
    ['v', 0.978],
    ['w', 2.360],
    ['x', 0.150],
    ['y', 1.974],
    ['z', 0.074]]

english_letter_probs = {lt: (per * 0.01) for lt, per in english_letter_probs_percent}


""""""
Scrabble Scores
table from https://en.wikipedia.org/wiki/Scrabble_letter_distributions
""""""
scrabble_dict = {""a"": 1, ""b"": 3, ""c"": 3,  ""d"": 2, ""e"": 1,  ""f"": 4,
                 ""g"": 2, ""h"": 4, ""i"": 1,  ""j"": 8, ""k"": 5,  ""l"": 1,
                 ""m"": 3, ""n"": 1, ""o"": 1,  ""p"": 3, ""q"": 10, ""r"": 1,
                 ""s"": 1, ""t"": 1, ""u"": 1,  ""v"": 4, ""w"": 4,  ""x"": 8,
                 ""y"": 4, ""z"": 10}
"
StringSifter,conftest.py,"# Copyright (C) 2019 FireEye, Inc. All Rights Reserved.

import pytest
import stringsifter.preprocess as preprocess

@pytest.fixture(scope='module')
def featurizer():
    f = preprocess.Featurizer()
    yield f
"
StringSifter,test_stringsifter.py,"# Copyright (C) 2019 FireEye, Inc. All Rights Reserved.

import os
import numpy
from io import StringIO
import stringsifter.rank_strings as rank_strings

test_strings = 'testing text\n' \
                'nagain\n' \
                'wheredoesitgo\n' \
                'testing text\n' \
                'nagain\n' \
                'wheredoesitgo\n' \
                'testing text\n' \
                'nagain\n' \
                'wheredoesitgo\n' \
                'testing text\n'


def _get_rank_strings_stdoutput(capsys, kwargs):
    rank_strings.main(**kwargs)
    stdout = capsys.readouterr().out
    return stdout.split('\n')[:-1]


def _get_kwargs(input_strings=test_strings, cutoff=None,
                cutoff_score=numpy.nan, scores=False, batch=False):
    return {'input_strings': StringIO(input_strings),
            'cutoff': cutoff,
            'cutoff_score': cutoff_score,
            'scores': scores,
            'batch': batch}


def test_string_length(featurizer):
    test_set = [['', 0],
                ['foo', 3],
                ['everybody', 9]]
    for s, true_len in test_set:
        feat_len = featurizer.string_length(s)
        assert feat_len == true_len


def test_default(capsys):
    """"""
    test default processing flow: # strings in == # strings out
    """"""
    output_lines = _get_rank_strings_stdoutput(capsys, _get_kwargs())
    assert len(output_lines) == 10


def test_scores(capsys):
    scores_value = True
    output_lines = _get_rank_strings_stdoutput(
        capsys, _get_kwargs(scores=scores_value))
    split_output_lines = [output_line.split("","") for output_line
                          in output_lines]
    previous_score = numpy.inf
    for output_score, output_string in split_output_lines:
        assert(type(output_string) is str)
        float_output_score = float(output_score)
        assert(type(float_output_score) is float)
        assert(previous_score >= float_output_score)
        previous_score = float_output_score


def test_cutoff(capsys):
    cutoff_value = 5
    output_lines = _get_rank_strings_stdoutput(
        capsys, _get_kwargs(cutoff=cutoff_value))
    assert len(output_lines) == cutoff_value


def test_cutoff_score(capsys):
    scores_value = True
    cutoff_score_value = 0.0
    output_lines = _get_rank_strings_stdoutput(
        capsys, _get_kwargs(scores=scores_value,
                            cutoff_score=cutoff_score_value))
    split_output_lines = [output_line.split("","") for output_line
                          in output_lines]
    for output_score, output_string in split_output_lines:
        assert float(output_score) >= cutoff_score_value


def test_batch():
    batch_value = 'tests/fixtures/'
    batch_files = [batch_value + batch_file for batch_file in
                   os.listdir(batch_value)]
    output_lines = rank_strings.main(
        **_get_kwargs(batch=batch_value))
    for batch_file in batch_files:
        ranking_file = batch_file + '.ranked_strings'
        assert os.path.isfile(ranking_file) is True
        os.remove(ranking_file)
"
Text Analysis of the Mexican Government Report,step2.py,"""""""
This script extracts features from the transcript txt file and saves them to .csv files
so they can be used in any toolkkit.
""""""

import csv
import spacy


def main():
    """"""Loads the model and processes it.
    
    The model used can be installed by running this command on your CMD/Terminal:

    python -m spacy download es_core_news_md
    
    """"""

    corpus = open(""transcript_clean.txt"", ""r"", encoding=""utf-8"").read()
    nlp = spacy.load(""es_core_news_md"")

    # Our corpus is bigger than the default limit, we will set
    # a new limit equal to its length.
    nlp.max_length = len(corpus)

    doc = nlp(corpus)

    get_tokens(doc)
    get_entities(doc)
    get_sentences(doc)


def get_tokens(doc):
    """"""Get the tokens and save them to .csv

    Parameters
    ----------
    doc : spacy.doc
        A doc object.

    """"""

    data_list = [[""text"", ""text_lower"", ""lemma"", ""lemma_lower"",
                  ""part_of_speech"", ""is_alphabet"", ""is_stopword""]]

    for token in doc:
        data_list.append([
            token.text, token.lower_, token.lemma_, token.lemma_.lower(),
            token.pos_, token.is_alpha, token.is_stop
        ])

    with open(""./tokens.csv"", ""w"", encoding=""utf-8"", newline="""") as tokens_file:
        csv.writer(tokens_file).writerows(data_list)


def get_entities(doc):
    """"""Get the entities and save them to .csv

    Parameters
    ----------
    doc : spacy.doc
        A doc object.

    """"""

    data_list = [[""text"", ""text_lower"", ""label""]]

    for ent in doc.ents:
        data_list.append([ent.text, ent.lower_, ent.label_])

    with open(""./entities.csv"", ""w"", encoding=""utf-8"", newline="""") as entities_file:
        csv.writer(entities_file).writerows(data_list)


def get_sentences(doc):
    """"""Get the sentences, score and save them to .csv

    You will require to download the dataset (zip) from the following url:

    https://www.kaggle.com/rtatman/sentiment-lexicons-for-81-languages

    Once downloaded you will require to extract 2 .txt files:

    negative_words_es.txt
    positive_words_es.txt

    Parameters
    ----------
    doc : spacy.doc
        A doc object.

    """"""

    # Load positive and negative words into lists.
    with open(""positive_words_es.txt"", ""r"", encoding=""utf-8"") as temp_file:
        positive_words = temp_file.read().splitlines()

    with open(""negative_words_es.txt"", ""r"", encoding=""utf-8"") as temp_file:
        negative_words = temp_file.read().splitlines()

    data_list = [[""text"", ""score""]]

    for sent in doc.sents:

        # Only take into account real sentences.
        if len(sent.text) > 10:

            score = 0

            # Start scoring the sentence.
            for word in sent:

                if word.lower_ in positive_words:
                    score += 1

                if word.lower_ in negative_words:
                    score -= 1

            data_list.append([sent.text, score])


    with open(""./sentences.csv"", ""w"", encoding=""utf-8"", newline="""") as sentences_file:
        csv.writer(sentences_file).writerows(data_list)


if __name__ == ""__main__"":

    main()
"
Text Analysis of the Mexican Government Report,step3.py,"""""""
This script generates plots and insights that were used in the infographic.
It is advised to only run one function at a time.
""""""

import geopandas
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

sns.set(style=""ticks"",
        rc={
            ""figure.figsize"": [12, 7],
            ""text.color"": ""white"",
            ""axes.labelcolor"": ""white"",
            ""axes.edgecolor"": ""white"",
            ""xtick.color"": ""white"",
            ""ytick.color"": ""white"",
            ""axes.facecolor"": ""#5C0E10"",
            ""figure.facecolor"": ""#5C0E10""}
        )

ACCENT_MARKS = [""á"", ""Á"", ""é"", ""É"", ""í"", ""Í"", ""ó"", ""Ó"", ""ú"", ""Ú""]
FRIENDLY_MARKS = [""a"", ""A"", ""e"", ""E"", ""i"", ""I"", ""o"", ""O"", ""u"", ""U""]

STATES = [
    ""Aguascalientes"",
    ""Baja California"",
    ""Baja California Sur"",
    ""Campeche"",
    ""Chiapas"",
    ""Chihuahua"",
    ""Ciudad de México"",
    ""Coahuila"",
    ""Colima"",
    ""Durango"",
    ""Estado de México"",
    ""Guanajuato"",
    ""Guerrero"",
    ""Hidalgo"",
    ""Jalisco"",
    ""Michoacán"",
    ""Morelos"",
    ""Nayarit"",
    ""Nuevo León"",
    ""Oaxaca"",
    ""Puebla"",
    ""Querétaro"",
    ""Quintana Roo"",
    ""San Luis Potosí"",
    ""Sinaloa"",
    ""Sonora"",
    ""Tabasco"",
    ""Tamaulipas"",
    ""Tlaxcala"",
    ""Veracruz"",
    ""Yucatán"",
    ""Zacatecas""
]


def get_word_counts(df):
    """"""Gets the total word count and the total unique lemmas.

    Parameters
    ----------
    df : pandas.DataFrame
        The DataFrame to be analyzed.

    """"""

    # Small fix for programa and programar.
    df.loc[df['lemma_lower'] == ""programa"", ""lemma_lower""] = ""programar""

    words = df[df[""is_alphabet""] == True][""text_lower""].count()
    print(""Words:"", words)

    unique_words = df[df[""is_alphabet""] == True][""lemma_lower""].nunique()
    print(""Unique words:"", unique_words)


def plot_most_used_words(df):
    """"""Generates a bar plot with the counts of the most used lemmas.

    Parameters
    ----------
    df : pandas.DataFrame
        The DataFrame to be plotted.

    """"""

    # Small fix for programa and programar.
    df.loc[df[""lemma_lower""] == ""programa"", ""lemma_lower""] = ""programar""

    # Only take into account alphabet tokens that are longer than 1 character and are not stop words.
    words = df[
        (df[""is_alphabet""] == True) &
        (df[""is_stopword""] == False) &
        (df[""lemma_lower""].str.len() > 1)
    ][""lemma_lower""].value_counts()[:20]

    sns.barplot(x=words.values, y=words.index, palette=""Blues_d"", linewidth=0)
    plt.xlabel(""Occurrences Count"")
    plt.title(""Most Frequent Words"")
    plt.savefig(""words_counts.png"", facecolor=""#5C0E10"")


def get_entity_counts(df):
    """"""Gets the number of counts per entity.

    Parameters
    ----------
    df : pandas.DataFrame
        The DataFrame to be analyzed.

    """"""

    entities = df[""label""].value_counts()
    print(entities)

    locations = df[df[""label""] == ""ORG""][""text""].value_counts()
    print(locations)


def get_state_counts(df):
    """"""Gets the number of counts per state.

    Parameters
    ----------
    df : pandas.DataFrame
        The DataFrame to be analyzed.

    """"""

    total_count = 0
    state_counts = list()

    # We will get the count for each state.
    for state in STATES:

        state_count = len(df[df[""text_lower""] == state.lower()])

        state_counts.append([state, state_count])
        total_count += state_count

    state_counts.sort(key=lambda x: x[1])

    print(state_counts)
    print(total_count)


def plot_map(df):
    """"""Generates a map using the state counts. You will require to download
    the following file and extract its contents to a folder named: mexicostates

    https://www.arcgis.com/home/item.html?id=ac9041c51b5c49c683fbfec61dc03ba8

    Parameters
    ----------
    df : pandas.DataFrame
        The DataFrame to be plotted.

    """"""

    # First we read the shape file from its unzipped folder.
    mexico_df = geopandas.read_file(""./mexicostates"")

    for state in STATES:

        # We remove accent marks and rename Ciudad de Mexico to its former name.
        clean_name = clean_word(state)

        if clean_name == ""Ciudad de Mexico"":
            clean_name = ""Distrito Federal""
        elif clean_name == ""Estado de Mexico"":
            clean_name = ""Mexico""

        # We insert the count value into the row with the matching ADMIN_NAME (state name).
        mexico_df.loc[
            mexico_df[""ADMIN_NAME""] == clean_name, ""count""
        ] = len(df[df[""text_lower""] == state.lower()])

    plt.rcParams[""figure.figsize""] = [12, 8]

    mexico_df.plot(column=""count"", cmap=""plasma"", legend=True)
    plt.title(""Mentions by State"")
    plt.axis(""off"")
    plt.tight_layout()
    plt.savefig(""map.png"", facecolor=""#5C0E10"")


def clean_word(word):
    """"""Cleans the word by replacing non-friendly characters.

    Parameters
    ----------
    word : str
        The word to be cleaned.

    Returns
    -------
    str
        The cleaned word.

    """"""

    for index, char in enumerate(ACCENT_MARKS):
        word = word.replace(char, FRIENDLY_MARKS[index])

    return word


def plot_sentiment_analysis(df):
    """"""Generates a bar plot with the sentiment scores of each sentence.

    Parameters
    ----------
    df : pandas.DataFrame
        The DataFrame to be plotted.

    """"""

    # Only take into account scores between -10 and 10.
    df = df[(df[""score""] <= 10) & (df[""score""] >= -10)]

    # We will make bars with a score below zero yellow and
    # bars with a score above zero blue.
    colors = np.array([(0.811, 0.913, 0.145)]*len(df[""score""]))
    colors[df[""score""] >= 0] = (0.529, 0.870, 0.972)

    yticks_labels = [str(i) for i in range(-12, 12, 2)]
    plt.yticks(np.arange(-12, 12, 2), yticks_labels)

    plt.bar(df.index, df[""score""], color=colors, linewidth=0)
    plt.xlabel(""Sentence Number"")
    plt.ylabel(""Score"")
    plt.title(""Sentiment Analysis"")
    plt.savefig(""sentiment_analysis.png"", facecolor=""#5C0E10"")


def plot_donut(df):
    """"""Generates a donut plot with the counts of 3 categories.

    Parameters
    ----------
    df : pandas.DataFrame
        The DataFrame to be plotted.

    """"""

    # We will only need 3 categories and 3 values.
    labels = [""Positivo"", ""Negativo"", ""Neutro""]

    positive = len(df[df[""score""] > 0])
    negative = len(df[df[""score""] < 0])
    neutral = len(df[df[""score""] == 0])

    values = [positive, negative, neutral]
    colors = [""green"", ""orange"", ""yellow""]
    explode = (0, 0, 0)  # Explode a slice if required

    plt.rcParams[""font.size""] = 18
    plt.rcParams[""legend.fontsize""] = 20

    plt.pie(values, explode=explode, labels=None,
            colors=colors, autopct='%1.1f%%', shadow=False)

    # We draw a circle in the Pie chart to make it a donut chart.
    centre_circle = plt.Circle(
        (0, 0), 0.75, color=""#5C0E10"", fc=""#5C0E10"", linewidth=0)

    fig = plt.gcf()
    fig.gca().add_artist(centre_circle)

    plt.axis(""equal"")
    plt.legend(labels)
    plt.savefig(""donut.png"",  facecolor=""#5C0E10"")


if __name__ == ""__main__"":

    tokens_df = pd.read_csv(""./data/tokens.csv"")
    entities_df = pd.read_csv(""./data/entities.csv"")
    sentences_df = pd.read_csv(""./data/sentences.csv"")

    # get_word_counts(tokens_df)
    # get_entity_counts(entities_df)
    # get_state_counts(entities_df)

    # plot_most_used_words(tokens_df)
    # plot_map(entities_df)
    # plot_sentiment_analysis(sentences_df)
    # plot_donut(sentences_df)
"
Text Analysis of the Mexican Government Report,step1.py,"""""""
This script reads the government report, extracts, cleans and saves
all the text so it can be analyzed in the next scripts.
""""""

import re

import PyPDF2


CHARACTERS = {
    ""ç"": ""Á"",
    ""⁄"": ""á"",
    ""…"": ""É"",
    ""”"": ""é"",
    ""ê"": ""Í"",
    ""™"": 'í',
    ""î"": ""Ó"",
    ""Š"": ""ó"",
    ""ı"": ""ö"",
    ""ò"": ""Ú"",
    ""œ"": ""ú"",
    ""Œ"": ""ñ"",
    ""Ô"": ""‘"",
    ""Õ"": ""’"",
    ""¥"": ""• "",
    ""Ñ"": ""—"",
    ""¨"": ""®"",
    ""«"": ""´"",
    ""Ò"": ""“""
}


def extract_text():
    """"""Read the PDF contents and extract the text that we need.""""""

    reader = PyPDF2.PdfFileReader(""informe.pdf"")
    full_text = """"

    # The page numbers in the PDF are not the same as the reported
    # number of pages, we use this variable to keep track of both.
    pdf_page_number = 3

    # We will only retrieve the first 3 sections of the government report
    # which are between pages 14 and 326.
    for i in range(14, 327):

        # This block is used to remove the page number at the start of
        # each page. The first if removes page numbers with one digit.
        # The second if removes page numbers with 2 digits and the else
        # statement removes page numbers with 3 digits.
        if pdf_page_number <= 9:
            page_text = reader.getPage(i).extractText().strip()[1:]
        elif pdf_page_number >= 10 and pdf_page_number <= 99:
            page_text = reader.getPage(i).extractText().strip()[2:]
        else:
            page_text = reader.getPage(i).extractText().strip()[3:]

        full_text += page_text.replace(""\n"", """")
        pdf_page_number += 1

    # There's a small issue when decoding the PDF file.
    # We will manually fix all the weird characters
    # with their correct equivalents.
    for item, replacement in CHARACTERS.items():
        full_text = full_text.replace(item, replacement)

    # We remove all extra white spaces.
    full_text = re.sub("" +"", "" "", full_text)

    # Finally we save the cleaned text into a .txt file.
    with open(""transcript_clean.txt"", ""w"", encoding=""utf-8"") as temp_file:
        temp_file.write(full_text)


if __name__ == ""__main__"":

    extract_text()
"
